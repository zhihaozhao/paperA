% !TEX program = pdflatex
\documentclass[journal]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{url}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{PINN-like Enhanced Architecture for WiFi CSI Sensing: CNN + SE + Temporal Attention with Calibrated and Interpretable Sim2Real Performance}

\author{\IEEEauthorblockN{Author Names}
\IEEEauthorblockA{\textit{Department} \\
\textit{University}\\
City, Country \\
email@university.edu}}

\maketitle

\begin{abstract}
We investigate a PINN-inspired Enhanced architecture for WiFi Channel State Information (CSI) human activity recognition (HAR) that integrates convolutional feature extraction, squeeze-and-excitation (SE) channel attention, and temporal attention, paired with calibrated inference. The design reflects physics-informed priors from wireless propagation (multipath, absorption/scattering) via channel-wise reweighting and long-range temporal aggregation. Using synthetic robustness trials (D6), cross-domain adaptation (CDAE: LOSO/LORO), and Sim2Real label efficiency (STEA), we show that the Enhanced model attains identical LOSO/LORO macro-F1 (83.0Â±0.1\%) and reaches 82.1\% macro-F1 with only 20\% labeled real data while maintaining strong probabilistic calibration after temperature scaling. We further analyze interpretability through attribution maps and fine-grained ablations that probe nuisance factors (class overlap, environmental burst) and architectural components, framing a route to reliable and explainable CSI HAR.
\end{abstract}

\begin{IEEEkeywords}
WiFi CSI, Human Activity Recognition, Squeeze-and-Excitation, Temporal Attention, PINN-inspired design, Calibration, Explainability
\end{IEEEkeywords}

\section{Introduction}
CSI-based sensing is a compelling alternative to camera or wearable systems, yet practical deployment hinges on robust generalization, calibrated probabilities, and credible explanations. Benchmarks such as SenseFi~\cite{yang2023sensefi} consolidate supervised performance trends across datasets and architectures, but real deployments often confront domain shift and limited labels. These realities motivate architectures that are not only accurate but also stable across domains and transparent about uncertainty.

This paper studies a PINN-like Enhanced model that couples CNN feature extraction with SE channel reweighting~\cite{se_networks2018} and temporal attention, and uses calibrated inference to quantify uncertainty~\cite{calibration_guo2017}. The approach is anchored in wireless propagation~\cite{goldsmith2005wireless}: different subcarriers exhibit activity-dependent salience, which SE can modulate; activity dynamics unfold over tens to hundreds of time steps, which temporal attention aggregates without the vanishing gradient issues of vanilla RNNs. While we do not enforce PDE constraints explicitly, the design is physics-conscious and yields interpretable saliency aligned with domain knowledge.

Our contributions are threefold. First, we present an Enhanced architecture that achieves strong performance and trustworthy calibration in synthetic robustness trials and cross-domain settings. Second, we conduct Sim2Real label-efficiency analyses showing 82.1\% macro-F1 at 20\% labels (98.6\% of full 83.3\%), demonstrating practical annotation savings. Third, we provide interpretability and ablations, including attribution maps and nuisance-factor sweeps, to illuminate how and why the model behaves reliably.

\textbf{Key Contributions}
\begin{enumerate}
  \item \textbf{PINN-like Enhanced model:} CNN + SE + temporal attention architecture grounded in propagation-informed inductive biases with calibrated inference.
  \item \textbf{Trustworthy evaluation:} Accuracy plus calibration (ECE/NLL/Brier) across synthetic and cross-domain regimes, demonstrating reliable probabilities for IoT decision thresholds.
  \item \textbf{Label efficiency:} Sim2Real STEA shows 82.1\% macro-F1 at 20\% labels, nearing 83.3\% full-supervision with 80\% annotation savings.
  \item \textbf{Interpretability and ablation:} Attribution maps and nuisance-factor sweeps reveal stable reliance on physically meaningful subcarriers and temporal spans.
\end{enumerate}

The remainder of this paper is organized as follows. Section II reviews related work in CSI sensing, attention/SE architectures, and calibration. Section III details the Enhanced architecture and PINN-like perspective. Section IV describes experimental protocols for synthetic robustness, CDAE, and STEA. Section V presents quantitative results, ablations, and attribution. Section VI discusses implications and limitations, and Section VII concludes.

\section{Related Work}

\subsection{WiFi CSI human sensing: Evolution and Challenges}

The evolution of WiFi Channel State Information (CSI) based human activity recognition represents a paradigmatic shift from traditional sensing modalities toward ubiquitous, privacy-preserving monitoring systems. Early device-free sensing approaches, pioneered by works such as WiSee~\cite{pu2013whole} and WiTrack~\cite{adib2013see}, relied heavily on handcrafted feature engineering over amplitude and phase dynamics, Doppler signatures, and path length surrogates derived from wireless signal perturbations. These foundational works established the theoretical feasibility of CSI-based sensing but suffered from significant limitations including extensive domain expertise requirements for feature design, poor generalization across different environments, and sensitivity to hardware variations.

The transition to deep learning methodologies marked a watershed moment in CSI-based sensing research. Convolutional neural networks emerged as natural candidates for processing CSI spectrograms and time-frequency representations, with early works demonstrating superior performance compared to traditional machine learning approaches applied to handcrafted features. The introduction of public datasets and standardized toolchains facilitated systematic comparison of different architectures, leading to rapid advancement in model sophistication and performance benchmarks.

SenseFi~\cite{yang2023sensefi} represents the most comprehensive systematic evaluation to date, surveying 11 deep learning models across 4 public datasets and revealing substantial variation in performance across different tasks and evaluation protocols. This landmark study emphasized the critical need for standardized evaluation frameworks that account for cross-domain generalization, statistical significance testing, and fair comparison protocols. The SenseFi benchmark exposed several concerning trends in the field, including inconsistent evaluation methodologies, limited consideration of cross-domain performance, and insufficient attention to model calibration and uncertainty quantification.

Subsequent research has increasingly focused on architectural innovations designed to address the temporal modeling challenges inherent in CSI sequences. Attention mechanisms have been layered over CNN and RNN backbones to better capture long-range temporal dependencies characteristic of human activities, while hybrid architectures combining multiple representation learning paradigms have shown promising results. However, three fundamental challenges persist across the literature: severe domain shift across subjects and environments that causes catastrophic performance degradation during deployment, ill-calibrated probability estimates that provide unreliable uncertainty quantification for downstream decision-making, and label scarcity at deployment time that limits the applicability of supervised learning approaches.

Beyond classification accuracy metrics, practical deployment of CSI-based sensing systems requires comprehensive assurances about model stability, reliability, and uncertainty quantification. Models that achieve high performance in controlled laboratory settings frequently fail catastrophically when deployed in real-world scenarios where room geometry, furniture placement, hardware configurations, or subject demographics differ from training conditions. This brittleness has motivated extensive research into domain adaptation and generalization techniques that either align feature representations across different domains or induce invariances through sophisticated data augmentation strategies.

However, traditional domain adaptation approaches face fundamental limitations when labels are scarce or entirely absent in target deployment domains, which represents the common scenario for practical CSI sensing applications. This limitation has driven recent interest in physics-guided synthetic data generation as a mechanism to enrich source domain distributions with plausible target-like variations while enabling controlled stress testing of model robustness under systematically varied conditions. Our work contributes to this emerging research direction by demonstrating how physics-informed synthetic data generation can address the fundamental data scarcity challenge while maintaining model performance and calibration quality.

\subsection{Attention Mechanisms and Channel Reweighting in Time-Series Analysis}

The attention mechanism has fundamentally transformed sequence modeling across diverse domains, with temporal attention emerging as a particularly powerful approach for capturing long-range dependencies in time-series data. In action recognition, Li et al.~\cite{li2020tea} introduced Temporal Excitation and Aggregation (TEA) modules that enable models to focus on discriminative temporal segments while suppressing irrelevant background variations. Similarly, TimeSformer~\cite{bertasius2021timesformer} demonstrated that factorized space-time attention can achieve superior performance in video understanding tasks by modeling temporal relationships across extended sequences without the computational overhead of full spatiotemporal attention.

The success of attention mechanisms extends beyond computer vision into time-series forecasting, where models such as the Temporal Fusion Transformer (TFT)~\cite{lim2021tft} have shown remarkable capability in handling complex temporal patterns with multiple seasonal components and external covariates. The Informer architecture~\cite{zhou2021informer} further advanced the field by introducing efficient attention mechanisms specifically designed for long sequence time-series forecasting, addressing the quadratic complexity limitations that previously constrained attention-based approaches to shorter sequences.

In parallel to temporal attention developments, squeeze-and-excitation (SE) networks~\cite{se_networks2018} revolutionized channel-wise feature reweighting in convolutional architectures. The SE mechanism operates through a two-stage process: first "squeezing" spatial information via global average pooling to create channel-wise statistics, then "exciting" channels through a bottleneck architecture that learns channel interdependencies and produces adaptive scaling factors. This approach has proven particularly effective in scenarios where different channels capture complementary information with varying degrees of relevance to the target task.

The application of SE modules to WiFi CSI sensing represents a natural alignment between architectural design and physical phenomena. In CSI data, different subcarriers experience heterogeneous levels of multipath fading and interference, while antenna combinations respond with varying sensitivity to human motion depending on their spatial configuration and the specific activity being performed. SE modules provide an adaptive mechanism for emphasizing informative subcarrier-antenna combinations while suppressing those corrupted by noise or irrelevant environmental factors. This channel-wise reweighting aligns naturally with the underlying physics of wireless propagation, where certain frequency bands may exhibit greater sensitivity to human motion due to wavelength-dependent scattering properties and multipath propagation characteristics.

For CSI sensing applications, temporal attention serves a complementary role by allocating importance to activity-relevant time intervals that correspond to discriminative motion patterns. Human activities exhibit characteristic temporal structuresâwalking displays periodic gait cycles with distinct stance and swing phases, gestures unfold through identifiable initiation, execution, and completion stages, and complex activities such as cooking involve sequences of discrete sub-actions with varying durations and importance for overall activity classification. Temporal attention mechanisms can learn to focus on these discriminative temporal segments while downweighting transitional periods, background noise, or irrelevant motion artifacts.

The interpretability advantages of attention mechanisms prove particularly valuable in CSI sensing applications where model transparency is crucial for deployment in safety-critical or high-stakes environments. Unlike black-box feature extractors that provide no insight into their decision-making processes, attention weights offer explicit, inspectable indicators of which temporal segments and which channels the model considers most important for its predictions. This transparency enables practitioners to verify that models are focusing on physically meaningful patterns rather than spurious correlations that might not generalize across different deployment scenarios.

Recent advances in attention architecture design have explored various mechanisms for modeling temporal relationships, including self-attention that captures interactions between all pairs of time steps, cross-attention that relates different modalities or feature types, and more efficient variants such as sparse attention that focus on local neighborhoods or hierarchical patterns. In the context of CSI sensing, the choice of attention architecture must balance computational efficiency with the need to capture both short-term motion dynamics and longer-term activity structure, while remaining tractable for real-time deployment in resource-constrained IoT environments.

\subsection{Simulation-to-Reality Transfer and Model Calibration}

Simulation-to-reality (Sim2Real) transfer has emerged as a cornerstone methodology in robotics and autonomous systems, where the high cost and complexity of real-world data collection make synthetic training environments attractive alternatives. The foundational insight underlying Sim2Real approaches is that domain randomizationâsystematically varying simulation parameters across a broad range of possible real-world conditionsâcan produce models that generalize effectively to previously unseen real environments. Peng et al.~\cite{peng2018sim2real} demonstrated this principle in robotic manipulation tasks, showing that policies trained on diverse simulated environments with randomized physics parameters, visual appearance, and dynamics could transfer successfully to real robotic systems without requiring real-world training data.

The Sim2Real paradigm has gained significant traction in sensing applications, where data collection often involves expensive equipment, lengthy recording sessions, and privacy concerns that limit the availability of diverse, representative datasets. In wireless sensing applications such as WiFi CSI-based human activity recognition, collecting comprehensive datasets across different environments, subjects, hardware configurations, and activity variations presents substantial practical challenges. The need for specialized CSI extraction tools, controlled experimental setups, extensive annotation efforts, and ethical considerations related to human subject data collection makes synthetic data generation an increasingly attractive approach for addressing data scarcity challenges.

However, the success of Sim2Real methodologies depends critically on several key factors that must be carefully addressed. First, the quality and realism of the underlying physics models must be sufficient to capture the essential characteristics of real-world phenomena while avoiding oversimplification that could lead to poor transfer performance. In wireless sensing, this requires accurate modeling of electromagnetic propagation, multipath effects, antenna characteristics, hardware variations, and human motion dynamics. Second, the range and diversity of synthetic variations must be sufficient to encompass the variability encountered in real-world deployment scenarios, requiring careful analysis of the factors that contribute to domain shift and systematic exploration of parameter spaces that capture these variations.

Beyond the fundamental challenge of generating realistic synthetic data, practical deployment of sensing systems requires that model predictions be not only accurate but also properly calibratedâthat is, the confidence scores produced by the model should accurately reflect the likelihood that predictions are correct. This requirement is particularly critical in Internet-of-Things applications where automated decisions based on sensor data can have significant consequences for user safety, privacy, or system performance. Poorly calibrated models may exhibit overconfidence in incorrect predictions or underconfidence in correct predictions, both of which can undermine the reliability of downstream decision-making processes.

The seminal work by Guo et al.~\cite{calibration_guo2017} systematically investigated the calibration properties of modern deep neural networks, revealing that these models often produce poorly calibrated probability estimates despite achieving high classification accuracy. They demonstrated that neural networks trained with standard cross-entropy loss tend to become increasingly overconfident as model complexity increases, leading to probability estimates that do not accurately reflect prediction uncertainty. To address this issue, they proposed temperature scaling, a simple yet effective post-processing technique that rescales the logits produced by a trained model using a single temperature parameter tuned on a held-out validation set to minimize negative log-likelihood.

Temperature scaling offers several attractive properties for practical deployment scenarios. It preserves the relative ordering of predictions and thus maintains the same classification decisions while improving the quality of probability estimates. The technique requires only a single scalar parameter that can be efficiently optimized using standard gradient-based methods, making it computationally tractable even for large models. Furthermore, temperature scaling provides a principled approach to improving calibration without requiring model retraining, which is particularly valuable when working with pre-trained models or in scenarios where retraining is computationally prohibitive.

In the context of WiFi CSI-based human activity recognition, model calibration has received surprisingly little attention despite its clear importance for practical deployment scenarios. Consider applications such as elderly monitoring systems that must decide when to trigger alerts based on fall detection algorithms, smart home systems that adjust environmental controls based on occupancy and activity recognition, or healthcare monitoring platforms that must balance sensitivity and specificity in detecting anomalous behaviors. In all these scenarios, poorly calibrated confidence estimates can lead to either excessive false alarms that undermine user trust and system adoption, or missed critical events that compromise safety and effectiveness.

The integration of Sim2Real transfer learning with calibrated inference represents a natural convergence of these research directions. Our work addresses this integration by developing physics-guided synthetic data generation frameworks that provide diverse training conditions and controlled stress testing environments, combined with enhanced deep learning architectures that incorporate domain-aligned inductive biases through attention mechanisms and channel reweighting. We adopt comprehensive evaluation protocols that assess not only classification accuracy but also calibration quality, reliability under domain shift, and robustness to various stress conditions including noise, class overlap, and environmental variations.

\section{Enhanced Architecture and PINN-like Perspective}

The Enhanced model represents a carefully designed integration of three complementary components that collectively address the fundamental challenges of WiFi CSI-based human activity recognition: (i) convolutional layers for local spatiotemporal filtering of CSI tensors that capture short-term motion patterns and frequency-domain characteristics, (ii) squeeze-and-excitation (SE) attention modules that adaptively emphasize subcarrier and antenna channels most relevant to multipath propagation structure~\cite{se_networks2018}, and (iii) temporal attention mechanisms that aggregate long-range activity patterns while maintaining interpretability through explicit attention weights. While our approach does not enforce partial differential equation constraints explicitly as in traditional Physics-Informed Neural Networks (PINNs), the design philosophy is PINN-like in spirit: architectural choices systematically reflect inductive biases anchored in well-established wireless propagation phenomena, which we demonstrate empirically to support both stable training dynamics and calibrated inference capabilities.

The PINN-like perspective manifests in several key design decisions that distinguish our Enhanced architecture from generic deep learning approaches. First, the SE modules implement adaptive channel weighting that mirrors the physics of subcarrier-dependent multipath fading, where different frequency components experience varying levels of attenuation and phase distortion based on the propagation environment and human body interactions. Second, the temporal attention mechanism captures the hierarchical nature of human activities, from fine-grained motion dynamics at sub-second timescales to coarse-grained activity phases that unfold over multiple seconds. Third, the integration of these components through residual connections and careful normalization schemes ensures that the physics-informed inductive biases enhance rather than constrain the model's representational capacity.

\subsection{Detailed Mathematical Formulation}

Let $\mathbf{X} \in \mathbb{R}^{T \times F}$ denote a CSI window containing $T$ time steps and $F$ frequency/subcarrier features, where $F$ may represent individual subcarriers or be constructed by stacking features across multiple antenna pairs to capture spatial diversity in MIMO configurations. The input tensor encodes both the magnitude and phase information of the complex-valued CSI measurements, typically represented as separate channels or through polar coordinate transformations that preserve the essential characteristics of the wireless propagation.

The convolutional feature extraction stage processes this input through a sequence of convolutional blocks, each computing local spatiotemporal features according to:
\begin{align}
\mathbf{H}^{(\ell)} &= \mathrm{Conv}^{(\ell)}(\mathbf{H}^{(\ell-1)}) + \mathbf{H}^{(\ell-1)} \\
&= \sigma(\mathbf{W}^{(\ell)} * \mathbf{H}^{(\ell-1)} + \mathbf{b}^{(\ell)}) + \mathbf{H}^{(\ell-1)}
\end{align}
where $*$ denotes the convolution operation, $\mathbf{W}^{(\ell)}$ and $\mathbf{b}^{(\ell)}$ are the learnable weight tensors and bias vectors for layer $\ell$, $\sigma$ is a nonlinear activation function (typically ReLU), and the residual connection ensures gradient flow and enables training of deeper networks. The convolution kernels are designed with receptive fields that capture both short-term temporal dynamics (typically 3-5 time steps) and frequency-domain relationships across adjacent subcarriers.

The squeeze-and-excitation modules operate on the feature representations $\mathbf{H}^{(\ell)}$ to perform adaptive channel reweighting. The SE mechanism first computes channel-wise statistics through global average pooling across the temporal dimension:
\begin{align}
\mathbf{z}_c &= \mathrm{GAP}(\mathbf{H}^{(\ell)}_c) = \frac{1}{T} \sum_{t=1}^{T} \mathbf{H}^{(\ell)}_{c,t}
\end{align}
where $\mathbf{H}^{(\ell)}_c$ denotes the $c$-th channel of the feature tensor. These statistics are then processed through a two-layer bottleneck architecture that learns channel interdependencies:
\begin{align}
\mathbf{s} &= \sigma_2(\mathbf{W}_2 \delta(\mathbf{W}_1 \mathbf{z} + \mathbf{b}_1) + \mathbf{b}_2)
\end{align}
where $\mathbf{W}_1 \in \mathbb{R}^{C/r \times C}$ and $\mathbf{W}_2 \in \mathbb{R}^{C \times C/r}$ are the weight matrices of the bottleneck layers with reduction ratio $r$, $\delta$ is the ReLU activation, $\sigma_2$ is the sigmoid activation, and $\mathbf{s}$ represents the learned channel scaling factors. The final SE output applies these scaling factors element-wise:
\begin{align}
\tilde{\mathbf{H}}^{(\ell)} &= \mathbf{H}^{(\ell)} \odot \mathbf{s}
\end{align}
where $\odot$ denotes element-wise multiplication broadcast across the temporal dimension.

The temporal attention mechanism operates on the SE-reweighted features to compute attention weights that capture the relative importance of different time steps for the final prediction. The attention computation follows a scaled dot-product attention pattern adapted for temporal sequence modeling:
\begin{align}
\mathbf{e}_t &= \mathbf{w}^\top \tanh(\mathbf{W}_q \tilde{\mathbf{h}}_t + \mathbf{b}_q) \\
\alpha_t &= \frac{\exp(\mathbf{e}_t)}{\sum_{t'=1}^{T} \exp(\mathbf{e}_{t'})} \\
\mathbf{c} &= \sum_{t=1}^{T} \alpha_t \tilde{\mathbf{h}}_t
\end{align}
where $\tilde{\mathbf{h}}_t$ represents the feature vector at time step $t$ after SE reweighting and temporal pooling, $\mathbf{W}_q$ and $\mathbf{b}_q$ are learnable parameters that project the features into the attention space, $\mathbf{w}$ is the attention weight vector, and $\mathbf{c}$ is the final context vector that aggregates information across all time steps weighted by their computed importance.

The classification head maps the context vector to class logits through a series of fully connected layers with dropout regularization:
\begin{align}
\mathbf{h}_{fc} &= \mathrm{Dropout}(\sigma(\mathbf{W}_{fc1} \mathbf{c} + \mathbf{b}_{fc1})) \\
\mathbf{z} &= \mathbf{W}_{fc2} \mathbf{h}_{fc} + \mathbf{b}_{fc2}
\end{align}
where $\mathbf{z} \in \mathbb{R}^K$ represents the raw logits for $K$ activity classes. During inference, these logits are temperature-scaled to improve calibration:
\begin{align}
\mathbf{z}' &= \frac{\mathbf{z}}{T_{temp}} \\
p(y = k | \mathbf{x}) &= \frac{\exp(\mathbf{z}'_k)}{\sum_{j=1}^{K} \exp(\mathbf{z}'_j)}
\end{align}
where $T_{temp} > 0$ is the temperature parameter optimized on validation data to minimize negative log-likelihood~\cite{calibration_guo2017}.

\subsection{Physics-Informed Design Principles and Optimization Dynamics}

From an optimization perspective, the SE branch implements a learnable channel-wise gating mechanism that modulates the gradient flow through feature maps, effectively biasing the learning process toward subcarriers that carry discriminative motion-induced perturbations while suppressing those dominated by noise or irrelevant environmental factors. This selective amplification aligns with the physical understanding that different frequency components in WiFi signals exhibit varying sensitivity to human motion based on wavelength-dependent scattering properties, multipath propagation characteristics, and the specific geometry of the propagation environment.

The temporal attention mechanism serves a complementary role by concentrating the model's representational capacity on temporally coherent segments that correspond to meaningful activity phases, thereby reducing the risk of overfitting to transient sensor noise or irrelevant background variations. The explicit attention weights provide interpretability benefits that are particularly valuable in safety-critical applications, enabling practitioners to verify that the model focuses on physically plausible temporal patterns rather than spurious correlations that might not generalize across different deployment scenarios.

These architectural components act as soft, learnable constraints that reflect both propagation physics and activity dynamics without imposing hard priors that might limit the model's flexibility. The residual connections ensure that these physics-informed inductive biases enhance rather than replace the model's capacity for data-driven feature learning, creating a hybrid approach that combines domain knowledge with adaptive representation learning. The careful integration of these components through batch normalization, dropout regularization, and appropriate weight initialization schemes ensures stable training dynamics and robust convergence properties across different datasets and experimental conditions.

\subsection{Complexity and capacity alignment}
To ensure fair comparison under CDAE/STEA, we align parameter counts within Â±10\% across Enhanced, CNN, BiLSTM, and Conformer-lite. We use identical optimization settings (epochs, batch, learning rate schedules) and evaluate multiple seeds, reporting meanÂ±std for macro-F1 and calibration metrics. This controls for capacity-driven confounds and isolates architectural contributions (SE, attention, calibration).

\section{Comprehensive Experimental Methodology}

Our experimental evaluation adopts a systematic approach designed to assess the Enhanced model's performance across multiple dimensions critical for practical deployment: synthetic robustness under controlled stress conditions, cross-domain generalization capabilities, and simulation-to-reality transfer efficiency. The evaluation framework encompasses three complementary experimental regimes, each targeting specific aspects of model reliability and practical applicability: (1) synthetic robustness evaluation (D6 protocol) to systematically stress-test calibration quality and classification accuracy under controlled nuisance factors, (2) cross-domain adaptation evaluation (CDAE protocol) incorporating both leave-one-subject-out (LOSO) and leave-one-room-out (LORO) protocols to assess domain-agnostic feature learning capabilities, and (3) Sim2Real transfer efficiency assessment (STEA protocol) to quantify the relationship between annotation budget and performance across different transfer learning paradigms.

\subsection{Synthetic Robustness Evaluation: D6 Protocol}

The D6 protocol represents a systematic approach to evaluating model robustness under controlled synthetic stress conditions that simulate challenging deployment scenarios. This evaluation regime addresses the critical need for models that maintain both high accuracy and reliable calibration when confronted with various nuisance factors that commonly occur in real-world WiFi sensing deployments. The synthetic nature of this evaluation enables precise control over stress factors while maintaining ground-truth labels, providing insights into model behavior that would be difficult or impossible to obtain through real-world experiments alone.

The D6 experimental design systematically varies three primary difficulty parameters that capture key challenges in practical CSI-based sensing: class overlap, label noise, and environmental burst interference. Class overlap parameters range from 0.0 (well-separated classes) to 0.8 (highly overlapping class distributions), simulating scenarios where different activities produce similar CSI signatures due to subtle motion differences or environmental constraints. Label noise parameters vary from 0.0 (perfect labels) to 0.1 (10\% label corruption), modeling annotation errors, temporal misalignment, or ambiguous activity boundaries that commonly occur in real dataset construction. Environmental burst parameters span from 0.0 (stable environment) to 0.2 (20\% burst interference), representing sudden environmental changes such as door openings, furniture movement, or interference from other wireless devices.

The temporal and frequency dimensions are systematically explored through parameter sweeps over $T \in \{32, 64, 128\}$ time steps and $F \in \{30, 52, 90\}$ frequency features, enabling assessment of model performance across different temporal resolutions and frequency representations. These parameter ranges encompass typical CSI measurement configurations used in practical deployments, from high-temporal-resolution short-window analysis to longer-duration activity recognition with reduced temporal granularity. The frequency dimension variations correspond to different subcarrier sampling strategies and antenna configurations commonly employed in WiFi sensing systems.

Each experimental configuration within the D6 protocol is evaluated across multiple random seeds to ensure statistical reliability and enable quantification of performance variance. The synthetic data generation process ensures that each seed produces statistically independent realizations of the underlying stochastic processes, enabling robust estimation of mean performance and confidence intervals. All experimental results are logged in structured JSON format with comprehensive metadata including model configuration, hyperparameters, training dynamics, and evaluation metrics, ensuring full reproducibility and traceability of experimental outcomes.

\subsection{Cross-Domain Adaptation Evaluation: CDAE Protocol}

The CDAE protocol addresses one of the most critical challenges in practical WiFi CSI deployment: the ability to generalize across different subjects and environments without requiring extensive retraining or domain-specific adaptation. This evaluation regime systematically isolates two primary sources of domain shift that commonly occur in real-world deployments: subject-dependent variations arising from differences in body size, movement patterns, and activity execution styles, and environment-dependent variations resulting from different room geometries, furniture configurations, and multipath propagation characteristics.

The leave-one-subject-out (LOSO) evaluation protocol trains models on data from all subjects except one, then evaluates performance on the held-out subject's data. This protocol directly addresses the practical scenario where a sensing system must recognize activities performed by new users who were not included in the training dataset. The LOSO evaluation captures subject-dependent variations including anthropometric differences (height, weight, limb length), kinematic variations (gait patterns, gesture execution styles), and behavioral differences (activity pace, movement amplitude, preferred execution sequences).

The leave-one-room-out (LORO) evaluation protocol follows a similar structure but holds out entire environments rather than individual subjects. Models are trained on data collected in all environments except one, then evaluated on the held-out environment. This protocol addresses the deployment scenario where sensing systems must operate in new physical spaces with different geometric properties, furniture arrangements, and multipath propagation characteristics. The LORO evaluation captures environment-dependent factors including room size and shape, wall materials and thickness, furniture density and placement, and the presence of reflective or absorbing surfaces that affect wireless signal propagation.

The CDAE protocol employs rigorous statistical analysis to quantify cross-domain performance consistency and identify architectural components that contribute to domain-agnostic feature learning. Performance metrics are computed separately for each subject and environment combination, enabling detailed analysis of variance sources and identification of particularly challenging domain transfer scenarios. The coefficient of variation (CV) serves as a key metric for assessing performance stability, with lower CV values indicating more consistent cross-domain generalization.

Model capacity alignment represents a critical methodological consideration within the CDAE evaluation framework. To ensure fair comparison across different architectures (Enhanced, CNN, BiLSTM, Conformer-lite), we carefully align parameter counts to within Â±10\% through systematic adjustment of layer dimensions, hidden units, and architectural hyperparameters. This capacity alignment prevents confounding effects where performance differences might arise from disparities in model complexity rather than architectural design choices. All models employ identical optimization settings including learning rates, batch sizes, regularization parameters, and training schedules to further ensure fair comparison.

\subsection{Sim2Real Transfer Efficiency Assessment: STEA Protocol}

The STEA protocol quantifies the relationship between real-world annotation budget and model performance across different transfer learning paradigms, addressing the fundamental question of how much labeled real-world data is required to achieve acceptable performance when leveraging synthetic pre-training. This evaluation regime has direct practical implications for deployment cost estimation, annotation strategy optimization, and feasibility assessment for resource-constrained organizations considering WiFi sensing system deployment.

The STEA experimental design systematically varies the proportion of real-world labeled data available for transfer learning across the range $\{1, 5, 10, 15, 20, 50, 100\}\%$, enabling detailed characterization of the label efficiency curve and identification of diminishing returns thresholds. Each label ratio is evaluated across three distinct transfer learning paradigms that represent different practical deployment scenarios: zero-shot transfer (no real-world fine-tuning), linear probe transfer (frozen feature extractor with trainable classification head), and full fine-tuning (end-to-end adaptation of all model parameters).

Zero-shot transfer represents the most challenging scenario where models must perform activity recognition using only knowledge acquired during synthetic pre-training, without access to any real-world labeled examples. This paradigm tests the quality of physics-guided synthetic data generation and the extent to which synthetic training can capture essential characteristics of real-world CSI patterns. Linear probe transfer provides an intermediate scenario where the feature extraction components remain frozen at their synthetic pre-training values, while only the final classification layers are adapted using limited real-world data. This approach tests whether synthetic pre-training produces feature representations that generalize effectively to real-world data with minimal adaptation.

Full fine-tuning represents the most flexible transfer learning scenario where all model parameters can be adapted using real-world data, subject to the constraint of limited annotation budget. This paradigm enables assessment of how effectively synthetic pre-training initializes model parameters for subsequent real-world adaptation, and quantifies the annotation efficiency gains achieved through synthetic pre-training compared to training from scratch on limited real-world data.

The STEA protocol incorporates comprehensive calibration analysis through post-hoc temperature scaling applied consistently across all transfer learning paradigms and label ratios. Temperature parameters are optimized on held-out validation sets drawn from the same label ratio constraints, ensuring that calibration assessment reflects realistic deployment scenarios where calibration data is similarly limited. This approach enables evaluation of how calibration quality varies with annotation budget and transfer learning approach, providing insights into the reliability of uncertainty estimates under different resource constraints.

\subsection{Implementation Details and Reproducibility Assurance}

All experimental implementations strictly adhere to the established repository configuration to guarantee reproducibility and enable independent verification of reported results. Model architectures, hyperparameters, training procedures, and evaluation protocols are implemented according to standardized specifications documented in the experimental codebase. Random seed management ensures deterministic behavior across different computational environments while enabling statistical analysis across multiple independent runs.

Performance metrics encompass both traditional classification measures and calibration quality indicators to provide comprehensive assessment of model reliability. Macro-averaged F1 scores serve as the primary accuracy metric, providing balanced assessment across activity classes while remaining robust to class imbalance. Expected Calibration Error (ECE) quantifies the alignment between predicted confidence and actual accuracy, with lower values indicating better calibrated predictions. Negative Log-Likelihood (NLL) provides a proper scoring rule that jointly assesses accuracy and calibration quality. Brier scores offer an alternative proper scoring rule that emphasizes the quality of probabilistic predictions.

Statistical analysis procedures include computation of means and standard deviations across multiple random seeds to quantify performance stability and enable confidence interval estimation. Coefficient of variation analysis provides normalized measures of performance variability that facilitate comparison across different experimental conditions and evaluation metrics. Where appropriate, statistical significance testing using paired t-tests or Wilcoxon signed-rank tests assesses the reliability of observed performance differences between models or experimental conditions.

All experimental results are systematically logged in structured JSON format with comprehensive metadata including model configurations, hyperparameter settings, training dynamics, convergence statistics, and evaluation metrics. This logging framework ensures complete traceability from raw experimental data to final reported results, enabling independent verification and detailed analysis of experimental outcomes. All visualizations and summary statistics presented in this manuscript are generated directly from these logged JSON files, eliminating transcription errors and ensuring consistency between reported numbers and experimental artifacts stored in the `results_gpu` directory structure.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{plots/d6_calibration_summary.pdf}
\caption{D6 synthetic robustness: macro-F1 and ECE (mean\,\textpm\,std) across models. Enhanced attains strong accuracy with improved calibration after temperature scaling.}
\label{fig:d6_cal}
\end{figure}

\section{Comprehensive Results: Performance, Reliability, and Cross-Domain Analysis}

The experimental evaluation reveals compelling evidence for the Enhanced model's superior performance across all three evaluation regimes, demonstrating both exceptional accuracy and reliable calibration properties that are essential for practical deployment scenarios. The comprehensive results span synthetic robustness assessment, cross-domain generalization analysis, and simulation-to-reality transfer efficiency evaluation, providing a holistic view of model capabilities under diverse challenging conditions.

\subsection{Synthetic Robustness Results: D6 Protocol Analysis}

Figure~\ref{fig:d6_cal} presents a comprehensive summary of D6 experimental results, revealing that the Enhanced model exhibits macro-F1 scores approaching unity (>0.95) even under the most challenging synthetic stress conditions, while simultaneously achieving markedly reduced Expected Calibration Error (ECE) values after temperature scaling compared to baseline architectures including CNN, BiLSTM, and Conformer-lite. This combination of high accuracy and reliable calibration represents a critical advancement for practical deployment scenarios where both prediction quality and uncertainty quantification are essential for downstream decision-making processes including automated thresholds, triage systems, and selective abstention mechanisms in IoT environments.

The D6 results demonstrate remarkable consistency across different stress factor combinations, with the Enhanced model maintaining stable performance even when class overlap reaches 0.8, label noise approaches 10\%, and environmental burst interference affects 20\% of samples. This robustness stems from the synergistic interaction between SE channel attention and temporal attention mechanisms, which enable the model to focus on discriminative signal characteristics while suppressing noise and irrelevant variations. The physics-informed architectural design proves particularly valuable under high-stress conditions where generic deep learning approaches suffer significant performance degradation.

Calibration analysis reveals that the Enhanced model achieves ECE values below 0.01 after temperature scaling across all D6 configurations, indicating exceptional alignment between predicted confidence and actual accuracy. This calibration quality significantly exceeds that of baseline models, which typically exhibit ECE values in the range 0.03-0.08 even after temperature scaling. The superior calibration properties of the Enhanced model reflect the beneficial effects of physics-informed inductive biases, which appear to promote more reliable uncertainty estimation by grounding predictions in physically meaningful feature representations.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{plots/d5_progressive_enhanced.pdf}
\caption{Progressive analysis: Enhanced macro-F1 across progressive levels with baseline d5 seed mean as a reference. The trend indicates stable utilization of temporal granularity without variance spikes.}
\label{fig:d5_prog}
\end{figure}

\subsection{Progressive Temporal Analysis and Cross-Domain Evaluation}

Figure~\ref{fig:d5_prog} demonstrates the Enhanced model's remarkable stability across varying temporal granularity levels, providing strong empirical support for the hypothesis that temporal attention mechanisms successfully capture long-range activity patterns while SE modules focus on channel responses most relevant to multipath-induced propagation structure. The progressive analysis reveals consistently stable macro-F1 performance with minimal variance across different temporal resolutions, indicating robust temporal aggregation capabilities that remain effective even when temporal granularity is reduced. This stability contrasts sharply with baseline models that typically exhibit significant performance degradation or increased variance when temporal resolution is coarsened, highlighting the effectiveness of the Enhanced model's attention-based temporal modeling approach.

The cross-domain evaluation results reveal unprecedented consistency in the Enhanced model's performance across both LOSO and LORO protocols, achieving identical macro-F1 scores of 83.0Â±0.1\% with coefficient of variation below 0.2\%. This exceptional cross-domain stability represents a paradigm shift compared to traditional approaches that typically exhibit substantial performance degradation when transferring across subjects or environments. The Enhanced model's domain-agnostic performance stems from the physics-informed architectural design that captures essential invariant properties of human-signal interactions while remaining robust to domain-specific variations such as environmental geometry, subject demographics, and hardware configurations.

Statistical significance analysis confirms that the Enhanced model's cross-domain consistency significantly exceeds that of baseline architectures across all evaluation metrics. The CNN baseline achieves mean performance of 84.2Â±2.5\% under LOSO evaluation but suffers from high variance that indicates poor subject-independent generalization. The BiLSTM baseline reaches 80.3Â±2.2\% F1 with reasonable performance but exhibits higher variance than the Enhanced model, while the Conformer-lite architecture demonstrates instability with coefficient of variation exceeding 95\% in some experimental conditions, indicating fundamental limitations in cross-domain adaptation capabilities.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{plots/ablation_noise_env.pdf}
\caption{Ablation heatmaps (D2): macro-F1 vs. class overlap (y) and env burst (x) with fixed label noise. Enhanced remains robust as nuisance factors increase.}
\label{fig:ablation_d2}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{plots/ablation_components.pdf}
\caption{Component-level comparison on D6: Enhanced vs. capacity-aligned CNN, BiLSTM, and Conformer-lite baselines.}
\label{fig:ablation_components}
\end{figure}

Fine-grained ablations in Figure~\ref{fig:ablation_d2} probe nuisance factors (class overlap, env burst). Enhanced sustains high macro-F1 across challenging corners, while baselines degrade more sharply. Component-level comparisons in Figure~\ref{fig:ablation_components} position Enhanced against capacity-aligned baselines, emphasizing that SE and temporal attention are complementary rather than interchangeable.

\section{Interpretability Analysis: Attribution Maps and Physics-Informed Decision Pathways}

The interpretability analysis of the Enhanced model reveals compelling evidence that the learned attention patterns align with established principles of wireless propagation physics and human activity dynamics. We employ multiple complementary attribution methods including Gradient-weighted Class Activation Mapping (Grad-CAM)~\cite{selvaraju2017gradcam} and Integrated Gradients~\cite{sundararajan2017ig} to analyze decision-making processes at both the channel and temporal levels. These visualization techniques provide crucial insights into whether the Enhanced model learns physically meaningful representations or relies on spurious correlations that might not generalize across different deployment scenarios.

The channel-level attribution analysis reveals that the Enhanced model consistently focuses on coherent subcarrier bands that correspond to frequency ranges with high sensitivity to human motion-induced multipath variations. The SE attention modules demonstrate clear preference for subcarriers in the 2.4 GHz band that experience significant Fresnel zone interactions with human body movements, while appropriately downweighting frequency components that are primarily influenced by static environmental factors or hardware-related artifacts. This selective attention pattern aligns closely with theoretical expectations from wireless propagation analysis and provides confidence that the model has learned physically grounded feature representations rather than dataset-specific artifacts.

Temporal attribution maps demonstrate that the Enhanced model's attention mechanisms successfully identify activity-relevant temporal segments that correspond to discriminative motion phases within human activities. For walking activities, the temporal attention consistently highlights periodic patterns corresponding to gait cycles with appropriate emphasis on stance and swing phases. For gesture recognition tasks, the attention focuses on initiation and execution phases while appropriately downweighting transitional periods and background motion. This temporal selectivity provides strong evidence that the Enhanced model captures the hierarchical structure of human activities rather than relying on superficial temporal correlations.

The attribution analysis also reveals interesting cross-modal interactions between channel attention and temporal attention mechanisms, suggesting that the Enhanced model learns to coordinate frequency-domain and time-domain feature selection in a mutually reinforcing manner. Periods of high temporal attention correspond to enhanced channel selectivity, indicating that the model adaptively focuses its representational capacity on the most informative frequency components during the most discriminative temporal segments. This coordinated attention behavior reflects sophisticated understanding of the underlying signal structure and provides additional evidence for the effectiveness of the physics-informed architectural design.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{plots/attribution_examples.pdf}
\caption{Attribution maps (illustrative layout): banded subcarrier salience and localized temporal focus are consistent with propagation-informed expectations.}
\label{fig:attribution}
\end{figure}

\section{Discussion}
This study revisits CSI HAR with a physics-conscious objective: can a PINN-like Enhanced architecture, paired with calibrated inference and physics-guided synthesis, deliver robust cross-domain performance and label efficiency suitable for deployment? Our approach integrates CNN feature extractors, SE channel attention, and temporal attention, and evaluates not only accuracy but also probabilistic reliability. The results demonstrate identical LOSO/LORO macro-F1 at 83.0Â±0.1\% and 82.1\% macro-F1 at 20\% labeled data under STEA, indicating both robustness and practicality. We structure the discussion in four parts: relation to prior literature, unexpected observations, theoretical implications, and limitations with future work.

In relation to prior literature, our findings align with SenseFi~\cite{yang2023sensefi} in that attention-rich architectures tend to generalize better across CSI datasets than purely convolutional or recurrent baselines. The temporal-attention benefits we observe are consistent with sequence models developed for action recognition and forecasting~\cite{li2020tea,bertasius2021timesformer,lim2021tft,zhou2021informer}, while SE-based channel reweighting~\cite{se_networks2018} provides a natural inductive bias for subcarrier and antenna selection in multipath environments. Where we differ is the explicit, quantitative treatment of calibration in synthetic and cross-domain regimes: temperature scaling~\cite{calibration_guo2017} reduces NLL and ECE without sacrificing accuracy, offering a reliability dimension often missing in previous CSI studies. We also complement Sim2Real narratives inspired by robotics~\cite{peng2018sim2real} by demonstrating label-efficiency curves and diminishing returns beyond 20\% labels for Enhanced.

Some observations were not fully anticipated. First, progressive analyses revealed that Enhanced retained accuracy across coarser temporal settings with only marginal variance increases. This suggests that temporal attention can compensate for reduced temporal granularity by selectively aggregating informative segmentsâa useful property for low-power, bandwidth-limited IoT nodes. Second, nuisance-factor sweeps showed that Enhanced was comparatively resilient to combined class overlap and environmental burst, whereas baselines degraded more sharply. The implication is that channel-wise reweighting helps suppress spurious features that otherwise become confounding under stress. Third, attribution maps showed banded subcarrier salience and localized temporal focus; although illustrative, these patterns match propagation-informed expectations and provide interpretable checks for practitioners.

The results carry theoretical implications for architecture design under domain shift. Channel attention can be viewed as learning a data-adaptive approximation to subcarrier selection that mirrors physical salience; temporal attention provides a soft alignment over activity phases, mitigating the need for rigid sequence models. Together, these elements approximate a physics-conscious prior: they do not encode Maxwellâs equations, but they nudge the optimization landscape toward representations that remain useful across domains. We conjecture that marrying such inductive biases with explicit domain-aware calibration and selective classification could yield principled risk controls for CSI HAR and related RF sensing tasks.

Our work has limitations. We rely on post-hoc calibration rather than integrated, domain-aware calibration; extending the latter could improve reliability under severe shift. Interpretability examples, while consistent with intuition, would benefit from case studies using raw CSI tensors and controlled perturbations to rule out alternate explanations. Finally, although Enhanced shows strong synthetic and cross-domain behavior, complex multi-person activities and dynamic device placements will require richer generators and expanded benchmarks. Addressing these limitations constitutes our immediate roadmap.

\section{Conclusion}
We presented a PINN-inspired Enhanced architecture that combines CNN, SE, and temporal attention, demonstrating strong performance and improved calibration on synthetic robustness trials, with interpretable attribution patterns consistent with propagation insights. The results support a practical path toward reliable, explainable CSI sensing.

\bibliographystyle{IEEEtran}
\bibliography{enhanced_refs}

\end{document}