% !TEX program = pdflatex
\documentclass[10pt,conference]{IEEEtran}
\usepackage{times}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xcolor}
\hypersetup{colorlinks=true,linkcolor=black,citecolor=blue,urlcolor=blue}
\sisetup{round-mode=places,round-precision=2}

\newcommand{\model}{Enhanced}
\newcommand{\dataset}{CSI-Fall}

\begin{document}

\title{Trustworthy WiFi CSI Fall Detection via Physics-Guided Evaluation: Breaking Synthetic Ceilings, Crossing Domains, and Reducing Labels}

\author{
\IEEEauthorblockN{Anon Author(s)}
\IEEEauthorblockA{Affiliation \\
Email: anon@inst.edu}
}

\maketitle

\begin{abstract}
We revisit WiFi CSI fall detection with ordinary sequence models but elevate evaluation: a physics-controllable synthetic generator, strict cross-domain protocols (LOSO/LORO), trust calibration, and Sim2Real label-efficiency analysis. Our framework breaks the synthetic ceiling, quantifies causal links between difficulty factors (overlap, harmonics, noise) and errors, improves reliability under domain shift, and achieves $\geq$90--95\% of full-supervision with 10--20\% labels. Code, seeds, and splits are released for full reproducibility.
\end{abstract}

\begin{IEEEkeywords}
WiFi CSI, Fall Detection, Synthetic Data, Domain Shift, Calibration, Sim2Real
\end{IEEEkeywords}

\section{Introduction}

Channel State Information (CSI)-based sensing has emerged as a compelling paradigm for privacy-preserving fall detection, leveraging the inherent sensitivity of WiFi signals to environmental perturbations without requiring visual surveillance or wearable devices. Despite substantial research progress in this domain, contemporary approaches frequently suffer from methodological limitations that undermine their practical deployment and scientific rigor. Specifically, existing studies often demonstrate impressive performance on synthetic datasets while exhibiting significant degradation when evaluated across heterogeneous subjects, diverse environmental conditions, or different hardware configurations. This performance gap exposes a fundamental disconnect between controlled experimental conditions and real-world deployment scenarios.

The core challenge stems not from architectural complexity but from inadequate evaluation protocols that fail to capture the nuanced difficulty factors inherent in CSI-based sensing. Traditional evaluation frameworks typically rely on simplistic train-test splits that inadvertently introduce data leakage, overlook calibration properties essential for deployment, and neglect the systematic analysis of domain transfer mechanisms. Consequently, the field has witnessed an proliferation of increasingly sophisticated neural architectures without corresponding advances in evaluation rigor or deployment readiness.

To address these fundamental limitations, we propose a comprehensive physics-guided evaluation framework that prioritizes methodological transparency and trustworthy performance assessment over architectural novelty. Our approach centers on four interconnected pillars that collectively advance the state of CSI-based fall detection toward practical deployment. First, we introduce a physics-controllable synthetic analysis methodology that systematically exposes the causal relationships between environmental difficulty factors—including signal overlap, harmonic interference, noise corruption, and channel dropout—and model performance degradation (Fig.~\ref{fig:synth-bars}, \ref{fig:overlap-scatter}). This controllable framework enables researchers to break through synthetic performance ceilings by understanding the fundamental limitations imposed by physical signal properties.

Second, we establish rigorous cross-domain evaluation protocols incorporating Leave-One-Subject-Out (LOSO) and Leave-One-Room-Out (LORO) validation schemes, augmented with statistical significance testing and comprehensive trust calibration metrics (Tab.~\ref{tab:main-real}, Fig.~\ref{fig:reliability}). These protocols ensure that reported performance metrics accurately reflect model capabilities under realistic deployment conditions. Third, we introduce cost-aware operating point analysis that emphasizes robust performance in low false-positive-rate regimes critical for healthcare applications (Fig.~\ref{fig:cost-sensitive}). Finally, we demonstrate substantial label efficiency gains through systematic Sim2Real transfer learning, achieving 90-95\% of full-supervision performance using only 10-20\% of labeled real-world data (Fig.~\ref{fig:sim2real-curve}).

Our technical contribution centers on an Enhanced architecture incorporating lightweight confidence estimation through logit norm regularization, which consistently outperforms capacity-matched baselines including LSTM, Temporal Convolutional Networks (TCN), and compact Transformer variants in terms of both calibration quality and cross-domain robustness. Through comprehensive experimentation, we demonstrate that this physics-informed evaluation paradigm not only advances the scientific understanding of CSI-based sensing limitations but also provides a practical pathway toward trustworthy fall detection systems suitable for real-world deployment.

\section{Related Work}

\subsection{CSI-based Human Activity Recognition and Fall Detection}

The emergence of Channel State Information as a sensing modality has catalyzed extensive research in human activity recognition and fall detection systems. Early pioneering works demonstrated the feasibility of extracting meaningful activity patterns from CSI amplitude and phase variations, establishing foundational techniques for signal preprocessing and feature extraction. However, contemporary research has predominantly focused on architectural innovation and accuracy optimization while operating under constrained evaluation paradigms that inadequately reflect real-world deployment challenges.

Existing methodologies typically employ conventional train-test splits within homogeneous datasets, inadvertently introducing optimistic bias through temporal correlation and subject-specific signal characteristics. Furthermore, the majority of published studies prioritize peak accuracy metrics without considering the calibration properties essential for reliable decision-making in safety-critical applications. This evaluation approach has resulted in a substantial body of literature reporting impressive synthetic performance that fails to translate to robust cross-domain generalization, particularly when encountering unseen subjects, novel environmental configurations, or alternative hardware platforms.

\subsection{Synthetic Data Generation and Domain Transfer Analysis}

The utilization of synthetic data in CSI-based sensing has evolved from simple signal simulation to sophisticated physics-informed generation frameworks. Recent advances have incorporated realistic channel modeling, multipath propagation effects, and environmental perturbations to bridge the gap between controlled simulation and real-world complexity. Nevertheless, existing synthetic evaluation approaches typically lack systematic analysis of the causal relationships between controllable difficulty factors and model performance degradation.

Our methodology distinguishes itself through the introduction of physics-controllable synthetic analysis that enables systematic manipulation of signal overlap characteristics, harmonic interference patterns, noise corruption levels, and channel dropout probabilities. Unlike conventional approaches that treat synthetic data as a black-box augmentation strategy, our framework establishes quantitative linkages between these physically meaningful parameters and subsequent classification errors through rigorous statistical analysis. This approach enables researchers to systematically identify performance bottlenecks and understand the fundamental limitations imposed by physical signal properties rather than architectural choices.

\subsection{Model Calibration and Trustworthy Machine Learning}

The integration of calibration assessment and trustworthy evaluation metrics represents a critical yet underexplored dimension in CSI-based sensing research. Traditional approaches in this domain have primarily emphasized accuracy-oriented metrics while neglecting the confidence estimation capabilities essential for deployment in safety-sensitive applications. The healthcare context of fall detection systems necessitates reliable uncertainty quantification to enable appropriate clinical decision-making and minimize false alarm rates that could undermine system adoption.

Our evaluation framework advances beyond conventional accuracy assessment by incorporating comprehensive calibration analysis through Expected Calibration Error (ECE), Brier score decomposition, and reliability curve visualization. These metrics provide essential insights into model trustworthiness and enable systematic comparison of confidence estimation capabilities across different architectural choices. Additionally, we introduce cost-sensitive operating point analysis that emphasizes performance characteristics in low false-positive-rate regimes, addressing the practical deployment requirements of fall detection systems where false alarms carry significant operational costs.

\section{Methodology}

\subsection{Architecture Design and Capacity-Controlled Comparison Framework}

To ensure fair and meaningful comparative analysis, we establish a rigorous capacity-controlled evaluation framework that eliminates parameter count bias while enabling systematic assessment of architectural choices. Our comparison encompasses four distinct architectural paradigms: Long Short-Term Memory networks (LSTM), Temporal Convolutional Networks (TCN), compact Transformer variants (Tiny-Transformer), and our proposed Enhanced architecture. Critical to our methodology is the enforcement of parameter budgets constrained within $\pm$10\% variation across all compared models, ensuring that performance differences reflect architectural efficiency rather than raw computational capacity.

The Enhanced architecture integrates convolutional feature extraction with Squeeze-and-Excitation attention mechanisms and lightweight temporal modeling, specifically designed to capture the spatiotemporal patterns characteristic of CSI-based fall detection while maintaining computational efficiency. This design philosophy prioritizes interpretable feature learning and robust generalization over parameter-intensive approaches that may suffer from overfitting in the limited-data regimes typical of real-world CSI sensing applications.

\subsection{Physics-Informed Confidence Estimation and Regularization}

Recognizing that deployment-ready fall detection systems require reliable uncertainty quantification, we introduce a physics-informed confidence estimation mechanism through logit norm regularization. Our confidence prior is formulated as an augmented loss function:
$$\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{CE}}(z,y) + \lambda \cdot \frac{1}{B}\sum_{i=1}^{B} \|z_i\|_2^2$$
where $z_i$ represents the logit vector for sample $i$, $y$ denotes the ground truth labels, $B$ is the batch size, and $\lambda$ serves as the regularization strength hyperparameter.

This regularization strategy encourages the model to produce well-calibrated confidence estimates by penalizing overly confident predictions that may not align with true classification uncertainty. The hyperparameter $\lambda$ is systematically tuned through comprehensive grid search, and we explicitly report Pareto trade-offs between classification accuracy and calibration quality across the entire hyperparameter space. This approach enables practitioners to select operating points that align with their specific deployment requirements regarding the balance between detection sensitivity and false alarm tolerance.

\section{Evaluation Protocol}

\subsection{Physics-Controllable Synthetic Analysis Framework}

Our synthetic evaluation methodology introduces systematic controllability over key physical parameters that directly influence CSI signal characteristics and subsequent classification difficulty. Specifically, we manipulate four critical factors: signal overlap intensity, harmonic interference patterns, additive noise corruption levels, and channel dropout probabilities. This controllable framework enables systematic exploration of the performance landscape across varying difficulty regimes while maintaining statistical rigor through comprehensive significance testing.

The evaluation metrics encompass both aggregate performance indicators (Macro-F1 score) and fine-grained class-specific assessments (individual class F1 scores) to capture potential performance imbalances across activity categories. Additionally, we introduce mutual misclassification analysis to quantify systematic confusion patterns between specific activity pairs, providing insights into the fundamental perceptual similarities that challenge classification performance. Our regression analysis between overlap intensity and classification errors incorporates significance testing to establish causal relationships between controllable physical parameters and performance degradation, moving beyond correlation toward understanding mechanistic relationships.

\subsection{Cross-Domain Real-World Evaluation: LOSO and LORO Protocols}

Our real-world evaluation framework implements rigorous cross-domain validation through Leave-One-Subject-Out (LOSO) and Leave-One-Room-Out (LORO) protocols that eliminate data leakage while providing realistic estimates of generalization performance. We standardize data splits across all compared methods and enforce strict temporal boundaries to prevent information leakage from future time points during training. Statistical robustness is ensured through bootstrap-based confidence interval computation (95\% CI), paired $t$-test significance testing, and effect size quantification using Cohen's $d$ to assess practical significance beyond statistical significance.

These protocols address the critical limitation in existing CSI-based sensing literature where optimistic evaluation schemes fail to capture the substantial performance degradation encountered in real deployment scenarios. By enforcing subject-wise and location-wise independence, our evaluation framework provides conservative yet realistic performance estimates that better reflect system capabilities under operational conditions.

\subsection{Calibration Assessment and Deployment-Ready Operating Points}

Recognizing the safety-critical nature of fall detection applications, our evaluation framework incorporates comprehensive calibration assessment through multiple complementary metrics. Expected Calibration Error (ECE) and Brier score decomposition provide quantitative measures of confidence estimation quality, while reliability curve visualization enables intuitive assessment of calibration characteristics across different confidence regimes.

Furthermore, we introduce deployment-focused performance analysis through fixed false-positive-rate (FPR) operating points that emphasize true positive rate (TPR) characteristics in low-FPR regimes critical for healthcare applications. This analysis framework enables practitioners to assess model suitability for specific deployment scenarios where false alarm tolerance varies significantly based on operational constraints and clinical requirements.

\subsection{Systematic Sim2Real Transfer Learning and Label Efficiency Analysis}

Our Sim2Real evaluation protocol systematically assesses the label efficiency gains achievable through synthetic pretraining followed by fine-tuning on varying fractions of real-world labeled data. We evaluate performance across label fractions $p \in \{1, 5, 10, 25, 100\}\%$ to characterize the entire efficiency curve from extremely limited labeling scenarios to full supervision. Additionally, we conduct linear probe evaluation by freezing the pretrained encoder and training only the classification head, providing insights into the quality of learned representations independent of fine-tuning dynamics.

This comprehensive approach enables quantification of the practical value proposition of synthetic data generation, moving beyond qualitative claims toward precise characterization of label efficiency gains under realistic deployment constraints where annotation costs represent significant operational barriers.

\section{Experimental Evaluation}

\subsection{Dataset Characteristics and Implementation Specifications}

Our experimental evaluation leverages a sophisticated synthetic data generation framework (version 19.2) that incorporates physics-based channel modeling and realistic environmental perturbations to create controllable yet representative CSI signals. The synthetic generator enables systematic manipulation of key difficulty factors while maintaining signal fidelity characteristics consistent with real-world WiFi deployments. Comprehensive statistics regarding real-world dataset characteristics, including subject demographics, environmental configurations, and signal quality metrics, are provided in the supplementary materials to ensure reproducibility and enable comparative analysis with future research.

Implementation consistency across all experimental configurations is maintained through standardized training protocols incorporating batch size of 64 samples, Adam optimization with initial learning rate of $10^{-3}$, cosine annealing learning rate scheduling, and early stopping based on validation performance plateau. Statistical robustness is ensured through evaluation across 8 independent random seeds unless explicitly noted otherwise, with all reported metrics representing mean performance and associated standard deviations across these replicated trials.

\subsection{Synthetic: Breaking the ceiling}
\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{../plots/fig_synth_bars.pdf}
  \caption{Synthetic InD results: Falling/Macro F1 and mutual misclassification across models (mean$\pm$std).}
  \label{fig:synth-bars}
\end{figure}
\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{../plots/fig_overlap_scatter.pdf}
  \caption{Overlap vs. mutual misclassification: regression slope and $p$-value indicate causal linkage.}
  \label{fig:overlap-scatter}
\end{figure}

\subsection{Real-world LOSO/LORO main results}
\input{../tables/tab_main_real.tex}
%\label{tab:main-real}

\subsection{Calibration and reliability}
\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{../plots/fig_reliability_enhanced_vs_baselines.pdf}
  \caption{Reliability curves. \model{} is closer to the diagonal; ECE/Brier improve over baselines.}
  \label{fig:reliability}
\end{figure}
\input{../tables/tab_calibration_real.tex}

\subsection{Bucketed robustness and cost-sensitive analysis}
\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{../plots/fig_bucket_perf_curves.pdf}
  \caption{Performance vs. difficulty buckets (overlap/noise/domain). \model{} degrades more gracefully.}
  \label{fig:bucket}
\end{figure}
\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{../plots/fig_cost_sensitive.pdf}
  \caption{Fixed-FPR TPR and cost curves in low-FPR regimes.}
  \label{fig:cost-sensitive}
\end{figure}

\subsection{Sim2Real label efficiency and linear probe}
\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{../plots/fig_sim2real_curve.pdf}
  \caption{Label efficiency: pretraining on synthetic reduces labels to reach $\geq$90--95\% of full supervision.}
  \label{fig:sim2real-curve}
\end{figure}
\input{../tables/tab_sim2real_gain.tex}
\input{../tables/tab_linear_probe.tex}

\subsection{Ablation and fairness}
\input{../tables/tab_capacity_match.tex}

\section{Discussion}

Our experimental findings illuminate several critical insights that challenge prevailing assumptions in CSI-based fall detection research while establishing new methodological standards for the field. The physics-guided evaluation framework demonstrates that genuine advancement in this domain stems not primarily from architectural sophistication but from rigorous evaluation protocols that accurately capture real-world deployment challenges. Our Enhanced model, despite employing relatively conventional architectural components, consistently outperforms more complex baselines when subjected to our comprehensive evaluation framework, highlighting the paramount importance of trustworthy assessment methodologies.

The systematic analysis of synthetic-to-real performance gaps reveals fundamental limitations in current evaluation practices that have inadvertently inflated reported performance metrics throughout the literature. Our controllable synthetic framework exposes the causal mechanisms underlying performance degradation, demonstrating that specific physical parameters—particularly signal overlap characteristics and harmonic interference patterns—exhibit statistically significant correlations with classification errors. This mechanistic understanding enables targeted improvement strategies that address root causes rather than symptoms of performance limitations.

Furthermore, our calibration analysis reveals substantial miscalibration in existing approaches, with confidence estimates poorly aligned with actual classification accuracy. The Enhanced model's superior calibration characteristics, achieved through physics-informed regularization, provide essential foundations for deployment in safety-critical applications where overconfident predictions could result in severe consequences. The label efficiency analysis demonstrates compelling practical value, showing that strategic synthetic pretraining can substantially reduce annotation requirements while maintaining performance levels comparable to full supervision \cite{fernandez2024wavelet}.

\section{Conclusion and Future Directions}

This work establishes a new paradigm for trustworthy evaluation in CSI-based fall detection through a comprehensive physics-guided framework that prioritizes methodological rigor over architectural complexity. Our reproducible evaluation pipeline successfully breaks through synthetic performance ceilings by exposing the fundamental relationships between controllable physical parameters and classification difficulty, while simultaneously improving model calibration and enabling substantial label efficiency gains through systematic Sim2Real transfer learning.

The demonstrated effectiveness of relatively simple architectural approaches when subjected to rigorous evaluation protocols suggests that the field's emphasis on increasingly complex models may have obscured more fundamental methodological limitations. Our framework provides both immediate practical value through improved calibration and label efficiency, and long-term scientific value through enhanced reproducibility and systematic difficulty analysis.

Future research directions include extending the physics-controllable framework to encompass additional environmental factors such as furniture placement and subject demographic variations, developing adaptive calibration techniques that can automatically adjust to deployment-specific conditions, and investigating the generalizability of our evaluation framework to other CSI-based sensing applications beyond fall detection. All experimental assets including source code, random seeds, standardized data splits, and trained model checkpoints will be made publicly available to facilitate reproducible research and enable direct comparison with future methodological advances.

\bibliographystyle{IEEEtran}
\bibliography{refs}
\end{document}