{
  "extraction_date": "2024-08-24",
  "data_source": "prisma_data.csv - user classified data",
  "total_relevant_papers": 159,
  "papers_with_algorithm_info": 159,
  "papers_with_performance_data": 10,
  "algorithm_classification": {
    "Faster R-CNN": [
      {
        "title": "DeepFruits: A Fruit Detection System Using Deep Neural Networks",
        "year": "2016",
        "algorithm": "Faster R-CNN\ntransfer learning",
        "performance": "Recall performances improving from 0.807 to 0.838 for the detection of sweet pepper.",
        "citation": "@article{sa2016deepfruits,\n  title={Deepfruits: A fruit detection system using deep neural networks},\n  author={Sa, Inkyu and Ge, Zongyuan and Dayoub, Feras and Upcroft, Ben and Perez, Tristan and McCool, Chris},\n  journal={sensors},\n  volume={16},\n  number={8},\n  pages={1222},\n  year={2016},\n  publisher={MDPI}\n}\n",
        "fruit_veg": "sweet pepper",
        "paper": {
          "relevant": "y",
          "": "\\cite{sa2016deepfruits}",
          "Article Title": "DeepFruits: A Fruit Detection System Using Deep Neural Networks",
          "Times Cited, All Databases": "662",
          "Publication Year": "2016",
          "Highly Cited Status": "Y",
          "Publisher": "sensors",
          "Citation": "@article{sa2016deepfruits,\n  title={Deepfruits: A fruit detection system using deep neural networks},\n  author={Sa, Inkyu and Ge, Zongyuan and Dayoub, Feras and Upcroft, Ben and Perez, Tristan and McCool, Chris},\n  journal={sensors},\n  volume={16},\n  number={8},\n  pages={1222},\n  year={2016},\n  publisher={MDPI}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "",
          "fruit/veg": "sweet pepper",
          "Data Modality": "RGB,NIR",
          "Learning Algorithm": "Faster R-CNN\ntransfer learning",
          "Locomotion": "",
          "Performance": "Recall performances improving from 0.807 to 0.838 for the detection of sweet pepper.",
          "challenges": "",
          "Authors": "Sa, I; Ge, ZY; Dayoub, F; Upcroft, B; Perez, T; McCool, C",
          "Abstract": "This paper presents a novel approach to fruit detection using deep convolutional neural networks. The aim is to build an accurate, fast and reliable fruit detection system, which is a vital element of an autonomous agricultural robotic platform; it is a key element for fruit yield estimation and automated harvesting. Recent work in deep neural networks has led to the development of a state-of-the-art object detector termed Faster Region-based CNN (Faster R-CNN). We adapt this model, through transfer learning, for the task of fruit detection using imagery obtained from two modalities: colour (RGB) and Near-Infrared (NIR). Early and late fusion methods are explored for combining the multi-modal (RGB and NIR) information. This leads to a novel multi-modal Faster R-CNN model, which achieves state-of-the-art results compared to prior work with the Fl score, which takes into account both precision and recall performances improving from 0.807 to 0.838 for the detection of sweet pepper. In addition to improved accuracy, this approach is also much quicker to deploy for new fruits, as it requires bounding box annotation rather than pixel-level annotation (annotating bounding boxes is approximately an order of magnitude quicker to perform). The model is retrained to perform the detection of seven fruits, with the entire process taking four hours to annotate and train the new model per fruit.",
          "Keywords Plus": "CLASSIFICATION; SCALE"
        }
      },
      {
        "title": "Faster R-CNN for multi-class fruit detection using a robotic vision system",
        "year": "2020",
        "algorithm": "Faster R-CNN\ntransfer learning",
        "performance": "",
        "citation": "@article{wan2020faster,\n  title={Faster R-CNN for multi-class fruit detection using a robotic vision system},\n  author={Wan, Shaohua and Goudos, Sotirios},\n  journal={Computer Networks},\n  volume={168},\n  pages={107036},\n  year={2020},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{wan2020faster}",
          "Article Title": "Faster R-CNN for multi-class fruit detection using a robotic vision system",
          "Times Cited, All Databases": "258",
          "Publication Year": "2020",
          "Highly Cited Status": "Y",
          "Publisher": "Computer Networks",
          "Citation": "@article{wan2020faster,\n  title={Faster R-CNN for multi-class fruit detection using a robotic vision system},\n  author={Wan, Shaohua and Goudos, Sotirios},\n  journal={Computer Networks},\n  volume={168},\n  pages={107036},\n  year={2020},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "An accurate and real-time image based multi-class fruit detection system",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "Faster R-CNN\ntransfer learning",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Wan, SH; Goudos, S",
          "Abstract": "An accurate and real-time image based multi-class fruit detection system is important for facilitating higher level smart farm tasks such as yield mapping and robotic harvesting. Robotic harvesting can reduce the costs of labour and increase fruit quality. This paper proposes a deep learning framework for multi-class fruits detection based on improved Faster R-CNN. The proposed framework includes fruits image library creation, data argumentation, improved Faster RCNN model generation, and performance evaluation. This work is a pioneer to create a multi-labeled and knowledge-based outdoor orchard image library using 4000 images in the real world. Also, improvement of the convolutional and pooling layers is achieved to have a more accurate and faster detection. The test results show the proposed algorithm has achieved higher detecting accuracy and lower processing time than the traditional detectors, which has excellent potential to build an autonomous and real-time harvesting or yield mapping/estimation system. (C) 2019 Elsevier B.V. All rights reserved.",
          "Keywords Plus": "IMAGES"
        }
      },
      {
        "title": "Faster R-CNN-based apple detection in dense-foliage fruiting-wall trees using RGB and depth features for robotic harvesting",
        "year": "2020",
        "algorithm": "Faster R-CNN",
        "performance": "a precision of 0.893",
        "citation": "@article{fu2020faster,\n  title={Faster R--CNN--based apple detection in dense-foliage fruiting-wall trees using RGB and depth features for robotic harvesting},\n  author={Fu, Longsheng and Majeed, Yaqoob and Zhang, Xin and Karkee, Manoj and Zhang, Qin},\n  journal={Biosystems Engineering},\n  volume={197},\n  pages={245--256},\n  year={2020},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "apples",
        "paper": {
          "relevant": "y",
          "": "\\cite{fu2020faster}",
          "Article Title": "Faster R-CNN-based apple detection in dense-foliage fruiting-wall trees using RGB and depth features for robotic harvesting",
          "Times Cited, All Databases": "124",
          "Publication Year": "2020",
          "Highly Cited Status": "Y",
          "Publisher": "N",
          "Citation": "@article{fu2020faster,\n  title={Faster R--CNN--based apple detection in dense-foliage fruiting-wall trees using RGB and depth features for robotic harvesting},\n  author={Fu, Longsheng and Majeed, Yaqoob and Zhang, Xin and Karkee, Manoj and Zhang, Qin},\n  journal={Biosystems Engineering},\n  volume={197},\n  pages={245--256},\n  year={2020},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "",
          "fruit/veg": "apples",
          "Data Modality": "Kinect V2 sensor",
          "Learning Algorithm": "Faster R-CNN",
          "Locomotion": "",
          "Performance": "a precision of 0.893",
          "challenges": "",
          "Authors": "Fu, LS; Majeed, Y; Zhang, X; Karkee, M; Zhang, Q",
          "Abstract": "Apples in modern orchards with vertical-fruiting-wall trees are comparatively easier to harvest and specifically suitable for robotic picking, where accurate apple detection and obstacle-free access are fundamentally important. However, field images have complex backgrounds because of the presence of nontarget trees and fruit in adjacent rows. An outdoor machine vision system was developed with a low-cost Kinect V2 sensor to improve the accuracy of apple detection by filtering the background objects using depth features. A total of 800 set images were acquired in a commercial fruiting-wall Scifresh apple orchard with dense-foliage canopy. Images were collected in both daytime and nighttime with artificial light. The sensor was kept at 0.5 m to the tree canopies. A depth threshold of 1.2 m was used to remove background. Two Faster ReCNN based architectures ZFNet and VGG16 were employed to detect the Original-RGB and the Foreground-RGB images. Results showed that the highest average precision (AP) of 0.893 was achieved for the Foreground-RGB images with VGG16, which cost 0.181 s on average to process a 1920 x 1080 image. AP values for the Foreground-RGB images with ZFNet and VGG16 were both higher than those of the Original-RGB images. The results indicated that the use of a depth filter to remove background trees improved fruit detection accuracy by 2.5% and that only a minimal difference was found in processing speed between two image datasets. The proposed technique and results are expected to be applicable for robotic harvesting on fruiting-wall apple orchards. (C) 2020 IAgrE. Published by Elsevier Ltd. All rights reserved.",
          "Keywords Plus": "SEGMENTATION; NETWORKS; LOCATION; SENSORS; IMAGES; COLOR"
        }
      },
      {
        "title": "Passion fruit detection and counting based on multiple scale faster R-CNN using RGB-D images",
        "year": "2020",
        "algorithm": "Faster R-CNN",
        "performance": "",
        "citation": "@article{tu2020passion,\n  title={Passion fruit detection and counting based on multiple scale faster R-CNN using RGB-D images},\n  author={Tu, Shuqin and Pang, Jing and Liu, Haofeng and Zhuang, Nan and Chen, Yong and Zheng, Chan and Wan, Hua and Xue, Yueju},\n  journal={Precision Agriculture},\n  volume={21},\n  pages={1072--1091},\n  year={2020},\n  publisher={Springer}\n}",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{tu2020passion}",
          "Article Title": "Passion fruit detection and counting based on multiple scale faster R-CNN using RGB-D images",
          "Times Cited, All Databases": "89",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{tu2020passion,\n  title={Passion fruit detection and counting based on multiple scale faster R-CNN using RGB-D images},\n  author={Tu, Shuqin and Pang, Jing and Liu, Haofeng and Zhuang, Nan and Chen, Yong and Zheng, Chan and Wan, Hua and Xue, Yueju},\n  journal={Precision Agriculture},\n  volume={21},\n  pages={1072--1091},\n  year={2020},\n  publisher={Springer}\n}",
          "Document Type": "Article",
          "Main Contribution": "detection count",
          "fruit/veg": "",
          "Data Modality": "RGB-D",
          "Learning Algorithm": "Faster R-CNN",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Tu, SQ; Pang, J; Liu, HF; Zhuang, N; Chen, Y; Zheng, C; Wan, H; Xue, YJ",
          "Abstract": "The accurate and reliable fruit detection in orchards is one of the most crucial tasks for supporting higher level agriculture tasks such as yield mapping and robotic harvesting. However, detecting and counting small fruit is a very challenging task under variable lighting conditions, low-resolutions and heavy occlusion by neighboring fruits or foliage. To robustly detect small fruits, an improved method is proposed based on multiple scale faster region-based convolutional neural networks (MS-FRCNN) approach using the color and depth images acquired with an RGB-D camera. The architecture of MS-FRCNN is improved to detect lower-level features by incorporating feature maps from shallower convolution feature maps for regions of interest (ROI) pooling. The detection framework consists of three phases. Firstly, multiple scale feature extractors are used to extract low and high features from RGB and depth images respectively. Then, RGB-detector and depth-detector are trained separately using MS-FRCNN. Finally, late fusion methods are explored for combining the RGB and depth detector. The detection framework was demonstrated and evaluated on two datasets that include passion fruit images under variable illumination conditions and occlusion. Compared with the faster R-CNN detector of RGB-D images, the recall, the precision and F1-score of MS-FRCNN method increased from 0.922 to 0.962, 0.850 to 0.931 and 0.885 to 0.946, respectively. Furthermore, the MS-FRCNN method effectively improves small passion fruit detection by achieving 0.909 of the F1 score. It is concluded that the detector based on MS-FRCNN can be applied practically in the actual orchard environment.",
          "Keywords Plus": "RECOGNITION"
        }
      },
      {
        "title": "Kiwifruit detection in field images using Faster R-CNN with ZFNet",
        "year": "2018",
        "algorithm": "Faster R-CNN with ZFNET",
        "performance": "",
        "citation": "@article{fu2018kiwifruit,\n  title={Kiwifruit detection in field images using Faster R-CNN with ZFNet},\n  author={Fu, Longsheng and Feng, Yali and Majeed, Yaqoob and Zhang, Xin and Zhang, Jing and Karkee, Manoj and Zhang, Qin},\n  journal={IFAC-PapersOnLine},\n  volume={51},\n  number={17},\n  pages={45--50},\n  year={2018},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "kiwifruit",
        "paper": {
          "relevant": "y",
          "": "\\cite{fu2018kiwifruit}",
          "Article Title": "Kiwifruit detection in field images using Faster R-CNN with ZFNet",
          "Times Cited, All Databases": "81",
          "Publication Year": "2018",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{fu2018kiwifruit,\n  title={Kiwifruit detection in field images using Faster R-CNN with ZFNet},\n  author={Fu, Longsheng and Feng, Yali and Majeed, Yaqoob and Zhang, Xin and Zhang, Jing and Karkee, Manoj and Zhang, Qin},\n  journal={IFAC-PapersOnLine},\n  volume={51},\n  number={17},\n  pages={45--50},\n  year={2018},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Proceedings Paper",
          "Main Contribution": "detection",
          "fruit/veg": "kiwifruit",
          "Data Modality": "",
          "Learning Algorithm": "Faster R-CNN with ZFNET",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Fu, LS; Feng, YL; Majeed, Y; Zhang, X; Zhang, J; Karkee, M; Zhang, Q",
          "Abstract": "A kiwifruit detection system for field images was developed based on the deep convolutional neural network, which has a good robustness against the subjectivity and limitation of the features selected artificially. Under different lighting conditions, 2,100 sub-images with 784x784 pixels were prepared by random sub-sampling from 700 field captured images with a pixel resolution of 2352x1568 pixels. Sub-images were used as network training and validation samples. A faster R-CNN was trained end-to-end by using back-propagation and stochastic gradient descent techniques with Zeiler and Fergus network (ZFNet). The average precision of the Faster R-CNN-based kiwifruit detector was 89.3%. Finally, another 100 images of kiwifruit canopies in the field environment (including 5,918 fruits) were used for testing the network. The test results showed that the recognition ratio of occluded fruit, overlapping fruit, adjacent fruit and separated fruit were 82.5%, 85.6%, 94.3% and 96.7%, respectively. Overall, the model reached a recognition rate of 92.3%. The technique took 0.274 s to process each image (for images with 2352x1568 pixels) and only 5.0 ms on average to detect a fruit. Comparing against the conventional methods, it suggested that the proposed method has higher recognition rate and faster speed. Especially, the proposed technique was able to simultaneously detect individual kiwifruit in clusters, which provides a promise for accurate yield mapping and multi-arm robotic harvesting. (C) 2018, IFAC (International Federation of Automatic Control) Hosting by Elsevier Ltd. All rights reserved.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Faster R¨CCNN¨Cbased apple detection in dense-foliage fruiting-wall trees using RGB and depth features for robotic harvesting",
        "year": "2020",
        "algorithm": "Faster R-CNN",
        "performance": "",
        "citation": "@article{fu2020faster,\n  title={Faster R--CNN--based apple detection in dense-foliage fruiting-wall trees using RGB and depth features for robotic harvesting},\n  author={Fu, Longsheng and Majeed, Yaqoob and Zhang, Xin and Karkee, Manoj and Zhang, Qin},\n  journal={Biosystems Engineering},\n  volume={197},\n  pages={245--256},\n  year={2020},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "apples",
        "paper": {
          "relevant": "y",
          "": "\\cite{fu2020faster}",
          "Article Title": "Faster R¨CCNN¨Cbased apple detection in dense-foliage fruiting-wall trees using RGB and depth features for robotic harvesting",
          "Times Cited, All Databases": "132",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{fu2020faster,\n  title={Faster R--CNN--based apple detection in dense-foliage fruiting-wall trees using RGB and depth features for robotic harvesting},\n  author={Fu, Longsheng and Majeed, Yaqoob and Zhang, Xin and Karkee, Manoj and Zhang, Qin},\n  journal={Biosystems Engineering},\n  volume={197},\n  pages={245--256},\n  year={2020},\n  publisher={Elsevier}\n}\n",
          "Document Type": "",
          "Main Contribution": "",
          "fruit/veg": "apples",
          "Data Modality": "RGB-D",
          "Learning Algorithm": "Faster R-CNN",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "Apples in modern orchards with vertical-fruiting-wall trees are comparatively easier to harvest and specifically suitable for robotic picking, where accurate apple detection and obstacle-free access are fundamentally important. However, field images have complex backgrounds because of the presence of nontarget trees and fruit in adjacent rows. An outdoor machine vision system was developed with a low-cost Kinect V2 sensor to improve the accuracy of apple detection by filtering the background objects using depth features. A total of 800 set images were acquired in a commercial fruiting-wall Scifresh apple orchard with dense-foliage canopy. Images were collected in both daytime and nighttime with artificial light. The sensor was kept at 0.5 m to the tree canopies. A depth threshold of 1.2 m was used to remove background. Two Faster R¨CCNN based architectures ZFNet and VGG16 were employed to detect the Original-RGB and the Foreground-RGB images. Results showed that the highest average precision (AP) of 0.893 was achieved for the Foreground-RGB images with VGG16, which cost 0.181 s on average to process a 1920 ¡Á 1080 image. AP values for the Foreground-RGB images with ZFNet and VGG16 were both higher than those of the Original-RGB images. The results indicated that the use of a depth filter to remove background trees improved fruit detection accuracy by 2.5% and that only a minimal difference was found in processing speed between two image datasets. The proposed technique and results are expected to be applicable for robotic harvesting on fruiting-wall apple orchards. ? 2020 IAgrE",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Multi-class fruit-on-plant detection for apple in SNAP system using Faster R-CNN",
        "year": "2020",
        "algorithm": "Faster R-CNN",
        "performance": "",
        "citation": "@article{gao2020multi,\n  title={Multi-class fruit-on-plant detection for apple in SNAP system using Faster R-CNN},\n  author={Gao, Fangfang and Fu, Longsheng and Zhang, Xin and Majeed, Yaqoob and Li, Rui and Karkee, Manoj and Zhang, Qin},\n  journal={Computers and Electronics in Agriculture},\n  volume={176},\n  pages={105634},\n  year={2020},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "apples",
        "paper": {
          "relevant": "y",
          "": "\\cite{gao2020multi}",
          "Article Title": "Multi-class fruit-on-plant detection for apple in SNAP system using Faster R-CNN",
          "Times Cited, All Databases": "196",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{gao2020multi,\n  title={Multi-class fruit-on-plant detection for apple in SNAP system using Faster R-CNN},\n  author={Gao, Fangfang and Fu, Longsheng and Zhang, Xin and Majeed, Yaqoob and Li, Rui and Karkee, Manoj and Zhang, Qin},\n  journal={Computers and Electronics in Agriculture},\n  volume={176},\n  pages={105634},\n  year={2020},\n  publisher={Elsevier}\n}\n",
          "Document Type": "",
          "Main Contribution": "",
          "fruit/veg": "apples",
          "Data Modality": "",
          "Learning Algorithm": "Faster R-CNN",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "Deep learning achieved high success of fruit-on-plant detection such as on apple. Most of studies on apple detection identified all target fruits as one class regardless of fruit condition and other canopy objects. However, some detected fruits were physically occluded by branches or trellis wires that could diminish the effectiveness of fruit picking and even damage the end-effector, especially when high-vigor rootstock apple cultivar was used. A multi-class apple detection method in dense-foliage fruiting-wall trees was thus proposed based on Faster Region-Convolutional Neural Network. It detected apples in different conditions such as non-occluded, leaf-occluded, branch/wire-occluded, and fruit-occluded fruit. A total of 800 images were acquired and then augmented to 12,800 images. Average precision of non-occluded, leaf-occluded, branch/wire-occluded, and fruit-occluded fruit were 0.909, 0.899, 0.858, and 0.848, respectively. Overall, the mean average precision of the four classes was 0.879, and an average of 0.241 s was needed to process an image. The results indicated that all the apples in different classes could be effectively detected, which can help the robot to decide the picking strategy (e.g., picking order and path planning) as well as to avoid the potential damage by the branches and trellis wires. ? 2020 Elsevier B.V.",
          "Keywords Plus": ""
        }
      }
    ],
    "Other": [
      {
        "title": "Harvesting Robots for High-value Crops: State-of-the-art Review and Challenges Ahead",
        "year": "2014",
        "algorithm": "",
        "performance": "localization success was 85%, detachment success was 75%, harvest success was 66%, fruit damage was 5%, peduncle damage was 45%, and cycle time was 33 s",
        "citation": "@article{bac2014harvesting,\n  title={Harvesting robots for high-value crops: State-of-the-art review and challenges ahead},\n  author={Bac, C Wouter and Van Henten, Eldert J and Hemming, Jochen and Edan, Yael},\n  journal={Journal of field robotics},\n  volume={31},\n  number={6},\n  pages={888--911},\n  year={2014},\n  publisher={Wiley Online Library}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{bac2014harvesting}",
          "Article Title": "Harvesting Robots for High-value Crops: State-of-the-art Review and Challenges Ahead",
          "Times Cited, All Databases": "388",
          "Publication Year": "2014",
          "Highly Cited Status": "Y",
          "Publisher": "Journal of field robotics",
          "Citation": "@article{bac2014harvesting,\n  title={Harvesting robots for high-value crops: State-of-the-art review and challenges ahead},\n  author={Bac, C Wouter and Van Henten, Eldert J and Hemming, Jochen and Edan, Yael},\n  journal={Journal of field robotics},\n  volume={31},\n  number={6},\n  pages={888--911},\n  year={2014},\n  publisher={Wiley Online Library}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "harvesting robots using quantitative measures",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "localization success was 85%, detachment success was 75%, harvest success was 66%, fruit damage was 5%, peduncle damage was 45%, and cycle time was 33 s",
          "challenges": "",
          "Authors": "Bac, CW; van Henten, EJ; Hemming, J; Edan, Y",
          "Abstract": "This review article analyzes state-of-the-art and future perspectives for harvesting robots in high-value crops. The objectives were to characterize the crop environment relevant for robotic harvesting, to perform a literature review on the state-of-the-art of harvesting robots using quantitative measures, and to reflect on the crop environment and literature review to formulate challenges and directions for future research and development. Harvesting robots were reviewed regarding the crop harvested in a production environment, performance indicators, design process techniques used, hardware design decisions, and algorithm characteristics. On average, localization success was 85%, detachment success was 75%, harvest success was 66%, fruit damage was 5%, peduncle damage was 45%, and cycle time was 33 s. A kiwi harvesting robot achieved the shortest cycle time of 1 s. Moreover, the performance of harvesting robots did not improve in the past three decades, and none of these 50 robots was commercialized. Four future challenges with R&D directions were identified to realize a positive trend in performance and to successfully implement harvesting robots in practice: (1) simplifying the task, (2) enhancing the robot, (3) defining requirements and measuring performance, and (4) considering additional requirements for successful implementation. This review article may provide new directions for future automation projects in high-value crops. C (C) 2014 Wiley Periodicals, Inc.",
          "Keywords Plus": "AGRICULTURAL ROBOT; ECONOMIC-ANALYSIS; AUTONOMOUS ROBOT; COMPUTER VISION; MOBILE ROBOTS; FRUIT; DESIGN; PICKING; SYSTEM; CITRUS"
        }
      },
      {
        "title": "Sensors and systems for fruit detection and localization: A review",
        "year": "2015",
        "algorithm": "",
        "performance": "",
        "citation": "@article{gongal2015sensors,\n  title={Sensors and systems for fruit detection and localization: A review},\n  author={Gongal, Aleana and Amatya, Suraj and Karkee, Manoj and Zhang, Qin and Lewis, Karen},\n  journal={Computers and Electronics in Agriculture},\n  volume={116},\n  pages={8--19},\n  year={2015},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "apples,pears,and citrus",
        "paper": {
          "relevant": "y",
          "": "\\cite{gongal2015sensors}",
          "Article Title": "Sensors and systems for fruit detection and localization: A review",
          "Times Cited, All Databases": "364",
          "Publication Year": "2015",
          "Highly Cited Status": "Y",
          "Publisher": "Computers and Electronics in Agriculture",
          "Citation": "@article{gongal2015sensors,\n  title={Sensors and systems for fruit detection and localization: A review},\n  author={Gongal, Aleana and Amatya, Suraj and Karkee, Manoj and Zhang, Qin and Lewis, Karen},\n  journal={Computers and Electronics in Agriculture},\n  volume={116},\n  pages={8--19},\n  year={2015},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "machine vision systems for fruit detection and localization",
          "fruit/veg": "apples,pears,and citrus",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "Variable lighting condition, occlusions, and clustering",
          "Authors": "Gongal, A; Amatya, S; Karkee, M; Zhang, Q; Lewis, K",
          "Abstract": "This paper reviews the research and development of machine vision systems for fruit detection and localization for robotic harvesting and/or crop-load estimation of specialty tree crops including apples, pears, and citrus. Variable lighting condition, occlusions, and clustering are some of the important issues needed to be addressed for accurate detection and localization of fruit in orchard environment. To address these issues, various techniques have been investigated using different types of sensors and their combinations as well as with different image processing techniques. This paper summarizes various techniques and their advantages and disadvantages in detecting fruit in plant or tree canopies. The paper also summarizes the sensors and systems developed and used by researchers to localize fruit as well as the potential and limitations of those systems. Finally, major challenges for the successful application of machine vision system for robotic fruit harvesting and crop-load estimation, and potential future directions for research and development are discussed. (C) 2015 Elsevier B.V. All rights reserved.",
          "Keywords Plus": "IMAGE-ANALYSIS; AUTOMATIC RECOGNITION; APPLE FRUITS; GREEN APPLES; COLOR; MACHINE; VISION; SEGMENTATION; FEATURES; NUMBER"
        }
      },
      {
        "title": "Deep Count: Fruit Counting Based on Deep Simulated Learning",
        "year": "2017",
        "algorithm": "Inception-ResNet",
        "performance": "a 91% average test accuracy on real images and 93% on synthetic images.",
        "citation": "@article{rahnemoonfar2017deep,\n  title={Deep count: fruit counting based on deep simulated learning},\n  author={Rahnemoonfar, Maryam and Sheppard, Clay},\n  journal={Sensors},\n  volume={17},\n  number={4},\n  pages={905},\n  year={2017},\n  publisher={MDPI}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{rahnemoonfar2017deep}",
          "Article Title": "Deep Count: Fruit Counting Based on Deep Simulated Learning",
          "Times Cited, All Databases": "332",
          "Publication Year": "2017",
          "Highly Cited Status": "Y",
          "Publisher": "sensors",
          "Citation": "@article{rahnemoonfar2017deep,\n  title={Deep count: fruit counting based on deep simulated learning},\n  author={Rahnemoonfar, Maryam and Sheppard, Clay},\n  journal={Sensors},\n  volume={17},\n  number={4},\n  pages={905},\n  year={2017},\n  publisher={MDPI}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "vision:yield estimation",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": " Inception-ResNet",
          "Locomotion": "",
          "Performance": "a 91% average test accuracy on real images and 93% on synthetic images.",
          "challenges": "fruits are under shadow, occluded by foliage, branches, or if there is some degree of overlap amongst fruits.",
          "Authors": "Rahnemoonfar, M; Sheppard, C",
          "Abstract": "Recent years have witnessed significant advancement in computer vision research based on deep learning. Success of these tasks largely depends on the availability of a large amount of training samples. Labeling the training samples is an expensive process. In this paper, we present a simulated deep convolutional neural network for yield estimation. Knowing the exact number of fruits, flowers, and trees helps farmers to make better decisions on cultivation practices, plant disease prevention, and the size of harvest labor force. The current practice of yield estimation based on the manual counting of fruits or flowers by workers is a very time consuming and expensive process and it is not practical for big fields. Automatic yield estimation based on robotic agriculture provides a viable solution in this regard. Our network is trained entirely on synthetic data and tested on real data. To capture features on multiple scales, we used a modified version of the Inception-ResNet architecture. Our algorithm counts efficiently even if fruits are under shadow, occluded by foliage, branches, or if there is some degree of overlap amongst fruits. Experimental results show a 91% average test accuracy on real images and 93% on synthetic images.",
          "Keywords Plus": "IMAGES"
        }
      },
      {
        "title": "Recognition and Localization Methods for Vision-Based Fruit Picking Robots: A Review",
        "year": "2020",
        "algorithm": "",
        "performance": "",
        "citation": "@article{tang2020recognition,\n  title={Recognition and localization methods for vision-based fruit picking robots: A review},\n  author={Tang, Yunchao and Chen, Mingyou and Wang, Chenglin and Luo, Lufeng and Li, Jinhui and Lian, Guoping and Zou, Xiangjun},\n  journal={Frontiers in Plant Science},\n  volume={11},\n  pages={510},\n  year={2020},\n  publisher={Frontiers Media SA}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{tang2020recognition}",
          "Article Title": "Recognition and Localization Methods for Vision-Based Fruit Picking Robots: A Review",
          "Times Cited, All Databases": "298",
          "Publication Year": "2020",
          "Highly Cited Status": "Y",
          "Publisher": "Frontiers in Plant Science",
          "Citation": "@article{tang2020recognition,\n  title={Recognition and localization methods for vision-based fruit picking robots: A review},\n  author={Tang, Yunchao and Chen, Mingyou and Wang, Chenglin and Luo, Lufeng and Li, Jinhui and Lian, Guoping and Zou, Xiangjun},\n  journal={Frontiers in Plant Science},\n  volume={11},\n  pages={510},\n  year={2020},\n  publisher={Frontiers Media SA}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "vision:localization,target recognition,3D reconstruction, and fault tolerance of complex agricultrual environment",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Tang, YC; Chen, MY; Wang, CL; Luo, LF; Li, JH; Lian, GP; Zou, XJ",
          "Abstract": "The utilization of machine vision and its associated algorithms improves the efficiency, functionality, intelligence, and remote interactivity of harvesting robots in complex agricultural environments. Machine vision and its associated emerging technology promise huge potential in advanced agricultural applications. However, machine vision and its precise positioning still have many technical difficulties, making it difficult for most harvesting robots to achieve true commercial applications. This article reports the application and research progress of harvesting robots and vision technology in fruit picking. The potential applications of vision and quantitative methods of localization, target recognition, 3D reconstruction, and fault tolerance of complex agricultural environment are focused, and fault-tolerant technology designed for utilization with machine vision and robotic systems are also explored. The two main methods used in fruit recognition and localization are reviewed, including digital image processing technology and deep learning-based algorithms. The future challenges brought about by recognition and localization success rates are identified: target recognition in the presence of illumination changes and occlusion environments; target tracking in dynamic interference-laden environments, 3D target reconstruction, and fault tolerance of the vision system for agricultural robots. In the end, several open research problems specific to recognition and localization applications for fruit harvesting robots are mentioned, and the latest development and future development trends of machine vision are described.",
          "Keywords Plus": "OF-THE-ART; MACHINE VISION; HARVESTING ROBOT; CITRUS-FRUIT; FIELD-EVALUATION; LITCHI CLUSTERS; GRAPE CLUSTERS; END-EFFECTOR; SYSTEM; COLOR"
        }
      },
      {
        "title": "A review of key techniques of vision-based control for harvesting robot",
        "year": "2016",
        "algorithm": "",
        "performance": "",
        "citation": "@article{zhao2016review,\n  title={A review of key techniques of vision-based control for harvesting robot},\n  author={Zhao, Yuanshen and Gong, Liang and Huang, Yixiang and Liu, Chengliang},\n  journal={Computers and Electronics in Agriculture},\n  volume={127},\n  pages={311--323},\n  year={2016},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{zhao2016review}",
          "Article Title": "A review of key techniques of vision-based control for harvesting robot",
          "Times Cited, All Databases": "241",
          "Publication Year": "2016",
          "Highly Cited Status": "",
          "Publisher": "Computers and Electronics in Agriculture",
          "Citation": "@article{zhao2016review,\n  title={A review of key techniques of vision-based control for harvesting robot},\n  author={Zhao, Yuanshen and Gong, Liang and Huang, Yixiang and Liu, Chengliang},\n  journal={Computers and Electronics in Agriculture},\n  volume={127},\n  pages={311--323},\n  year={2016},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "vision information acquisition strategies, fruit recognition algorithms, and eye-hand coordination methods",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Zhao, YS; Gong, L; Huang, YX; Liu, CL",
          "Abstract": "Although there is a rapid development of agricultural robotic technologies, a lack of access to robust fruit recognition and precision picking capabilities has limited the commercial application of harvesting robots. On the other hand, recent advances in key techniques in vision-based control have improved this situation. These techniques include vision information acquisition strategies, fruit recognition algorithms, and eye-hand coordination methods. In a fruit or vegetable harvesting robot, vision control is employed to solve two major problems in detecting objects in tree canopies and picking objects using visual information. This paper presents a review on these key vision control techniques and their potential applications in fruit or vegetable harvesting robots. The challenges and feature trends of applying these vision control techniques in harvesting robots are also described and discussed in the review. (C) 2016 Elsevier B.V. All rights reserved.",
          "Keywords Plus": "AGRICULTURAL ROBOT; COMPUTER VISION; TARGET RECOGNITION; FRUIT; PICKING; CITRUS; SYSTEM; COLOR; MACHINE; LOCALIZATION"
        }
      },
      {
        "title": "Research and development in agricultural robotics: A perspective of digital farming",
        "year": "2018",
        "algorithm": "",
        "performance": "",
        "citation": "@article{r2018research,\n  title={Research and development in agricultural robotics: A perspective of digital farming},\n  author={R Shamshiri, Redmond and Weltzien, Cornelia and Hameed, Ibrahim A and J Yule, Ian and E Grift, Tony and Balasundram, Siva K and Pitonakova, Lenka and Ahmad, Desa and Chowdhary, Girish},\n  year={2018},\n  publisher={Chinese Society of Agricultural Engineering}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{r2018research}",
          "Article Title": "Research and development in agricultural robotics: A perspective of digital farming",
          "Times Cited, All Databases": "219",
          "Publication Year": "2018",
          "Highly Cited Status": "Y",
          "Publisher": "Chinese Society of Agricultural Engineering",
          "Citation": "@article{r2018research,\n  title={Research and development in agricultural robotics: A perspective of digital farming},\n  author={R Shamshiri, Redmond and Weltzien, Cornelia and Hameed, Ibrahim A and J Yule, Ian and E Grift, Tony and Balasundram, Siva K and Pitonakova, Lenka and Ahmad, Desa and Chowdhary, Girish},\n  year={2018},\n  publisher={Chinese Society of Agricultural Engineering}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "weed control, field scouting, and harvesting",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Shamshiri, RR; Weltzien, C; Hameed, IA; Yule, IJ; Grift, TE; Balasundram, SK; Pitonakova, L; Ahmad, D; Chowdhary, G",
          "Abstract": "Digital farming is the practice of modern technologies such as sensors, robotics, and data analysis for shifting from tedious operations to continuously automated processes. This paper reviews some of the latest achievements in agricultural robotics, specifically those that are used for autonomous weed control, field scouting, and harvesting. Object identification, task planning algorithms, digitalization and optimization of sensors are highlighted as some of the facing challenges in the context of digital farming. The concepts of multi-robots, human-robot collaboration, and environment reconstruction from aerial images and ground-based sensors for the creation of virtual farms were highlighted as some of the gateways of digital farming. It was shown that one of the trends and research focuses in agricultural field robotics is towards building a swarm of small scale robots and drones that collaborate together to optimize farming inputs and reveal denied or concealed information. For the case of robotic harvesting, an autonomous framework with several simple axis manipulators can be faster and more efficient than the currently adapted professional expensive manipulators. While robots are becoming the inseparable parts of the modern farms, our conclusion is that it is not realistic to expect an entirely automated farming system in the future.",
          "Keywords Plus": "VISION-BASED CONTROL; MOBILE ROBOT; AUTONOMOUS ROBOT; HARVESTING ROBOT; FRUIT DETECTION; MACHINE VISION; DISCRIMINANT-ANALYSIS; SOFTWARE APPLICATION; AUTOMATIC GUIDANCE; YIELD ESTIMATION"
        }
      },
      {
        "title": "Design, integration, and field evaluation of a robotic apple harvester",
        "year": "2017",
        "algorithm": "",
        "performance": "Localization time of 1.5s, a picking time of 6s; a success pick rate of 84%.",
        "citation": "@article{silwal2017design,\n  title={Design, integration, and field evaluation of a robotic apple harvester},\n  author={Silwal, Abhisesh and Davidson, Joseph R and Karkee, Manoj and Mo, Changki and Zhang, Qin and Lewis, Karen},\n  journal={Journal of Field Robotics},\n  volume={34},\n  number={6},\n  pages={1140--1159},\n  year={2017},\n  publisher={Wiley Online Library}\n}\n",
        "fruit_veg": "apples",
        "paper": {
          "relevant": "y",
          "": "\\cite{silwal2017design}",
          "Article Title": "Design, integration, and field evaluation of a robotic apple harvester",
          "Times Cited, All Databases": "213",
          "Publication Year": "2017",
          "Highly Cited Status": "",
          "Publisher": "Journal of Field Robotics",
          "Citation": "@article{silwal2017design,\n  title={Design, integration, and field evaluation of a robotic apple harvester},\n  author={Silwal, Abhisesh and Davidson, Joseph R and Karkee, Manoj and Mo, Changki and Zhang, Qin and Lewis, Karen},\n  journal={Journal of Field Robotics},\n  volume={34},\n  number={6},\n  pages={1140--1159},\n  year={2017},\n  publisher={Wiley Online Library}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "a low-cost system required sensing, planning, and manipulation functionality",
          "fruit/veg": "apples",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "7DOF",
          "Performance": "Localization time of 1.5s, a picking time of 6s; a success pick rate of 84%.",
          "challenges": "The highly unstructured orchard environment has been a major challenge to the development of commercially viable robotic harvesting systems.",
          "Authors": "Silwal, A; Davidson, JR; Karkee, M; Mo, CK; Zhang, Q; Lewis, K",
          "Abstract": "Every apple destined for the fresh market is picked by the human hand. Despite extensive research over the past four decades, there are no mechanical apple harvesters for the fresh market commercially available, which is a significant concern because of increasing uncertainty about the availability of manual labor and rising production costs. The highly unstructured orchard environment has been a major challenge to the development of commercially viable robotic harvesting systems. This paper reports the design and field evaluation of a robotic apple harvester. The approach adopted was to use a low-cost system to assess required sensing, planning, and manipulation functionality in a modern orchard system with a planar canopy. The system was tested in a commercial apple orchard in Washington State. Workspace modifications and performance criteria are thoroughly defined and reported to help evaluate the approach and guide future enhancements. The machine vision system was accurate and had an average localization time of 1.5 s per fruit. The seven degree of freedom harvesting system successfully picked 127 of the 150 fruit attempted for an overall success rate of 84% with an average picking time of 6.0 s per fruit. Future work will include integration of additional sensing and obstacle detection for improved system robustness.",
          "Keywords Plus": "FRUIT DETECTION; IMAGE-ANALYSIS; COLOR; HAND; LOCALIZATION; MACHINE; SYSTEMS; VISION; CHERRY; SHAPE"
        }
      },
      {
        "title": "Development of a sweet pepper harvesting robot",
        "year": "2020",
        "algorithm": "",
        "performance": "Picking time of 24s and 15s in field and lab respectively; a success rate of 61\\%",
        "citation": "@article{arad2020development,\n  title={Development of a sweet pepper harvesting robot},\n  author={Arad, Boaz and Balendonck, Jos and Barth, Ruud and Ben-Shahar, Ohad and Edan, Yael and Hellstr{\\\"o}m, Thomas and Hemming, Jochen and Kurtser, Polina and Ringdahl, Ola and Tielen, Toon and others},\n  journal={Journal of Field Robotics},\n  volume={37},\n  number={6},\n  pages={1027--1039},\n  year={2020},\n  publisher={Wiley Online Library}\n}",
        "fruit_veg": "sweet pepper",
        "paper": {
          "relevant": "y",
          "": "\\cite{arad2020development}",
          "Article Title": "Development of a sweet pepper harvesting robot",
          "Times Cited, All Databases": "202",
          "Publication Year": "2020",
          "Highly Cited Status": "Y",
          "Publisher": "N",
          "Citation": "@article{arad2020development,\n  title={Development of a sweet pepper harvesting robot},\n  author={Arad, Boaz and Balendonck, Jos and Barth, Ruud and Ben-Shahar, Ohad and Edan, Yael and Hellstr{\\\"o}m, Thomas and Hemming, Jochen and Kurtser, Polina and Ringdahl, Ola and Tielen, Toon and others},\n  journal={Journal of Field Robotics},\n  volume={37},\n  number={6},\n  pages={1027--1039},\n  year={2020},\n  publisher={Wiley Online Library}\n}",
          "Document Type": "Article",
          "Main Contribution": "",
          "fruit/veg": "sweet pepper",
          "Data Modality": "RGB-D",
          "Learning Algorithm": "",
          "Locomotion": "6DOF",
          "Performance": "Picking time of 24s and 15s in field and lab respectively; a success rate of 61\\%",
          "challenges": "",
          "Authors": "Arad, B; Balendonck, J; Barth, R; Ben-Shahar, O; Edan, Y; Hellstr?m, T; Hemming, J; Kurtser, P; Ringdahl, O; Tielen, T; van Tuijl, B",
          "Abstract": "This paper presents the development, testing and validation of SWEEPER, a robot for harvesting sweet pepper fruit in greenhouses. The robotic system includes a six degrees of freedom industrial arm equipped with a specially designed end effector, RGB-D camera, high-end computer with graphics processing unit, programmable logic controllers, other electronic equipment, and a small container to store harvested fruit. All is mounted on a cart that autonomously drives on pipe rails and concrete floor in the end-user environment. The overall operation of the harvesting robot is described along with details of the algorithms for fruit detection and localization, grasp pose estimation, and motion control. The main contributions of this paper are the integrated system design and its validation and extensive field testing in a commercial greenhouse for different varieties and growing conditions. A total of 262 fruits were involved in a 4-week long testing period. The average cycle time to harvest a fruit was 24 s. Logistics took approximately 50% of this time (7.8 s for discharge of fruit and 4.7 s for platform movements). Laboratory experiments have proven that the cycle time can be reduced to 15 s by running the robot manipulator at a higher speed. The harvest success rates were 61% for the best fit crop conditions and 18% in current crop conditions. This reveals the importance of finding the best fit crop conditions and crop varieties for successful robotic harvesting. The SWEEPER robot is the first sweet pepper harvesting robot to demonstrate this kind of performance in a commercial greenhouse.",
          "Keywords Plus": "AGRICULTURAL ROBOT; GRIPPER DESIGN; SYSTEM; INTEGRATION; SIMULATION; FRAMEWORK; SOFTWARE; FRUITS"
        }
      },
      {
        "title": "An autonomous strawberry-harvesting robot: Design, development, integration, and field evaluation",
        "year": "2020",
        "algorithm": "",
        "performance": "picking time of 4.6s, a success rate from 75\\% to 100\\%.",
        "citation": "@article{xiong2020autonomous,\n  title={An autonomous strawberry-harvesting robot: Design, development, integration, and field evaluation},\n  author={Xiong, Ya and Ge, Yuanyue and Grimstad, Lars and From, P{\\aa}l J},\n  journal={Journal of Field Robotics},\n  volume={37},\n  number={2},\n  pages={202--224},\n  year={2020},\n  publisher={Wiley Online Library}\n}",
        "fruit_veg": "strawberry",
        "paper": {
          "relevant": "y",
          "": "\\cite{xiong2020autonomous}",
          "Article Title": "An autonomous strawberry-harvesting robot: Design, development, integration, and field evaluation",
          "Times Cited, All Databases": "197",
          "Publication Year": "2020",
          "Highly Cited Status": "Y",
          "Publisher": "N",
          "Citation": "@article{xiong2020autonomous,\n  title={An autonomous strawberry-harvesting robot: Design, development, integration, and field evaluation},\n  author={Xiong, Ya and Ge, Yuanyue and Grimstad, Lars and From, P{\\aa}l J},\n  journal={Journal of Field Robotics},\n  volume={37},\n  number={2},\n  pages={202--224},\n  year={2020},\n  publisher={Wiley Online Library}\n}",
          "Document Type": "Article",
          "Main Contribution": "A novel obstacle-separation algorithm. We present the theoretical method to generate pushing paths based on the surrounding obstacles.",
          "fruit/veg": "strawberry",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "Dual-arm",
          "Performance": "picking time of 4.6s, a success rate from 75\\% to 100\\%.",
          "challenges": "Robotic harvesting in cluttered and unstructured environment",
          "Authors": "Xiong, Y; Ge, YY; Grimstad, L; From, PJ",
          "Abstract": "This paper presents an autonomous robot capable of picking strawberries continuously in polytunnels. Robotic harvesting in cluttered and unstructured environment remains a challenge. A novel obstacle-separation algorithm was proposed to enable the harvesting system to pick strawberries that are located in clusters. The algorithm uses the gripper to push aside surrounding leaves, strawberries, and other obstacles. We present the theoretical method to generate pushing paths based on the surrounding obstacles. In addition to manipulation, an improved vision system is more resilient to lighting variations, which was developed based on the modeling of color against light intensity. Further, a low-cost dual-arm system was developed with an optimized harvesting sequence that increases its efficiency and minimizes the risk of collision. Improvements were also made to the existing gripper to enable the robot to pick directly into a market punnet, thereby eliminating the need for repacking. During tests on a strawberry farm, the robots first-attempt success rate for picking partially surrounded or isolated strawberries ranged from 50% to 97.1%, depending on the growth situations. Upon an additional attempt, the pick success rate increased to a range of 75-100%. In the field tests, the system was not able to pick a target that was entirely surrounded by obstacles. This failure was attributed to limitations in the vision system as well as insufficient dexterity in the grippers. However, the picking speed improved upon previous systems, taking just 6.1 s for manipulation operation in the one-arm mode and 4.6 s in the two-arm mode.",
          "Keywords Plus": "FASTER R-CNN; FRUIT; SYSTEM"
        }
      },
      {
        "title": "Robotic kiwifruit harvesting using machine vision, convolutional neural networks, and robotic arms",
        "year": "2019",
        "algorithm": "deep neural networks",
        "performance": "picking time of 5.5s, a success rate of 51\\%.",
        "citation": "@article{williams2019robotic,\n  title={Robotic kiwifruit harvesting using machine vision, convolutional neural networks, and robotic arms},\n  author={Williams, Henry AM and Jones, Mark H and Nejati, Mahla and Seabright, Matthew J and Bell, Jamie and Penhall, Nicky D and Barnett, Josh J and Duke, Mike D and Scarfe, Alistair J and Ahn, Ho Seok and others},\n  journal={biosystems engineering},\n  volume={181},\n  pages={140--156},\n  year={2019},\n  publisher={Elsevier}\n}",
        "fruit_veg": "kiwifruit",
        "paper": {
          "relevant": "y",
          "": "\\cite{williams2019robotic}",
          "Article Title": "Robotic kiwifruit harvesting using machine vision, convolutional neural networks, and robotic arms",
          "Times Cited, All Databases": "174",
          "Publication Year": "2019",
          "Highly Cited Status": "Y",
          "Publisher": "N",
          "Citation": "@article{williams2019robotic,\n  title={Robotic kiwifruit harvesting using machine vision, convolutional neural networks, and robotic arms},\n  author={Williams, Henry AM and Jones, Mark H and Nejati, Mahla and Seabright, Matthew J and Bell, Jamie and Penhall, Nicky D and Barnett, Josh J and Duke, Mike D and Scarfe, Alistair J and Ahn, Ho Seok and others},\n  journal={biosystems engineering},\n  volume={181},\n  pages={140--156},\n  year={2019},\n  publisher={Elsevier}\n}",
          "Document Type": "Article",
          "Main Contribution": " a novel dynamic fruit scheduling system is presented that has been developed to coordinate the four arms throughout the harvesting process.",
          "fruit/veg": "kiwifruit",
          "Data Modality": "3D",
          "Learning Algorithm": "deep neural networks",
          "Locomotion": "multi-arm, end-effector",
          "Performance": "picking time of 5.5s, a success rate of 51\\%.",
          "challenges": "",
          "Authors": "Williams, HAM; Jones, MH; Nejati, M; Seabright, MJ; Bell, J; Penhall, ND; Barnett, JJ; Duke, MD; Scarfe, AJ; Ahn, HS; Lim, J; MacDonald, BA",
          "Abstract": "As labour requirements in horticultural become more challenging, automated solutions are becoming an effective approach to maintain productivity and quality. This paper presents the design and performance evaluation of a novel multi-arm kiwifruit harvesting robot designed to operate autonomously in pergola style orchards. The harvester consists of four robotic arms that have been designed specifically for kiwifruit harvesting, each with a novel end-effector developed to enable safe harvesting of the kiwifruit. The vision system leverages recent advances in deep neural networks and stereo matching for reliably detecting and locating kiwifruit in real-world lighting conditions. Furthermore, a novel dynamic fruit scheduling system is presented that has been developed to coordinate the four arms throughout the harvesting process. The performance of the harvester has been measured through a comprehensive and realistic field-trial in a commercial orchard environment. The results show that the presented harvester is capable of successfully harvesting 51.0% of the total number of kiwifruit within the orchard with an average cycle time of 5.5s/fruit. (C) 2019 IAgrE. Published by Elsevier Ltd. All rights reserved.",
          "Keywords Plus": "RECOGNITION"
        }
      },
      {
        "title": "Human-robot interaction in agriculture: A survey and current challenges",
        "year": "2019",
        "algorithm": "",
        "performance": "",
        "citation": "@article{vasconez2019human,\n  title={Human--robot interaction in agriculture: A survey and current challenges},\n  author={Vasconez, Juan P and Kantor, George A and Cheein, Fernando A Auat},\n  journal={Biosystems engineering},\n  volume={179},\n  pages={35--48},\n  year={2019},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{vasconez2019human}",
          "Article Title": "Human-robot interaction in agriculture: A survey and current challenges",
          "Times Cited, All Databases": "153",
          "Publication Year": "2019",
          "Highly Cited Status": "",
          "Publisher": "Biosystems engineering",
          "Citation": "@article{vasconez2019human,\n  title={Human--robot interaction in agriculture: A survey and current challenges},\n  author={Vasconez, Juan P and Kantor, George A and Cheein, Fernando A Auat},\n  journal={Biosystems engineering},\n  volume={179},\n  pages={35--48},\n  year={2019},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "Human-robot interaction",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Vasconez, JP; Kantor, GA; Cheein, FAA",
          "Abstract": "Human-robot interaction (HRI) is an extensive and diverse research topic that has been gaining importance in last years. Different fields of study have used HRI approaches for solving complicated problems, where humans and robots interact in some way to obtain advantages from their collaboration. Many industrial areas benefit by applying HRI strategies in their applications, and agriculture is one of the most challenging of them. Currently, field crops can reach highly autonomous levels whereas speciality crops do not. In particular, crops such as fruits and vegetables are still harvested manually, and also some tasks such as pruning and thinning have long been considered to be too complex to automate completely. In addition, several countries face the problem of farm labour shortages. As a consequence, the production process is affected. In this context, we survey HRI approaches and ap-plications focused on improving the working conditions, agility, efficiency, safety, productivity and profitability of agricultural processes, in cases where manual labour cannot be replaced by but can be complemented with robots. (C) 2018 IAgrE. Published by Elsevier Ltd. All rights reserved.",
          "Keywords Plus": "IMAGE-ANALYSIS; DESIGN; FRUIT; CLASSIFICATION; COLLABORATION; AWARENESS; MACHINERY; TRACTOR; SYSTEMS; LEVEL"
        }
      },
      {
        "title": "Vision-based control of robotic manipulator for citrus harvesting",
        "year": "2014",
        "algorithm": "",
        "performance": "accuracy of controller is 15mm",
        "citation": "@article{mehta2014vision,\n  title={Vision-based control of robotic manipulator for citrus harvesting},\n  author={Mehta, SS and Burks, TF},\n  journal={Computers and electronics in agriculture},\n  volume={102},\n  pages={146--158},\n  year={2014},\n  publisher={Elsevier}\n}",
        "fruit_veg": "citrus",
        "paper": {
          "relevant": "y",
          "": "\\cite{mehta2014vision}",
          "Article Title": "Vision-based control of robotic manipulator for citrus harvesting",
          "Times Cited, All Databases": "158",
          "Publication Year": "2014",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{mehta2014vision,\n  title={Vision-based control of robotic manipulator for citrus harvesting},\n  author={Mehta, SS and Burks, TF},\n  journal={Computers and electronics in agriculture},\n  volume={102},\n  pages={146--158},\n  year={2014},\n  publisher={Elsevier}\n}",
          "Document Type": "Article",
          "Main Contribution": "vision-based manipulator",
          "fruit/veg": "citrus",
          "Data Modality": "3D",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "accuracy of controller is 15mm",
          "challenges": "",
          "Authors": "Mehta, SS; Burks, TF",
          "Abstract": "The main contribution of this paper is in the development of vision-based estimation and control system for robotic fruit harvesting and rigorous stability analysis to guarantee performance of the closed-loop system. The presented cooperative visual servo controller benefits from the large field-of-view of a fixed camera and the accuracy of a camera-in-hand (CiH). Computationally inexpensive perspective transformation-based range estimation method obtains 3D fruit position using a monocular camera to enable real-time manipulator control. A rotation controller is developed to orient the robot such that the target fruit selected by the fixed camera can be viewed by the CiH attached to the end-effector. Subsequently, the end-effector can be servoed to the target fruit location using the presented pursuit guidance based hybrid translation controller. Lyapunov-based stability analysis guarantees global exponential regulation of the end-effector. Numerical simulations verify the feasibility of the developed controller while the performance is evaluated on a seven degrees-of-freedom kinematically redundant manipulator using an artificial citrus tree. The position of the fruit was randomly selected, and the closed-loop visual servo control experiment was performed 21 times to analyze the repeatability and accuracy of the developed controller. With 95% confidence level the expected position of the robot end-effector is observed to lie within the confidence ellipsoid. The accuracy of the controller was observed to be about 15 mm, thus making the system suitable for harvesting medium and large varieties of citrus fruit but may limit operation for small varieties such as page and blood oranges. (C) 2014 Elsevier B.V. All rights reserved.",
          "Keywords Plus": "PICKING"
        }
      },
      {
        "title": "Development and field evaluation of a strawberry harvesting robot with a cable-driven gripper",
        "year": "2019",
        "algorithm": "",
        "performance": "picking time of 7.5s, a success rate of 59\\% in cluster.",
        "citation": "@article{xiong2019development,\n  title={Development and field evaluation of a strawberry harvesting robot with a cable-driven gripper},\n  author={Xiong, Ya and Peng, Cheng and Grimstad, Lars and From, P{\\aa}l Johan and Isler, Volkan},\n  journal={Computers and electronics in agriculture},\n  volume={157},\n  pages={392--402},\n  year={2019},\n  publisher={Elsevier}\n}",
        "fruit_veg": "strawberry",
        "paper": {
          "relevant": "y",
          "": "\\cite{xiong2019development}",
          "Article Title": "Development and field evaluation of a strawberry harvesting robot with a cable-driven gripper",
          "Times Cited, All Databases": "136",
          "Publication Year": "2019",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{xiong2019development,\n  title={Development and field evaluation of a strawberry harvesting robot with a cable-driven gripper},\n  author={Xiong, Ya and Peng, Cheng and Grimstad, Lars and From, P{\\aa}l Johan and Isler, Volkan},\n  journal={Computers and electronics in agriculture},\n  volume={157},\n  pages={392--402},\n  year={2019},\n  publisher={Elsevier}\n}",
          "Document Type": "Article",
          "Main Contribution": "",
          "fruit/veg": "strawberry",
          "Data Modality": "RGB-D",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "picking time of 7.5s, a success rate of 59\\% in cluster.",
          "challenges": "",
          "Authors": "Xiong, Y; Peng, C; Grimstad, L; From, PJ; Isler, V",
          "Abstract": "This paper presents the development and evaluation of a robot for harvesting strawberries (Fragaria x ananassa) grown on table-tops in polytunnels. The robot is comprised of a newly-designed gripper mounted on an industrial which in turn is mounted on a mobile base along with an RGB-D camera. The novel cable-driven gripper can open fingers to swallow a target. Since it is designed to target the fruit not the stem, it just requires the fruit location for picking. Moreover, equipped with internal sensors, the gripper can sense and correct for positional errors, and is robust to the localisation errors introduced by the vision module. Another important feature of the gripper is the internal container that is used to collect berries during picking. Since the manipulator does not need to go back and forth between each berry and a separate punnet, picking time is reduced significantly. The vision system uses colour thresholding combined with screening of the object area and the depth range to select ripe and reachable strawberries, which is fast for processing. These components are integrated into a complete system whose performance is analysed starting with the four main failure cases of the vision system: undetected, duplicate detections, inaccurate localisation and segmentation failure. The integration enables the robot to harvest continuously by moving the platform with a joystick. Field experiments show the average cycle time of continuous single strawberry picking is 7.5 s and 10.6 s when including all procedures. Furthermore, the robot is able to pick isolated strawberries with a close-to-perfect success rate (96.8%). However, in farm settings, the average picking success rate is 53.6%, and 59.0% when including success with damage, testing on the strawberry cultivar of FAVORI. The failure cases are analysed and most failures are found when picking strawberries in clusters, in which both the detection algorithm and the gripper struggles to separate the berries.",
          "Keywords Plus": "INTEGRATION; DESIGN"
        }
      },
      {
        "title": "Autonomous Sweet Pepper Harvesting for Protected Cropping Systems",
        "year": "2017",
        "algorithm": "",
        "performance": "the success rate of 46\\%,58\\%,90\\% in unmodified, modified and favorable cultivar respectively.",
        "citation": "@article{lehnert2017autonomous,\n  title={Autonomous sweet pepper harvesting for protected cropping systems},\n  author={Lehnert, Christopher and English, Andrew and McCool, Christopher and Tow, Adam W and Perez, Tristan},\n  journal={IEEE Robotics and Automation Letters},\n  volume={2},\n  number={2},\n  pages={872--879},\n  year={2017},\n  publisher={IEEE}\n}",
        "fruit_veg": "sweet pepper",
        "paper": {
          "relevant": "y",
          "": "\\cite{lehnert2017autonomous}",
          "Article Title": "Autonomous Sweet Pepper Harvesting for Protected Cropping Systems",
          "Times Cited, All Databases": "138",
          "Publication Year": "2017",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{lehnert2017autonomous,\n  title={Autonomous sweet pepper harvesting for protected cropping systems},\n  author={Lehnert, Christopher and English, Andrew and McCool, Christopher and Tow, Adam W and Perez, Tristan},\n  journal={IEEE Robotics and Automation Letters},\n  volume={2},\n  number={2},\n  pages={872--879},\n  year={2017},\n  publisher={IEEE}\n}",
          "Document Type": "Article",
          "Main Contribution": "",
          "fruit/veg": "sweet pepper",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "the success rate of 46\\%,58\\%,90\\% in unmodified, modified and favorable cultivar respectively.",
          "challenges": "",
          "Authors": "Lehnert, C; English, A; McCool, C; Tow, AW; Perez, T",
          "Abstract": "In this letter, we present a new robotic harvester (Harvey) that can autonomously harvest sweet pepper in protected cropping environments. Our approach combines effective vision algorithms with a novel end-effector design to enable successful harvesting of sweet peppers. Initial field trials in protected cropping environments, with two cultivar, demonstrate the efficacy of this approach achieving a 46% success rate for unmodified crop, and 58% for modified crop. Furthermore, for the more favourable cultivar we were also able to detach 90% of sweet peppers, indicating that improvements in the grasping success rate would result in greatly improved harvesting performance.",
          "Keywords Plus": "ROBOT"
        }
      },
      {
        "title": "Nonconventional Weed Management Strategies for Modern Agriculture",
        "year": "2015",
        "algorithm": "",
        "performance": "",
        "citation": "@article{bajwa2015nonconventional,\n  title={Nonconventional weed management strategies for modern agriculture},\n  author={Bajwa, Ali A and Mahajan, Gulshan and Chauhan, Bhagirath S},\n  journal={Weed science},\n  volume={63},\n  number={4},\n  pages={723--747},\n  year={2015},\n  publisher={Cambridge University Press}\n}\n",
        "fruit_veg": "weed",
        "paper": {
          "relevant": "y",
          "": "\\cite{bajwa2015nonconventional}",
          "Article Title": "Nonconventional Weed Management Strategies for Modern Agriculture",
          "Times Cited, All Databases": "139",
          "Publication Year": "2015",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{bajwa2015nonconventional,\n  title={Nonconventional weed management strategies for modern agriculture},\n  author={Bajwa, Ali A and Mahajan, Gulshan and Chauhan, Bhagirath S},\n  journal={Weed science},\n  volume={63},\n  number={4},\n  pages={723--747},\n  year={2015},\n  publisher={Cambridge University Press}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "section 1",
          "fruit/veg": "weed",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Bajwa, AA; Mahajan, G; Chauhan, BS",
          "Abstract": "Weeds are a significant problem in crop production and their management in modern agriculture is crucial to avoid yield losses and ensure food security. Intensive agricultural practices, changing climate, and natural disasters affect weed dynamics and that requires a change in weed management protocols. The existing manual control options are no longer viable because of labor shortages; chemical control options are limited by ecodegradation, health hazards, and development of herbicide resistance in weeds. We are therefore reviewing some potential nonconventional weed management strategies for modern agriculture that are viable, feasible, and efficient. Improvement in tillage regimes has long been identified as an impressive weed-control measure. Harvest weed seed control and seed predation have been shown as potential tools for reducing weed emergence and seed bank reserves. Development in the field of allelopathy for weed management has led to new techniques for weed control. The remarkable role of biotechnological advancements in developing herbicide-resistant crops, bioherbicides, and harnessing the allelopathic potential of crops is also worth mentioning in a modern weed management program. Thermal weed management has also been observed as a useful technique, especially under conservation agriculture systems. Last, precision weed management has been elaborated with sufficient details. The role of remote sensing, modeling, and robotics as an integral part of precision weed management has been highlighted in a realistic manner. All these strategies are viable for today's agriculture; however, site-specific selection and the use of right combinations will be the key to success. No single strategy is perfect, and therefore an integrated approach may provide better results. Future research is needed to explore the potential of these strategies and to optimize them on technological and cultural bases. The adoption of such methods may improve the efficiency of cropping systems under sustainable and conservation practices.",
          "Keywords Plus": "STARTHISTLE CENTAUREA-SOLSTITIALIS; POSTDISPERSAL SEED PREDATION; HERBICIDE-RESISTANT CROPS; QUANTITATIVE TRAIT LOCI; WINTER-WHEAT; COVER CROPS; NITROGEN-FERTILIZER; BIOLOGICAL-CONTROL; SOIL SOLARIZATION; PLANT-PATHOGENS"
        }
      },
      {
        "title": "A survey of public datasets for computer vision tasks in precision agriculture",
        "year": "2020",
        "algorithm": "",
        "performance": "",
        "citation": "@article{lu2020survey,\n  title={A survey of public datasets for computer vision tasks in precision agriculture},\n  author={Lu, Yuzhen and Young, Sierra},\n  journal={Computers and Electronics in Agriculture},\n  volume={178},\n  pages={105760},\n  year={2020},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{lu2020survey}",
          "Article Title": "A survey of public datasets for computer vision tasks in precision agriculture",
          "Times Cited, All Databases": "128",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{lu2020survey,\n  title={A survey of public datasets for computer vision tasks in precision agriculture},\n  author={Lu, Yuzhen and Young, Sierra},\n  journal={Computers and Electronics in Agriculture},\n  volume={178},\n  pages={105760},\n  year={2020},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Lu, YZ; Young, S",
          "Abstract": "Computer vision technologies have attracted significant interest in precision agriculture in recent years. At the core of robotics and artificial intelligence, computer vision enables various tasks from planting to harvesting in the crop production cycle to be performed automatically and efficiently. However, the scarcity of public image datasets remains a crucial bottleneck for fast prototyping and evaluation of computer vision and machine learning algorithms for the targeted tasks. Since 2015, a number of image datasets have been established and made publicly available to alleviate this bottleneck. Despite this progress, a dedicated survey on these datasets is still lacking. To fill this gap, this paper makes the first comprehensive but not exhaustive review of the public image datasets collected under field conditions for facilitating precision agriculture, which include 15 datasets on weed control, 10 datasets on fruit detection, and 9 datasets on miscellaneous applications. We survey the main characteristics and applications of these datasets, and discuss the key considerations for creating high-quality public image datasets. This survey paper will be valuable for the research community on the selection of suitable image datasets for algorithm development and identification of where creation of new image datasets is needed to support precision agriculture.",
          "Keywords Plus": "WEED-CONTROL; SEMANTIC SEGMENTATION; APPLE DETECTION; SUGAR-BEET; CLASSIFICATION; LOCALIZATION; CROPS; IMAGES; ROBOTS"
        }
      },
      {
        "title": "Performance Evaluation of a Harvesting Robot for Sweet Pepper",
        "year": "2017",
        "algorithm": "",
        "performance": "26\\% and 33\\% with two end-effector respectively.",
        "citation": "@article{bac2017performance,\n  title={Performance evaluation of a harvesting robot for sweet pepper},\n  author={Bac, C Wouter and Hemming, Jochen and Van Tuijl, BAJ and Barth, Ruud and Wais, Ehud and van Henten, Eldert J},\n  journal={Journal of Field Robotics},\n  volume={34},\n  number={6},\n  pages={1123--1139},\n  year={2017},\n  publisher={Wiley Online Library}\n}\n",
        "fruit_veg": "sweet pepper",
        "paper": {
          "relevant": "y",
          "": "\\cite{bac2017performance}",
          "Article Title": "Performance Evaluation of a Harvesting Robot for Sweet Pepper",
          "Times Cited, All Databases": "135",
          "Publication Year": "2017",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{bac2017performance,\n  title={Performance evaluation of a harvesting robot for sweet pepper},\n  author={Bac, C Wouter and Hemming, Jochen and Van Tuijl, BAJ and Barth, Ruud and Wais, Ehud and van Henten, Eldert J},\n  journal={Journal of Field Robotics},\n  volume={34},\n  number={6},\n  pages={1123--1139},\n  year={2017},\n  publisher={Wiley Online Library}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "vision: pick point",
          "fruit/veg": "sweet pepper",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "two types of end effectors (Fin Ray; Lip type)",
          "Performance": "26\\% and 33\\% with two end-effector respectively.",
          "challenges": "",
          "Authors": "Bac, CW; Hemming, J; van Tuijl, BAJ; Barth, R; Wais, E; van Henten, EJ",
          "Abstract": "This paper evaluates a robot developed for autonomous harvesting of sweet peppers in a commercial greenhouse. Objectives were to assess robot performance under unmodified and simplified crop conditions, using two types of end effectors (Fin Ray; Lip type), and to evaluate the performance contribution of stem-dependent determination of the grasp pose. We describe and discuss the performance of hardware and software components developed for fruit harvesting in a complex environment that includes lighting variation, occlusions, and densely spaced obstacles. After simplifying the crop, harvest success significantly improved from 6% to 26% (Fin Ray) and from 2% to 33% (Lip type). We observed a decrease in stem damage and an increase in grasp success after enabling stem-dependent determination of the grasp pose. Generally, the robot had difficulty in successfully picking sweet peppers and we discuss possible causes. The robot's novel capability of perceiving the stem of a plant may serve as useful functionality for future robots. (C) 2017 Wiley Periodicals, Inc.",
          "Keywords Plus": "DESIGN; LOCALIZATION; FRAMEWORK"
        }
      },
      {
        "title": "Machine Vision Systems in Precision Agriculture for Crop Farming",
        "year": "2019",
        "algorithm": "",
        "performance": "",
        "citation": "@article{mavridou2019machine,\n  title={Machine vision systems in precision agriculture for crop farming},\n  author={Mavridou, Efthimia and Vrochidou, Eleni and Papakostas, George A and Pachidis, Theodore and Kaburlasos, Vassilis G},\n  journal={Journal of Imaging},\n  volume={5},\n  number={12},\n  pages={89},\n  year={2019},\n  publisher={MDPI}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{mavridou2019machine}",
          "Article Title": "Machine Vision Systems in Precision Agriculture for Crop Farming",
          "Times Cited, All Databases": "128",
          "Publication Year": "2019",
          "Highly Cited Status": "",
          "Publisher": "Journal of Imaging",
          "Citation": "@article{mavridou2019machine,\n  title={Machine vision systems in precision agriculture for crop farming},\n  author={Mavridou, Efthimia and Vrochidou, Eleni and Papakostas, George A and Pachidis, Theodore and Kaburlasos, Vassilis G},\n  journal={Journal of Imaging},\n  volume={5},\n  number={12},\n  pages={89},\n  year={2019},\n  publisher={MDPI}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "vision:fruit grading, fruit counting, and yield estimation",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Mavridou, E; Vrochidou, E; Papakostas, GA; Pachidis, T; Kaburlasos, VG",
          "Abstract": "Machine vision for precision agriculture has attracted considerable research interest in recent years. The aim of this paper is to review the most recent work in the application of machine vision to agriculture, mainly for crop farming. This study can serve as a research guide for the researcher and practitioner alike in applying cognitive technology to agriculture. Studies of different agricultural activities that support crop harvesting are reviewed, such as fruit grading, fruit counting, and yield estimation. Moreover, plant health monitoring approaches are addressed, including weed, insect, and disease detection. Finally, recent research efforts considering vehicle guidance systems and agricultural harvesting robots are also reviewed.",
          "Keywords Plus": "AUTOMATED DETECTION; CITRUS DISEASES; COMPUTER VISION; SUGAR-BEET; IMAGE; CLASSIFICATION; SEGMENTATION; GUIDANCE; NAVIGATION; RECOGNITION"
        }
      },
      {
        "title": "Agricultural Robotics for Field Operations",
        "year": "2020",
        "algorithm": "",
        "performance": "",
        "citation": "@article{fountas2020agricultural,\n  title={Agricultural robotics for field operations},\n  author={Fountas, Spyros and Mylonas, Nikos and Malounas, Ioannis and Rodias, Efthymios and Hellmann Santos, Christoph and Pekkeriet, Erik},\n  journal={Sensors},\n  volume={20},\n  number={9},\n  pages={2672},\n  year={2020},\n  publisher={MDPI}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{fountas2020agricultural}",
          "Article Title": "Agricultural Robotics for Field Operations",
          "Times Cited, All Databases": "129",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "sensors",
          "Citation": "@article{fountas2020agricultural,\n  title={Agricultural robotics for field operations},\n  author={Fountas, Spyros and Mylonas, Nikos and Malounas, Ioannis and Rodias, Efthymios and Hellmann Santos, Christoph and Pekkeriet, Erik},\n  journal={Sensors},\n  volume={20},\n  number={9},\n  pages={2672},\n  year={2020},\n  publisher={MDPI}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "harvesting and weeding robotics",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Fountas, S; Mylonas, N; Malounas, I; Rodias, E; Santos, CH; Pekkeriet, E",
          "Abstract": "Modern agriculture is related to a revolution that occurred in a large group of technologies (e.g., informatics, sensors, navigation) within the last decades. In crop production systems, there are field operations that are quite labour-intensive either due to their complexity or because of the fact that they are connected to sensitive plants/edible product interaction, or because of the repetitiveness they require throughout a crop production cycle. These are the key factors for the development of agricultural robots. In this paper, a systematic review of the literature has been conducted on research and commercial agricultural robotics used in crop field operations. This study underlined that the most explored robotic systems were related to harvesting and weeding, while the less studied were the disease detection and seeding robots. The optimization and further development of agricultural robotics are vital, and should be evolved by producing faster processing algorithms, better communication between the robotic platforms and the implements, and advanced sensing systems.",
          "Keywords Plus": "STRAWBERRY-HARVESTING ROBOT; WEED-CONTROL; SYSTEM; MACHINE; DESIGN; SEGMENTATION; PLATFORM; PLANNER; ROW"
        }
      },
      {
        "title": "Advances in Agriculture Robotics: A State-of-the-Art Review and Challenges Ahead",
        "year": "2021",
        "algorithm": "",
        "performance": "",
        "citation": "@article{oliveira2021advances,\n  title={Advances in agriculture robotics: A state-of-the-art review and challenges ahead},\n  author={Oliveira, Luiz FP and Moreira, Ant{\\'o}nio P and Silva, Manuel F},\n  journal={Robotics},\n  volume={10},\n  number={2},\n  pages={52},\n  year={2021},\n  publisher={MDPI}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{oliveira2021advances}",
          "Article Title": "Advances in Agriculture Robotics: A State-of-the-Art Review and Challenges Ahead",
          "Times Cited, All Databases": "118",
          "Publication Year": "2021",
          "Highly Cited Status": "",
          "Publisher": "Robotics",
          "Citation": "@article{oliveira2021advances,\n  title={Advances in agriculture robotics: A state-of-the-art review and challenges ahead},\n  author={Oliveira, Luiz FP and Moreira, Ant{\\'o}nio P and Silva, Manuel F},\n  journal={Robotics},\n  volume={10},\n  number={2},\n  pages={52},\n  year={2021},\n  publisher={MDPI}\n}\n",
          "Document Type": "Review",
          "Main Contribution": " locomotion systems, sensors, computer vision algorithms and communication technologies",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Oliveira, LFP; Moreira, AP; Silva, MF",
          "Abstract": "The constant advances in agricultural robotics aim to overcome the challenges imposed by population growth, accelerated urbanization, high competitiveness of high-quality products, environmental preservation and a lack of qualified labor. In this sense, this review paper surveys the main existing applications of agricultural robotic systems for the execution of land preparation before planting, sowing, planting, plant treatment, harvesting, yield estimation and phenotyping. In general, all robots were evaluated according to the following criteria: its locomotion system, what is the final application, if it has sensors, robotic arm and/or computer vision algorithm, what is its development stage and which country and continent they belong. After evaluating all similar characteristics, to expose the research trends, common pitfalls and the characteristics that hinder commercial development, and discover which countries are investing into Research and Development (R&D) in these technologies for the future, four major areas that need future research work for enhancing the state of the art in smart agriculture were highlighted: locomotion systems, sensors, computer vision algorithms and communication technologies. The results of this research suggest that the investment in agricultural robotic systems allows to achieve short-harvest monitoring-and long-term objectives-yield estimation.",
          "Keywords Plus": "AUTONOMOUS MOBILE ROBOT; WEEDING ROBOT; LOCALIZATION; EXTRACTION; SIMULATION; DESIGN"
        }
      },
      {
        "title": "Mapping almond orchard canopy volume, flowers, fruit and yield using lidar and vision sensors",
        "year": "2016",
        "algorithm": "",
        "performance": "",
        "citation": "@article{underwood2016mapping,\n  title={Mapping almond orchard canopy volume, flowers, fruit and yield using lidar and vision sensors},\n  author={Underwood, James P and Hung, Calvin and Whelan, Brett and Sukkarieh, Salah},\n  journal={Computers and electronics in agriculture},\n  volume={130},\n  pages={83--96},\n  year={2016},\n  publisher={Elsevier}\n}",
        "fruit_veg": "almond",
        "paper": {
          "relevant": "y",
          "": "\\cite{underwood2016mapping}",
          "Article Title": "Mapping almond orchard canopy volume, flowers, fruit and yield using lidar and vision sensors",
          "Times Cited, All Databases": "125",
          "Publication Year": "2016",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{underwood2016mapping,\n  title={Mapping almond orchard canopy volume, flowers, fruit and yield using lidar and vision sensors},\n  author={Underwood, James P and Hung, Calvin and Whelan, Brett and Sukkarieh, Salah},\n  journal={Computers and electronics in agriculture},\n  volume={130},\n  pages={83--96},\n  year={2016},\n  publisher={Elsevier}\n}",
          "Document Type": "Article",
          "Main Contribution": "present a mobile terrestrial scanning system for almond orchards ",
          "fruit/veg": "almond",
          "Data Modality": "3D",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Underwood, JP; Hung, C; Whelan, B; Sukkarieh, S",
          "Abstract": "This paper present a mobile terrestrial scanning system for almond orchards, that is able to efficiently map flower and fruit distributions and to estimate and predict yield for individual trees. A mobile robotic ground vehicle scans the orchard while logging data from on-board lidar and camera sensors. An automated software pipeline processes the data offline, to produce a 3D map of the orchard and to automatically detect each tree within that map, including correct associations for the same trees seen on prior occasions. Colour images are also associated to each tree, leading to a database of images and canopy models, at different times throughout the season and spanning multiple years. A canopy volume measure is derived from the 3D models, and classification is performed on the images to estimate flower and fruit density. These measures were compared to individual tree harvest weights to assess the relationship to yield. A block of approximately 580 trees was scanned at peak bloom, fruit-set and just before harvest for two subsequent years, with up to 50 trees individually harvested for comparison. Lidar canopy volume had the strongest linear relationship to yield with R-2 = 0.77 for 39 tree samples spanning two years. An additional experiment was performed using hand-held photography and image processing to measure fruit density, which exhibited similar performance (R-2 = 0.71). Flower density measurements were not strongly related to yield, however, the maps show clear differentiation between almond varieties and may be useful for other studies. (C) 2016 Elsevier B.V. All rights reserved.",
          "Keywords Plus": "APPLE ORCHARDS; LASER SCANNER; LOCALIZATION; VINEYARDS; DENSITY; CAMERA"
        }
      },
      {
        "title": "Robotics and labour in agriculture. A context consideration",
        "year": "2019",
        "algorithm": "",
        "performance": "",
        "citation": "@article{marinoudi2019robotics,\n  title={Robotics and labour in agriculture. A context consideration},\n  author={Marinoudi, Vasso and S{\\o}rensen, Claus G and Pearson, Simon and Bochtis, Dionysis},\n  journal={Biosystems Engineering},\n  volume={184},\n  pages={111--121},\n  year={2019},\n  publisher={Elsevier}\n}",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{marinoudi2019robotics}",
          "Article Title": "Robotics and labour in agriculture. A context consideration",
          "Times Cited, All Databases": "124",
          "Publication Year": "2019",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{marinoudi2019robotics,\n  title={Robotics and labour in agriculture. A context consideration},\n  author={Marinoudi, Vasso and S{\\o}rensen, Claus G and Pearson, Simon and Bochtis, Dionysis},\n  journal={Biosystems Engineering},\n  volume={184},\n  pages={111--121},\n  year={2019},\n  publisher={Elsevier}\n}",
          "Document Type": "Article",
          "Main Contribution": "section 1",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Marinoudi, V; Sorensen, CG; Pearson, S; Bochtis, D",
          "Abstract": "Over the last century, agriculture transformed from a labour-intensive industry towards mechanisation and power-intensive production systems, while over the last 15 years agricultural industry has started to digitise. Through this transformation there was a continuous labour outflow from agriculture, mainly from standardized tasks within production process. Robots and artificial intelligence can now be used to conduct non-standardised tasks (e.g. fruit picking, selective weeding, crop sensing) previously reserved for human workers and at economically feasible costs. As a consequence, automation is no longer restricted to standardized tasks within agricultural production (e.g. ploughing, combine harvesting). In addition, many job roles in agriculture may be augmented but not replaced by robots. Robots in many instances will work collaboratively with humans. This new robotic ecosystem creates complex ethical, legislative and social impacts. A key question, we consider here, is what are the short and mid-term effects of robotised agriculture on sector jobs and employment? The presented work outlines the conditions, constraints, and inherent relationships between labour input and technology input in bio-production, as well as, provides the procedural framework and research design to be followed in order to evaluate the effect of adoption automation and robotics in agriculture. (C) 2019 IAgrE. Published by Elsevier Ltd. All rights reserved.",
          "Keywords Plus": "CAPITAL-SKILL COMPLEMENTARITY; CAPACITATED FIELD OPERATIONS; TECHNOLOGICAL-CHANGE; JOB POLARIZATION; INEQUALITY; GROWTH; SYSTEM"
        }
      },
      {
        "title": "Fruit detection, segmentation and 3D visualisation of environments in apple orchards",
        "year": "2020",
        "algorithm": "DaSNet-v2",
        "performance": "DaSNet-v2 with resnet-18 achieves 0.85, 0.87 and 0.866 on recall and precision of detection, and accuracy of instance segmentation on fruits, and 0.775 on the accuracy of branches segmentation, respectively.",
        "citation": "@article{kang2020fruit,\n  title={Fruit detection, segmentation and 3D visualisation of environments in apple orchards},\n  author={Kang, Hanwen and Chen, Chao},\n  journal={Computers and Electronics in Agriculture},\n  volume={171},\n  pages={105302},\n  year={2020},\n  publisher={Elsevier}\n}",
        "fruit_veg": "apples",
        "paper": {
          "relevant": "y",
          "": "\\cite{kang2020fruit}",
          "Article Title": "Fruit detection, segmentation and 3D visualisation of environments in apple orchards",
          "Times Cited, All Databases": "119",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{kang2020fruit,\n  title={Fruit detection, segmentation and 3D visualisation of environments in apple orchards},\n  author={Kang, Hanwen and Chen, Chao},\n  journal={Computers and Electronics in Agriculture},\n  volume={171},\n  pages={105302},\n  year={2020},\n  publisher={Elsevier}\n}",
          "Document Type": "Article",
          "Main Contribution": "detection",
          "fruit/veg": "apples",
          "Data Modality": "3D",
          "Learning Algorithm": " DaSNet-v2",
          "Locomotion": "",
          "Performance": "DaSNet-v2 with resnet-18 achieves 0.85, 0.87 and 0.866 on recall and precision of detection, and accuracy of instance segmentation on fruits, and 0.775 on the accuracy of branches segmentation, respectively.",
          "challenges": "There are many complex conditions in orchard environments, such as changing illumination, appearance variation, and occlusion.  semantic segmentation returns the mask for each class instead of each object",
          "Authors": "Kang, HW; Chen, C",
          "Abstract": "Development of an accurate and reliable fruit detection system is a challenging task. There are many complex conditions in orchard environments, such as changing illumination, appearance variation, and occlusion. Robotic vision is required to understand the working environments from the sensory data and guide the robotic arm to detach the fruits. In our previous work, a deep neural network DaSNet-v1 was developed to perform detection and segmentation on fruits and branches in orchard environments. However, semantic segmentation returns the mask for each class instead of each object. Segmentation on each fruit is important as it can provide abundant information of each object, especially for those overlapped fruits. This work presents an improved deep neural network DaSNet-v2, which can perform detection and instance segmentation on fruits, and semantic segmentation on branches. DaSNet-v2 is tested and validated by experimental results obtained from field-testing in an apple orchard. From the experiment results, DaSNet-v2 with resnet-101 achieves 0.868, 0.88 and 0.873 on recall and precision of detection, and accuracy of instance segmentation on fruits, and 0.794 on the accuracy of branches segmentation, respectively. DaSNet-v2 with light-weight backbone resnet-18 achieves 0.85, 0.87 and 0.866 on recall and precision of detection, and accuracy of instance segmentation on fruits, and 0.775 on the accuracy of branches segmentation, respectively. The average running time and weight size of light-weight DaSNet-v2 are 55 ms and 8.1 M, respectively. Experimental results show DaSNet-v2 can robustly and efficiently perform the vision sensing for robotic harvesting in apple orchards.",
          "Keywords Plus": "AGRICULTURE; RGB"
        }
      },
      {
        "title": "Color-, depth-, and shape-based 3D fruit detection",
        "year": "2020",
        "algorithm": "SVM",
        "performance": "For the pepper, eggplant, and guava datasets, the detection precision was 0.864, 0.886, and 0.888, and the recall was 0.889, 0.762, and 0.812, respectively.",
        "citation": "@article{lin2020color,\n  title={Color-, depth-, and shape-based 3D fruit detection},\n  author={Lin, Guichao and Tang, Yunchao and Zou, Xiangjun and Xiong, Juntao and Fang, Yamei},\n  journal={Precision Agriculture},\n  volume={21},\n  pages={1--17},\n  year={2020},\n  publisher={Springer}\n}",
        "fruit_veg": "spherical or cylindrical fruits.the pepper, eggplant, and guava datasets",
        "paper": {
          "relevant": "y",
          "": "\\cite{lin2020color}",
          "Article Title": "Color-, depth-, and shape-based 3D fruit detection",
          "Times Cited, All Databases": "104",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{lin2020color,\n  title={Color-, depth-, and shape-based 3D fruit detection},\n  author={Lin, Guichao and Tang, Yunchao and Zou, Xiangjun and Xiong, Juntao and Fang, Yamei},\n  journal={Precision Agriculture},\n  volume={21},\n  pages={1--17},\n  year={2020},\n  publisher={Springer}\n}",
          "Document Type": "Article",
          "Main Contribution": "Segmentation",
          "fruit/veg": "spherical or cylindrical fruits.the pepper, eggplant, and guava datasets",
          "Data Modality": "3D",
          "Learning Algorithm": "SVM",
          "Locomotion": "",
          "Performance": "For the pepper, eggplant, and guava datasets, the detection precision was 0.864, 0.886, and 0.888, and the recall was 0.889, 0.762, and 0.812, respectively.",
          "challenges": "",
          "Authors": "Lin, GH; Tang, YH; Zou, XJ; Xiong, JT; Fang, YM",
          "Abstract": "A novel detection algorithm based on color, depth, and shape information is proposed for detecting spherical or cylindrical fruits on plants in natural environments and thus guiding harvesting robots to pick them automatically. A probabilistic image segmentation method is first presented to segment a red-green-blue image as a binary mask. Multiplied by this mask, a filtered depth image is obtained. Region growing, a region-based image segmentation method, is then applied to group the depth image into multiple clusters. Each cluster represents a fruit, leaf, or branch that is later transformed into a point cloud. Next, a 3D shape detection method based on M-estimator sample consensus, a model parameter estimator, is employed to detect potential fruits from each point cloud. Finally, an angle/color/shape-based global point cloud descriptor (GPCD) is developed to extract a feature vector for an entire point cloud, and a support vector machine classifier trained on the GPCD features is used to exclude false positives. Pepper, eggplant, and guava datasets were captured in the field. For the pepper, eggplant, and guava datasets, the detection precision was 0.864, 0.886, and 0.888, and the recall was 0.889, 0.762, and 0.812, respectively. Experiments revealed that the proposed algorithm was universal and robust and hence applicable to an agricultural harvesting robot.",
          "Keywords Plus": "RECOGNITION; FRAMEWORK; ALGORITHM; APPLES"
        }
      },
      {
        "title": "Data synthesis methods for semantic segmentation in agriculture: A Capsicum annuum dataset",
        "year": "2018",
        "algorithm": "",
        "performance": "",
        "citation": "@article{barth2018data,\n  title={Data synthesis methods for semantic segmentation in agriculture: A Capsicum annuum dataset},\n  author={Barth, Ruud and IJsselmuiden, Joris and Hemming, Jochen and Van Henten, Eldert J},\n  journal={Computers and electronics in agriculture},\n  volume={144},\n  pages={284--296},\n  year={2018},\n  publisher={Elsevier}\n}",
        "fruit_veg": "Capsicum",
        "paper": {
          "relevant": "y",
          "": "\\cite{barth2018data}",
          "Article Title": "Data synthesis methods for semantic segmentation in agriculture: A Capsicum annuum dataset",
          "Times Cited, All Databases": "107",
          "Publication Year": "2018",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{barth2018data,\n  title={Data synthesis methods for semantic segmentation in agriculture: A Capsicum annuum dataset},\n  author={Barth, Ruud and IJsselmuiden, Joris and Hemming, Jochen and Van Henten, Eldert J},\n  journal={Computers and electronics in agriculture},\n  volume={144},\n  pages={284--296},\n  year={2018},\n  publisher={Elsevier}\n}",
          "Document Type": "Article",
          "Main Contribution": "Segmentation",
          "fruit/veg": "Capsicum",
          "Data Modality": "3D",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Barth, R; IJsselmuiden, J; Hemming, J; Van Henten, EJ",
          "Abstract": "This paper provides synthesis methods for large-scale semantic image segmentation datasets of agricultural scenes with the objective to bridge the gap between state-of-the art computer vision performance and that of computer vision in the agricultural robotics domain. We propose a novel methodology to generate renders of random meshes of plants based on empirical measurements, including the automated generation per-pixel class and depth labels for multiple plant parts. A running example is given of Capsicum annuum (sweet or bell pepper) in a high-tech greenhouse. A synthetic dataset of 10,500 images was rendered through Blender, using scenes with 42 procedurally generated plant models with randomised plant parameters. These parameters were based on 21 empirically measured plant properties at 115 positions on 15 plant stems. Fruit models were obtained by 3D scanning and plant part textures were gathered photographically. As reference dataset for modelling and evaluate segmentation performance, 750 empirical images of 50 plants were collected in a greenhouse from multiple angles and distances using image acquisition hardware of a sweet pepper harvest robot prototype. We hypothesised high similarity between synthetic images and empirical images, which we showed by analysing and comparing both sets qualitatively and quantitatively. The sets and models are publicly released with the intention to allow performance comparisons between agricultural computer vision methods, to obtain feedback for modelling improvements and to gain further validations on usability of synthetic bootstrapping and empirical fine-tuning. Finally, we provide a brief perspective on our hypothesis that related synthetic dataset bootstrapping and empirical fine-tuning can be used for improved learning.",
          "Keywords Plus": "PEPPER PLANTS; IMAGE; LOCALIZATION; TOOL"
        }
      },
      {
        "title": "Detection of red and bicoloured apples on tree with an RGB-D camera",
        "year": "2016",
        "algorithm": "",
        "performance": "100% of the fully visible apples and 82% of the partially occluded apples",
        "citation": "@article{nguyen2016detection,\n  title={Detection of red and bicoloured apples on tree with an RGB-D camera},\n  author={Nguyen, Tien Thanh and Vandevoorde, Koenraad and Wouters, Niels and Kayacan, Erdal and De Baerdemaeker, Josse G and Saeys, Wouter},\n  journal={Biosystems Engineering},\n  volume={146},\n  pages={33--44},\n  year={2016},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "apples",
        "paper": {
          "relevant": "y",
          "": "\\cite{nguyen2016detection}",
          "Article Title": "Detection of red and bicoloured apples on tree with an RGB-D camera",
          "Times Cited, All Databases": "105",
          "Publication Year": "2016",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{nguyen2016detection,\n  title={Detection of red and bicoloured apples on tree with an RGB-D camera},\n  author={Nguyen, Tien Thanh and Vandevoorde, Koenraad and Wouters, Niels and Kayacan, Erdal and De Baerdemaeker, Josse G and Saeys, Wouter},\n  journal={Biosystems Engineering},\n  volume={146},\n  pages={33--44},\n  year={2016},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "",
          "fruit/veg": "apples",
          "Data Modality": "RGB-D",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": " 100% of the fully visible apples and 82% of the partially occluded apples",
          "challenges": "but have had limited success due to the occlusion of the target fruits by foliage, branches or other fruits as well as due to the nonuniform and unstructured nature of an orchard environment. ",
          "Authors": "Nguyen, TT; Vandevoorde, K; Wouters, N; Kayacan, E; De Baerdemaeker, JG; Saeys, W",
          "Abstract": "Recognising and accurately locating fruits on a tree is a critical challenge in developing fruit-by-fruit robotic harvesting. Many researchers have investigated the potential of red, green, blue (RGB) colour imaging for this purpose, but have had limited success due to the occlusion of the target fruits by foliage, branches or other fruits as well as due to the nonuniform and unstructured nature of an orchard environment. Recently, novel, cost-effective camera systems have become available which provide both colour (RGB) and three dimensional (3D) shape information. As these have shown potential for 3D perception for robots operating in unstructured environments, the potential of such an RGB-D camera for the detection and localisation of red and bicoloured apples on tree was investigated in this study. Images were acquired with this camera system in fruit orchards under a light shield blocking direct sunlight, and an algorithm to detect and localise red and bicoloured apples based on colour and shape features was developed. When the algorithm was applied to the data acquired in these orchards, 100% of the fully visible apples and 82% of the partially occluded apples were detected correctly. The location estimation error was below 10 mm in all the coordinate axes of the Cartesian space. This high detection and location accuracy and short processing time (below 1 s for simultaneous detection of 20 apples), makes the developed algorithm suitable for implementation in a robotic harvesting system, and for yield estimation and orchard monitoring. (C) 2016 IAgrE. Published by Elsevier Ltd. All rights reserved.",
          "Keywords Plus": "FRUIT; SYSTEM; RECOGNITION"
        }
      },
      {
        "title": "Development of An Autonomous Tomato Harvesting Robot with Rotational Plucking Gripper",
        "year": "2016",
        "algorithm": "",
        "performance": "Picking time of 23s",
        "citation": "@inproceedings{yaguchi2016development,\n  title={Development of an autonomous tomato harvesting robot with rotational plucking gripper},\n  author={Yaguchi, Hiroaki and Nagahama, Kotaro and Hasegawa, Takaomi and Inaba, Masayuki},\n  booktitle={2016 IEEE/RSJ international conference on intelligent robots and systems (IROS)},\n  pages={652--657},\n  year={2016},\n  organization={IEEE}\n}\n",
        "fruit_veg": "tomato",
        "paper": {
          "relevant": "y",
          "": "\\cite{yaguchi2016development}",
          "Article Title": "Development of An Autonomous Tomato Harvesting Robot with Rotational Plucking Gripper",
          "Times Cited, All Databases": "103",
          "Publication Year": "2016",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@inproceedings{yaguchi2016development,\n  title={Development of an autonomous tomato harvesting robot with rotational plucking gripper},\n  author={Yaguchi, Hiroaki and Nagahama, Kotaro and Hasegawa, Takaomi and Inaba, Masayuki},\n  booktitle={2016 IEEE/RSJ international conference on intelligent robots and systems (IROS)},\n  pages={652--657},\n  year={2016},\n  organization={IEEE}\n}\n",
          "Document Type": "Proceedings Paper",
          "Main Contribution": "",
          "fruit/veg": "tomato",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "end-effector",
          "Performance": "Picking time of 23s",
          "challenges": " the gripper may grasp multiple fruits in case of very cluttered cluster and the calyx also may be broken when the stem angle is deep from the rotation axis.",
          "Authors": "Yaguchi, H; Nagahama, K; Hasegawa, T; Inaba, M",
          "Abstract": "In this paper, we present a design and development an autonomous tomato harvesting robot. We developed a harvesting robot with stereo camera which can measure depth in short range and direct sunlight and plucking gripper using the infinity rotational joint. We also evaluate the developed robot through harvesting in the tomato robot competition and the real farm. In the tomato robot competition, the robot harvested tometoes from tomato clusters and tomato trees, harvesting speed was about 80[s/fruit] and successful rate was about 60%. In the real farm we evaluated the robot with tomato trees in semi-outdoor environment to show the effectiveness and robustness under direct sunlight. According to the result of harvesting with real tomatoes, we improved the robot motion and finally harvesting speed was up to 23[s/fruit], however the gripper may grasp multiple fruits in case of very cluttered cluster and the calyx also may be broken when the stem angle is deep from the rotation axis. To avoid this situation, a grasp state estimation of the gripper and simultaneous recognition fruit and stem positions are next problems to improve the harvesting successful rate.",
          "Keywords Plus": "DESIGN"
        }
      },
      {
        "title": "Guava Detection and Pose Estimation Using a Low-Cost RGB-D Sensor in the Field",
        "year": "2019",
        "algorithm": "",
        "performance": "the precision and recall of guava fruit detection were 0.983 and 0.948,execution time of 0.565s",
        "citation": "@article{lin2019guava,\n  title={Guava detection and pose estimation using a low-cost RGB-D sensor in the field},\n  author={Lin, Guichao and Tang, Yunchao and Zou, Xiangjun and Xiong, Juntao and Li, Jinhui},\n  journal={Sensors},\n  volume={19},\n  number={2},\n  pages={428},\n  year={2019},\n  publisher={MDPI}\n}",
        "fruit_veg": "guava",
        "paper": {
          "relevant": "y",
          "": "\\cite{lin2019guava}",
          "Article Title": "Guava Detection and Pose Estimation Using a Low-Cost RGB-D Sensor in the Field",
          "Times Cited, All Databases": "96",
          "Publication Year": "2019",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{lin2019guava,\n  title={Guava detection and pose estimation using a low-cost RGB-D sensor in the field},\n  author={Lin, Guichao and Tang, Yunchao and Zou, Xiangjun and Xiong, Juntao and Li, Jinhui},\n  journal={Sensors},\n  volume={19},\n  number={2},\n  pages={428},\n  year={2019},\n  publisher={MDPI}\n}",
          "Document Type": "Article",
          "Main Contribution": "vision: pick point,segmentation",
          "fruit/veg": "guava",
          "Data Modality": "RGB-D",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "the precision and recall of guava fruit detection were 0.983 and 0.948,execution time of 0.565s",
          "challenges": "",
          "Authors": "Lin, GC; Tang, YC; Zou, XJ; Xiong, JT; Li, JH",
          "Abstract": "Fruit detection in real outdoor conditions is necessary for automatic guava harvesting, and the branch-dependent pose of fruits is also crucial to guide a robot to approach and detach the target fruit without colliding with its mother branch. To conduct automatic, collision-free picking, this study investigates a fruit detection and pose estimation method by using a low-cost red-green-blue-depth (RGB-D) sensor. A state-of-the-art fully convolutional network is first deployed to segment the RGB image to output a fruit and branch binary map. Based on the fruit binary map and RGB-D depth image, Euclidean clustering is then applied to group the point cloud into a set of individual fruits. Next, a multiple three-dimensional (3D) line-segments detection method is developed to reconstruct the segmented branches. Finally, the 3D pose of the fruit is estimated using its center position and nearest branch information. A dataset was acquired in an outdoor orchard to evaluate the performance of the proposed method. Quantitative experiments showed that the precision and recall of guava fruit detection were 0.983 and 0.948, respectively; the 3D pose error was 23.43 degrees +/- 14.18 degrees; and the execution time per fruit was 0.565 s. The results demonstrate that the developed method can be applied to a guava-harvesting robot.",
          "Keywords Plus": "GREEN CITRUS-FRUIT; ROBOT; TREE; LOCALIZATION; WIRE"
        }
      },
      {
        "title": "Detection of Fruit-Bearing Branches and Localization of Litchi Clusters for Vision-Based Harvesting Robots",
        "year": "2020",
        "algorithm": "",
        "performance": "an accuracy of 83.3\\%, detection time of 0.464s",
        "citation": "@article{li2020detection,\n  title={Detection of fruit-bearing branches and localization of litchi clusters for vision-based harvesting robots},\n  author={Li, Jinhui and Tang, Yunchao and Zou, Xiangjun and Lin, Guichao and Wang, Hongjun},\n  journal={IEEE Access},\n  volume={8},\n  pages={117746--117758},\n  year={2020},\n  publisher={IEEE}\n}\n",
        "fruit_veg": "Litch",
        "paper": {
          "relevant": "y",
          "": "\\cite{li2020detection}",
          "Article Title": "Detection of Fruit-Bearing Branches and Localization of Litchi Clusters for Vision-Based Harvesting Robots",
          "Times Cited, All Databases": "92",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{li2020detection,\n  title={Detection of fruit-bearing branches and localization of litchi clusters for vision-based harvesting robots},\n  author={Li, Jinhui and Tang, Yunchao and Zou, Xiangjun and Lin, Guichao and Wang, Hongjun},\n  journal={IEEE Access},\n  volume={8},\n  pages={117746--117758},\n  year={2020},\n  publisher={IEEE}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "vision: pick point,segmentation",
          "fruit/veg": "Litch",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "an accuracy of 83.3\\%, detection time of 0.464s",
          "challenges": "",
          "Authors": "Li, JH; Tang, YC; Zou, XJ; Lin, GC; Wang, HJ",
          "Abstract": "Litchi clusters in fruit groves are randomly scattered and occur irregularly, so it is difficult to detect and locate the fruit-bearing branches of multiple litchi clusters at one time. This is a highly challenging task related to continuous operation in the natural environment for visual-based harvesting robots to carry out. In this study, a reliable algorithm based on RGB-depth (RGB-D) cameras in the fields was developed to accurately and automatically detect and locate the fruit-bearing branches of multiple litchi clusters simultaneously in large environments. A semantics segmentation method, Deeplabv3, was employed to segment the RGB images into three categories: background, fruit and twig. A pre-processing step is proposed to align the segmented RGB images and remove the twigs that did not bear fruits. Subsequently, the twig binary map image was processed via skeleton extraction and pruning operations, which left behind only the main branches of twigs. A method for non-parametric density-based spatial clustering of application with noise was used to cluster the pixels in the three-dimensional space of the skeleton map of the branches; thus, the fruit-bearing branches belonging to the same litchi clusters were determined. Finally, a three-dimensional straight line was fitted to each cluster via principal component analysis, and the linear information corresponded to the location of the fruit-bearing branches. In the experiments, 452 pairs of RGB-D images under different illumination were collected to test the proposed algorithm. The results show that the detection accuracy of a litchi fruit-bearing branch is 83.33%, positioning accuracy is 17.29 degrees +/- 24.57 degrees, and execution time for the determination of a single litchi fruit-bearing branch is 0.464s. Field experiments show that this method can effectively guide the robot to complete continuous picking tasks.",
          "Keywords Plus": "TREE; RECOGNITION; ALGORITHM; DESIGN; SYSTEM"
        }
      },
      {
        "title": "Detecting tomatoes in greenhouse scenes by combining AdaBoost classifier and colour analysis",
        "year": "2016",
        "algorithm": "adaboost classifier and color",
        "performance": "over 96% of ripe tomatoes",
        "citation": "@article{zhao2016detecting,\n  title={Detecting tomatoes in greenhouse scenes by combining AdaBoost classifier and colour analysis},\n  author={Zhao, Yuanshen and Gong, Liang and Zhou, Bin and Huang, Yixiang and Liu, Chengliang},\n  journal={Biosystems Engineering},\n  volume={148},\n  pages={127--137},\n  year={2016},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "tomato",
        "paper": {
          "relevant": "y",
          "": "\\cite{zhao2016detecting}",
          "Article Title": "Detecting tomatoes in greenhouse scenes by combining AdaBoost classifier and colour analysis",
          "Times Cited, All Databases": "95",
          "Publication Year": "2016",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{zhao2016detecting,\n  title={Detecting tomatoes in greenhouse scenes by combining AdaBoost classifier and colour analysis},\n  author={Zhao, Yuanshen and Gong, Liang and Zhou, Bin and Huang, Yixiang and Liu, Chengliang},\n  journal={Biosystems Engineering},\n  volume={148},\n  pages={127--137},\n  year={2016},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "detection",
          "fruit/veg": "tomato",
          "Data Modality": "",
          "Learning Algorithm": "adaboost classifier and color",
          "Locomotion": "",
          "Performance": "over 96% of ripe tomatoes",
          "challenges": "",
          "Authors": "Zhao, YS; Gong, L; Zhou, B; Huang, YX; Liu, CL",
          "Abstract": "Despite the rapid development of agricultural robotics, a lack of access to automatic fruit detection and precision picking is limiting the commercial application of harvesting robots. An algorithm for the automatic detection of ripe tomatoes in greenhouse was developed for a simple machine vision system. The images of tomato planting scenes were captured by a colour digital camera, and most of the ripe tomatoes were correctly recognised using the proposed algorithm. The proposed tomato detection approach worked in two steps: (1) by extracting the Haar-like features of grey scale image and classifying with the AdaBoost classifier, the possible tomato objects were identified; (2) the false negatives in the results of classification were eliminated using average pixel value (APV) based colour analysis approach. Comparative test results showed that the C style of Haar-like features and I component image were optimum in the proposed algorithm. The results of validation experiments show that combination of AdaBoost classification and colour analysis can correctly detect over 96% of ripe tomatoes in the real-world environment. However, the false negative rate was about 10% and 3.5% of the tomatoes were not detected. (C) 2016 Published by Elsevier Ltd on behalf of IAgrE.",
          "Keywords Plus": "OBJECT DETECTION; RECOGNITION; FRUIT; APPLES; NUMBER"
        }
      },
      {
        "title": "Fruit detection in natural environment using partial shape matching and probabilistic Hough transform",
        "year": "2020",
        "algorithm": "Hough transform",
        "performance": "",
        "citation": "@article{lin2020fruit,\n  title={Fruit detection in natural environment using partial shape matching and probabilistic Hough transform},\n  author={Lin, Guichao and Tang, Yunchao and Zou, Xiangjun and Cheng, Jiabing and Xiong, Juntao},\n  journal={Precision Agriculture},\n  volume={21},\n  pages={160--177},\n  year={2020},\n  publisher={Springer}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{lin2020fruit}",
          "Article Title": "Fruit detection in natural environment using partial shape matching and probabilistic Hough transform",
          "Times Cited, All Databases": "89",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{lin2020fruit,\n  title={Fruit detection in natural environment using partial shape matching and probabilistic Hough transform},\n  author={Lin, Guichao and Tang, Yunchao and Zou, Xiangjun and Cheng, Jiabing and Xiong, Juntao},\n  journal={Precision Agriculture},\n  volume={21},\n  pages={160--177},\n  year={2020},\n  publisher={Springer}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "detection",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "Hough transform",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Lin, GC; Tang, YC; Zou, XJ; Cheng, JB; Xiong, JT",
          "Abstract": "This paper proposes a novel technique for fruit detection in natural environments which is applicable in automatic harvesting robots, yield estimation systems and quality monitoring systems. As most color-based techniques are highly sensitive to illumination changes and low contrasts between fruits and leaves, the proposed technique, conversely, is based on contour information. Firstly, a discriminative shape descriptor is derived to represent geometrical properties of arbitrary fragment, and applied to a bidirectional partial shape matching to detect sub-fragments of interest that match parts of a reference contour. Then, a novel probabilistic Hough transform is developed to aggregate these sub-fragments for obtaining fruit candidates. Finally, all fruit candidates are verified by a support vector machine classifier trained on color and texture features. Citrus, tomato, pumpkin, bitter gourd, towel gourd and mango datasets were provided. Experiments on these datasets demonstrated that the proposed approach was competitive for detecting most type of fruits, such as green, orange, circular and non-circular, in natural environments.",
          "Keywords Plus": "COLOR IMAGES; RECOGNITION; VISION; APPLES; LOCALIZATION; NUMBER; NIGHT"
        }
      },
      {
        "title": "Automatic method of fruit object extraction under complex agricultural background for vision system of fruit picking robot",
        "year": "2014",
        "algorithm": "",
        "performance": "",
        "citation": "@article{wei2014automatic,\n  title={Automatic method of fruit object extraction under complex agricultural background for vision system of fruit picking robot},\n  author={Wei, Xiangqin and Jia, Kun and Lan, Jinhui and Li, Yuwei and Zeng, Yiliang and Wang, Chunmei},\n  journal={Optik},\n  volume={125},\n  number={19},\n  pages={5684--5689},\n  year={2014},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{wei2014automatic}",
          "Article Title": "Automatic method of fruit object extraction under complex agricultural background for vision system of fruit picking robot",
          "Times Cited, All Databases": "103",
          "Publication Year": "2014",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{wei2014automatic,\n  title={Automatic method of fruit object extraction under complex agricultural background for vision system of fruit picking robot},\n  author={Wei, Xiangqin and Jia, Kun and Lan, Jinhui and Li, Yuwei and Zeng, Yiliang and Wang, Chunmei},\n  journal={Optik},\n  volume={125},\n  number={19},\n  pages={5684--5689},\n  year={2014},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "vision",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Wei, XQ; Jia, K; Lan, JH; Li, YW; Zeng, YL; Wang, CM",
          "Abstract": "Fruit picking robot is required for agricultural automation for fruit harvest, and vision system is the important and crucial composition of a robot system. An automatic extraction method of fruit object under complex agricultural background for vision system in fruit picking robot is presented in this study. The method is based on an improving Otsu threshold algorithm using a new feature in OHTA color space. Color features are extracted in OHTA color space and then used as an input for the Otsu threshold algorithm which calculates the segmentation threshold value automatically. Four kinds of fruit images are selected to validate the automatic extraction method. The fruit objects are automatically extracted with this method and the outputs are presented in binary images. Numerous of experiments show that the automatic extraction method can extract mature fruit from complex agricultural background and the extraction accuracy is more than 95%. The results indicate an effective fruit object extraction method for vision system of fruit picking robot. (C) 2014 Elsevier GmbH. All rights reserved.",
          "Keywords Plus": "COLOR; SEGMENTATION; INFORMATION"
        }
      },
      {
        "title": "Smart farming for improving agricultural management",
        "year": "2021",
        "algorithm": "",
        "performance": "",
        "citation": "@article{mohamed2021smart,\n  title={Smart farming for improving agricultural management},\n  author={Mohamed, Elsayed Said and Belal, AA and Abd-Elmabod, Sameh Kotb and El-Shirbeny, Mohammed A and Gad, A and Zahran, Mohamed B},\n  journal={The Egyptian Journal of Remote Sensing and Space Science},\n  volume={24},\n  number={3},\n  pages={971--981},\n  year={2021},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{mohamed2021smart}",
          "Article Title": "Smart farming for improving agricultural management",
          "Times Cited, All Databases": "83",
          "Publication Year": "2021",
          "Highly Cited Status": "",
          "Publisher": "The Egyptian Journal of Remote Sensing and Space Science",
          "Citation": "@article{mohamed2021smart,\n  title={Smart farming for improving agricultural management},\n  author={Mohamed, Elsayed Said and Belal, AA and Abd-Elmabod, Sameh Kotb and El-Shirbeny, Mohammed A and Gad, A and Zahran, Mohamed B},\n  journal={The Egyptian Journal of Remote Sensing and Space Science},\n  volume={24},\n  number={3},\n  pages={971--981},\n  year={2021},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Mohamed, ES; Belal, A; Abd-Elmabod, SK; El-Shirbeny, MA; Gad, A; Zahran, MB",
          "Abstract": "The food shortage and the population growth are the most challenges facing sustainable development worldwide. Advanced technologies such as artificial intelligence (AI), the Internet of Things (IoT), and the mobile internet can provide realistic solutions to the challenges that are facing the world. Therefore, this work focuses on the new approaches regarding smart farming (SF) from 2019 to 2021, where the work illustrates the data gathering, transmission, storage, analysis, and also, suitable solutions. IoT is one of the essential pillars in smart systems, as it connects sensor devices to perform various basic tasks. The smart irrigation system included those sensors for monitoring water level, irrigation efficiency, climate, etc. Smart irrigation is based on smart controllers and sensors as well as some mathematical relations. In addition, this work illustrated the application of unmanned aerial vehicles (UAV) and robots, where they can be achieved several functions such as harvesting, seedling, weed detection, irrigation, spraying of agricultural pests, livestock applications, etc. real-time using IoT, artificial intelligence (AI), deep learning (DL), machine learning (ML) and wireless communications. Moreover, this work demonstrates the importance of using a 5G mobile network in developing smart systems, as it leads to highspeed data transfer, up to 20 Gbps, and can link a large number of devices per square kilometer. Although the applications of smart farming in developing countries are facing several challenges, this work highlighted some approaches the smart farming. In addition, the implementation of Smart Decision Support Systems (SDSS) in developing countries supports the real-time analysis, mapping of soil characteristics and also helps to make proper decision management. Finally, smart agriculture in developing countries needs more support from governments at the small farms and the private sector.(c) 2021 National Authority for Remote Sensing and Space Sciences. Production and hosting by Elsevier B. V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/bync-nd/4.0/).",
          "Keywords Plus": "LAND; POTENTIALITY; RESOURCES; BASIN"
        }
      },
      {
        "title": "Design and test of robotic harvesting system for cherry tomato",
        "year": "2018",
        "algorithm": "",
        "performance": "",
        "citation": "@article{mohamed2021smart,\n  title={Smart farming for improving agricultural management},\n  author={Mohamed, Elsayed Said and Belal, AA and Abd-Elmabod, Sameh Kotb and El-Shirbeny, Mohammed A and Gad, A and Zahran, Mohamed B},\n  journal={The Egyptian Journal of Remote Sensing and Space Science},\n  volume={24},\n  number={3},\n  pages={971--981},\n  year={2021},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "tomato",
        "paper": {
          "relevant": "y",
          "": "\\cite{mohamed2021smart}",
          "Article Title": "Design and test of robotic harvesting system for cherry tomato",
          "Times Cited, All Databases": "91",
          "Publication Year": "2018",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{mohamed2021smart,\n  title={Smart farming for improving agricultural management},\n  author={Mohamed, Elsayed Said and Belal, AA and Abd-Elmabod, Sameh Kotb and El-Shirbeny, Mohammed A and Gad, A and Zahran, Mohamed B},\n  journal={The Egyptian Journal of Remote Sensing and Space Science},\n  volume={24},\n  number={3},\n  pages={971--981},\n  year={2021},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "",
          "fruit/veg": "tomato",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Feng, QC; Zou, W; Fan, PF; Zhang, CF; Wang, X",
          "Abstract": "Harvesting of fresh-eating cherry tomato was highly costly on labor and time. In order to achieve mechanical harvesting for the fresh-eating tomato, a new harvesting robot was designed, which consisted of a stereo visual unit, an end-effector, manipulator, a fruit collector, and a railed vehicle. The robot configuration and workflow design focused on the special cultivating condition. Three key parts were introduced in detail: a railroad vehicle capably moving on both ground and rail was adopted as the robot's carrier, a visual servo unit was used to identify and locate the mature fruits bunch, and the end-effector to hold and separate the fruit bunch was designed based on the stalk's mechanical features. The field test of the new developed robot was conducted and the results were analyzed. The successful harvest rate of the robot was 83%, however, each successful harvest averagely needed 1.4 times attempt, and a single successful harvesting cycle cost 8 s excluding the time cost on moving.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "iPathology: Robotic Applications and Management of Plants and Plant Diseases",
        "year": "2017",
        "algorithm": "",
        "performance": "",
        "citation": "@article{ampatzidis2017ipathology,\n  title={iPathology: robotic applications and management of plants and plant diseases},\n  author={Ampatzidis, Yiannis and De Bellis, Luigi and Luvisi, Andrea},\n  journal={Sustainability},\n  volume={9},\n  number={6},\n  pages={1010},\n  year={2017},\n  publisher={MDPI}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{ampatzidis2017ipathology}",
          "Article Title": "iPathology: Robotic Applications and Management of Plants and Plant Diseases",
          "Times Cited, All Databases": "86",
          "Publication Year": "2017",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{ampatzidis2017ipathology,\n  title={iPathology: robotic applications and management of plants and plant diseases},\n  author={Ampatzidis, Yiannis and De Bellis, Luigi and Luvisi, Andrea},\n  journal={Sustainability},\n  volume={9},\n  number={6},\n  pages={1010},\n  year={2017},\n  publisher={MDPI}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "diseases management",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Ampatzidis, Y; De Bellis, L; Luvisi, A",
          "Abstract": "The rapid development of new technologies and the changing landscape of the online world (e.g., Internet of Things (IoT), Internet of All, cloud-based solutions) provide a unique opportunity for developing automated and robotic systems for urban farming, agriculture, and forestry. Technological advances in machine vision, global positioning systems, laser technologies, actuators, and mechatronics have enabled the development and implementation of robotic systems and intelligent technologies for precision agriculture. Herein, we present and review robotic applications on plant pathology and management, and emerging agricultural technologies for intra-urban agriculture. Greenhouse advanced management systems and technologies have been greatly developed in the last years, integrating IoT and WSN (Wireless Sensor Network). Machine learning, machine vision, and AI (Artificial Intelligence) have been utilized and applied in agriculture for automated and robotic farming. Intelligence technologies, using machine vision/learning, have been developed not only for planting, irrigation, weeding (to some extent), pruning, and harvesting, but also for plant disease detection and identification. However, plant disease detection still represents an intriguing challenge, for both abiotic and biotic stress. Many recognition methods and technologies for identifying plant disease symptoms have been successfully developed; still, the majority of them require a controlled environment for data acquisition to avoid false positives. Machine learning methods (e.g., deep and transfer learning) present promising results for improving image processing and plant symptom identification. Nevertheless, diagnostic specificity is a challenge for microorganism control and should drive the development of mechatronics and robotic solutions for disease management.",
          "Keywords Plus": "MACHINE VISION; PRECISION AGRICULTURE; IMAGE-ANALYSIS; SYSTEMS; INFORMATION; FUTURE; IDENTIFICATION; TECHNOLOGY; SENSORS; DESIGN"
        }
      },
      {
        "title": "A field-tested robotic harvesting system for iceberg lettuce",
        "year": "2020",
        "algorithm": "",
        "performance": "",
        "citation": "@article{birrell2020field,\n  title={A field-tested robotic harvesting system for iceberg lettuce},\n  author={Birrell, Simon and Hughes, Josie and Cai, Julia Y and Iida, Fumiya},\n  journal={Journal of Field Robotics},\n  volume={37},\n  number={2},\n  pages={225--245},\n  year={2020},\n  publisher={Wiley Online Library}\n}",
        "fruit_veg": "iceberg lettuce",
        "paper": {
          "relevant": "y",
          "": "\\cite{birrell2020field}",
          "Article Title": "A field-tested robotic harvesting system for iceberg lettuce",
          "Times Cited, All Databases": "89",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{birrell2020field,\n  title={A field-tested robotic harvesting system for iceberg lettuce},\n  author={Birrell, Simon and Hughes, Josie and Cai, Julia Y and Iida, Fumiya},\n  journal={Journal of Field Robotics},\n  volume={37},\n  number={2},\n  pages={225--245},\n  year={2020},\n  publisher={Wiley Online Library}\n}",
          "Document Type": "Article",
          "Main Contribution": "",
          "fruit/veg": "iceberg lettuce",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Birrell, S; Hughes, J; Cai, JY; Iida, F",
          "Abstract": "Agriculture provides an unique opportunity for the development of robotic systems; robots must be developed which can operate in harsh conditions and in highly uncertain and unknown environments. One particular challenge is performing manipulation for autonomous robotic harvesting. This paper describes recent and current work to automate the harvesting of iceberg lettuce. Unlike many other produce, iceberg is challenging to harvest as the crop is easily damaged by handling and is very hard to detect visually. A platform called Vegebot has been developed to enable the iterative development and field testing of the solution, which comprises of a vision system, custom end effector and software. To address the harvesting challenges posed by iceberg lettuce a bespoke vision and learning system has been developed which uses two integrated convolutional neural networks to achieve classification and localization. A custom end effector has been developed to allow damage free harvesting. To allow this end effector to achieve repeatable and consistent harvesting, a control method using force feedback allows detection of the ground. The system has been tested in the field, with experimental evidence gained which demonstrates the success of the vision system to localize and classify the lettuce, and the full integrated system to harvest lettuce. This study demonstrates how existing state-of-the art vision approaches can be applied to agricultural robotics, and mechanical systems can be developed which leverage the environmental constraints imposed in such environments.",
          "Keywords Plus": "AGRICULTURAL ROBOT; AUTONOMOUS ROBOT; VINEYARD; PLANTS"
        }
      },
      {
        "title": "Autobot for Precision Farming",
        "year": "2017",
        "algorithm": "",
        "performance": "",
        "citation": "@article{hariharanautobot,\n  title={Autobot for Precision Farming},\n  author={Hariharan, Abhishek and Solomon, Nishanth and DevaDharshini, U and Saranghan, M and Vignajeth, KK}\n}",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{hariharanautobot}",
          "Article Title": "Autobot for Precision Farming",
          "Times Cited, All Databases": "85",
          "Publication Year": "2017",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{hariharanautobot,\n  title={Autobot for Precision Farming},\n  author={Hariharan, Abhishek and Solomon, Nishanth and DevaDharshini, U and Saranghan, M and Vignajeth, KK}\n}",
          "Document Type": "Proceedings Paper",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Abu Basim, NM; Hariharan, GA; Solomon, N; DevaDharshini, U; Banu, NR; Saranghan, M; Vignajeth, KK",
          "Abstract": "Agriculture is considered as one of the traditional occupations in India, due to the fact of specialization and skills we possess. Ancient farmer used their advanced skills and techniques to yields production from various sources, but in today's scenario farmers are striving hard to cultivate the land and yield the production. The farmers today are not the same as yesterdays and the farmers present today may not be present tomorrow. Hence it is necessary to conserve the knowledge and skills keeping this in mind we have developed a robot to this critical situation with mechanisms connected together. Right from sowing to harvesting the system guides and controls the growth.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Deep learning based segmentation for automated training of apple trees on trellis wires",
        "year": "2020",
        "algorithm": "",
        "performance": "",
        "citation": "@article{majeed2020deep,\n  title={Deep learning based segmentation for automated training of apple trees on trellis wires},\n  author={Majeed, Yaqoob and Zhang, Jing and Zhang, Xin and Fu, Longsheng and Karkee, Manoj and Zhang, Qin and Whiting, Matthew D},\n  journal={Computers and Electronics in Agriculture},\n  volume={170},\n  pages={105277},\n  year={2020},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "apples",
        "paper": {
          "relevant": "y",
          "": "\\cite{majeed2020deep}",
          "Article Title": "Deep learning based segmentation for automated training of apple trees on trellis wires",
          "Times Cited, All Databases": "88",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{majeed2020deep,\n  title={Deep learning based segmentation for automated training of apple trees on trellis wires},\n  author={Majeed, Yaqoob and Zhang, Jing and Zhang, Xin and Fu, Longsheng and Karkee, Manoj and Zhang, Qin and Whiting, Matthew D},\n  journal={Computers and Electronics in Agriculture},\n  volume={170},\n  pages={105277},\n  year={2020},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "detection and segmentation",
          "fruit/veg": "apples",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Majeed, Y; Zhang, J; Zhang, X; Fu, LS; Karkee, M; Zhang, Q; Whiting, MD",
          "Abstract": "Trellised fruiting-wall training systems are becoming standard for modern apple orchards due to their high fruit yield and quality, and their suitability to robotic operations in pruning and harvesting. In a common practice of training young apple trees to a trellis-trained canopy system in PNW region of US, trees branches are manually selected and then tied to horizontal trellis wires in 6 or 7 tiers. As manual training of apple trees to these modern orchard architectures is becoming challenging due to less availability of skilled human labor with quickly increased labor cost, automated training using sensing and robotic techniques could be an alternative solution. Segmenting trunks, branches, and trellis wires is a critical step in automating a tree training operation. In this study, a deep learning-based semantic segmentation method was developed for automatically performing this segmentation task. A Kinect V2 sensor was used to obtain the RGB and point cloud data of target trees. Then both Simple- and Foreground-RGB images were used for training a convolutional neural network (CNN)-based segmentation network (SegNet) to segment the trunk, branch, and trellis wire. Trunks and branches, which share some common features, were segmented from each other with accuracies of 0.82 and 0.89 for Simple-RGB images and 0.91 and 0.92 for Foreground-RGB images, respectively. Similarly, trellis wires, which have distinct features from both the trunk and branches, were segmented with accuracies of 0.92 and 0.97 for the Simple- and Foreground-RGB images, respectively. Obtained results showed that the performance of the developed semantic segmentation technique was better with Foreground-RGB images compared to the same with Simple-RGB images. Accuracy in identifying the segmented region boundaries in Foreground-RGB images, represented by the Boundary-F1 score, was 0.93, 0.89, and 0.91 for the trunk, branch, and trellis wire, respectively. These results showed a promising potential for adopting deep learning-based semantic segmentation for automating apple tree training in orchard environment.",
          "Keywords Plus": "RGB; SYSTEM; RECONSTRUCTION; FIELD; ARCHITECTURE; AGRICULTURE; BRANCHES; COLOR"
        }
      },
      {
        "title": "Date Fruit Classification for Robotic Harvesting in a Natural Environment Using Deep Learning",
        "year": "2019",
        "algorithm": "deep learning",
        "performance": "",
        "citation": "@article{altaheri2019date,\n  title={Date fruit classification for robotic harvesting in a natural environment using deep learning},\n  author={Altaheri, Hamdi and Alsulaiman, Mansour and Muhammad, Ghulam},\n  journal={IEEE Access},\n  volume={7},\n  pages={117115--117133},\n  year={2019},\n  publisher={IEEE}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{altaheri2019date}",
          "Article Title": "Date Fruit Classification for Robotic Harvesting in a Natural Environment Using Deep Learning",
          "Times Cited, All Databases": "86",
          "Publication Year": "2019",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{altaheri2019date,\n  title={Date fruit classification for robotic harvesting in a natural environment using deep learning},\n  author={Altaheri, Hamdi and Alsulaiman, Mansour and Muhammad, Ghulam},\n  journal={IEEE Access},\n  volume={7},\n  pages={117115--117133},\n  year={2019},\n  publisher={IEEE}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "fruit classification",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "deep learning",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Altaheri, H; Alsulaiman, M; Muhammad, G",
          "Abstract": "An accurate vision system to classify and analyze fruits in real time is critical for harvesting robots to be cost-effective and efficient. However, practical success in this area is still limited, and to the best of our knowledge, there is no research in the area of machine vision for date fruits in an orchard environment. In this work, we propose an efficient machine vision framework for date fruit harvesting robots. The framework consists of three classification models used to classify date fruit images in real time according to their type, maturity, and harvesting decision. In the classification models, deep convolutional neural networks are utilized with transfer learning and fine-tuning on pre-trained models. To build a robust vision system, we create a rich image dataset of date fruit bunches in an orchard that consists of more than 8000 images of five date types in different pre-maturity and maturity stages. The dataset has a large degree of variations that reflects the challenges in the date orchard environment including variations in angles, scales, illumination conditions, and date bunches covered by bags. The proposed date fruit classification models achieve accuracies of 99.01%, 97.25%, and 98.59% with classification times of 20.6, 20.7, and 35.9 msec for the type, maturity, and harvesting decision classification tasks, respectively.",
          "Keywords Plus": "NEURAL-NETWORKS; MATURITY"
        }
      },
      {
        "title": "Localisation of litchi in an unstructured environment using binocular stereo vision",
        "year": "2016",
        "algorithm": "",
        "performance": "",
        "citation": "@article{wang2016localisation,\n  title={Localisation of litchi in an unstructured environment using binocular stereo vision},\n  author={Wang, Chenglin and Zou, Xiangjun and Tang, Yunchao and Luo, Lufeng and Feng, Wenxian},\n  journal={Biosystems Engineering},\n  volume={145},\n  pages={39--51},\n  year={2016},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "Litch",
        "paper": {
          "relevant": "y",
          "": "\\cite{wang2016localisation}",
          "Article Title": "Localisation of litchi in an unstructured environment using binocular stereo vision",
          "Times Cited, All Databases": "89",
          "Publication Year": "2016",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{wang2016localisation,\n  title={Localisation of litchi in an unstructured environment using binocular stereo vision},\n  author={Wang, Chenglin and Zou, Xiangjun and Tang, Yunchao and Luo, Lufeng and Feng, Wenxian},\n  journal={Biosystems Engineering},\n  volume={145},\n  pages={39--51},\n  year={2016},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "vision:localization",
          "fruit/veg": "Litch",
          "Data Modality": "bionocular",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Wang, CL; Zou, XJ; Tang, YC; Luo, LF; Feng, WX",
          "Abstract": "The major constraints for a litchi harvesting robot were to recognise and locate litchi in an unstructured environment with varying illumination and random occlusion. A rapid and reliable method based on binocular stereo vision was developed with the aim of effectively recognising and locating litchi in the natural environment. The method involved the application of wavelet transform to a pair acquired images of litchi to normalise illumination of an object surface. A litchi recognition algorithm based on K-means clustering was presented to separate litchi from leaves, branches and background. A matching algorithm to locate litchi based on a label template was discussed. Litchis with a similar label template were matched according to the preset threshold by traversing a litchi label template of a left image in a right image to find optimal matching. The experimental results showed that the proposed recognition method could be robust against the influences of varying illumination and precisely recognising litchi, the highest average recognition rate for unoccluded and partially occluded litchi was 98.8% and 97.5% respectively. From 100 pairs of tested images of unoccluded and partially occluded litchis 98% and 94% were successfully matched, respectively. Errors had no significant difference and they were less than 15 mm when the measuring distance was between 300 mm and 1600 mm under varying illumination and partially occluded conditions. (C) 2016 IAgrE. Published by Elsevier Ltd. All rights reserved.",
          "Keywords Plus": "HARVESTING ROBOT; FRUIT"
        }
      },
      {
        "title": "Location of apples in trees using stereoscopic vision",
        "year": "2015",
        "algorithm": "",
        "performance": "",
        "citation": "@article{si2015location,\n  title={Location of apples in trees using stereoscopic vision},\n  author={Si, Yongsheng and Liu, Gang and Feng, Juan},\n  journal={Computers and Electronics in Agriculture},\n  volume={112},\n  pages={68--74},\n  year={2015},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "apples",
        "paper": {
          "relevant": "y",
          "": "\\cite{si2015location}",
          "Article Title": "Location of apples in trees using stereoscopic vision",
          "Times Cited, All Databases": "87",
          "Publication Year": "2015",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{si2015location,\n  title={Location of apples in trees using stereoscopic vision},\n  author={Si, Yongsheng and Liu, Gang and Feng, Juan},\n  journal={Computers and Electronics in Agriculture},\n  volume={112},\n  pages={68--74},\n  year={2015},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "vision:localization",
          "fruit/veg": "apples",
          "Data Modality": "3D",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Si, YS; Gang, L; Juan, F",
          "Abstract": "In order to design a robot which can automatically recognize and locate apples for harvesting, a machine vision system was developed. Three algorithms used in the vision system to recognize and locate apples were described in this study. An apple recognition algorithm with color difference R - G and color difference ratio (R - G)/(G - B) was presented. If a pixel met R - G > 0 and (R - G)/(G - B) > 1, either the pixel was identified as an apple, else the pixel was background. The fruit shape features were extracted from contour images based on random ring method (RRM). A matching algorithm based on area and epipolar geometry was discussed to locate the apples. The apples with similar areas were matched according to the principle of,ordering constraint by calculating the maximum value of cross-correlation function of vertical projections. The experiment results showed that the proposed recognition method could eliminate the influences of shade and soil. Over 89.5% of fruits were successfully recognized. from 160 tested images. The circle centers and radii were extracted precisely based on the random, ring method. The errors were less than 20 mm when the measuring distance was between 400 mm and 1500 mm. (C) 2015 Elsevier B.V. All rights reserved.",
          "Keywords Plus": "LOCALIZATION; PICKING; SYSTEM; IMAGES; ROBOT"
        }
      },
      {
        "title": "The Status and Future of the Strawberry Industry in the United States",
        "year": "2019",
        "algorithm": "",
        "performance": "",
        "citation": "@article{samtani2019status,\n  title={The status and future of the strawberry industry in the United States},\n  author={Samtani, Jayesh B and Rom, Curt R and Friedrich, Heather and Fennimore, Steven A and Finn, Chad E and Petran, Andrew and Wallace, Russell W and Pritts, Marvin P and Fernandez, Gina and Chase, Carlene A and others},\n  journal={HortTechnology},\n  volume={29},\n  number={1},\n  pages={11--24},\n  year={2019},\n  publisher={American Society for Horticultural Science}\n}\n",
        "fruit_veg": "strawberry",
        "paper": {
          "relevant": "y",
          "": "\\cite{samtani2019status}",
          "Article Title": "The Status and Future of the Strawberry Industry in the United States",
          "Times Cited, All Databases": "103",
          "Publication Year": "2019",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{samtani2019status,\n  title={The status and future of the strawberry industry in the United States},\n  author={Samtani, Jayesh B and Rom, Curt R and Friedrich, Heather and Fennimore, Steven A and Finn, Chad E and Petran, Andrew and Wallace, Russell W and Pritts, Marvin P and Fernandez, Gina and Chase, Carlene A and others},\n  journal={HortTechnology},\n  volume={29},\n  number={1},\n  pages={11--24},\n  year={2019},\n  publisher={American Society for Horticultural Science}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "",
          "fruit/veg": "strawberry",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Samtani, JB; Rom, CR; Friedrich, H; Fennimore, SA; Finn, CE; Petran, A; Wallace, RW; Pritts, MP; Fernandez, G; Chase, CA; Kubota, C; Bergefurd, B",
          "Abstract": "Strawberry (Fragaria xananassa) production practices followed by growers in the United States vary by region. Understanding the challenges, needs, and opportunities in each region is essential to guide research, policy, and marketing strategies for the strawberry industry across the country, and to enable the development of general and region-specific educational and production tools. This review divided the United States into eight distinct geographic regions and an indoor controlled or protected environment production system. Current production systems, markets, cultivars, trends, and future directions for each region are discussed. A common trend across all regions is the increasing use of protected culture strawberry production with both day-neutral and short-day cultivars for season extension to meet consumer demand for year-round availability. All regions experience challenges with pests and obtaining adequate harvest labor. Increasing consumer demand for berries, climate change-induced weather variability, high pesticide use, labor and immigration policies, and land availability impact regional production, thus facilitating the adoption of new technologies such as robotics and network communications to assist with strawberry harvesting in open-field production and production under controlled-environment agriculture and protected culture.",
          "Keywords Plus": "DAY-NEUTRAL STRAWBERRY; SOIL DISINFESTATION; WEED-CONTROL; HIGH-TUNNEL; ANTHRACNOSE; YIELD; STRATEGIES; MANAGEMENT; RESISTANCE"
        }
      },
      {
        "title": "Vision-based extraction of spatial information in grape clusters for harvesting robots",
        "year": "2016",
        "algorithm": "",
        "performance": "",
        "citation": "@article{luo2016vision,\n  title={Vision-based extraction of spatial information in grape clusters for harvesting robots},\n  author={Luo, Lufeng and Tang, Yunchao and Zou, Xiangjun and Ye, Min and Feng, Wenxian and Li, Guoqing},\n  journal={Biosystems Engineering},\n  volume={151},\n  pages={90--104},\n  year={2016},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "grape",
        "paper": {
          "relevant": "y",
          "": "\\cite{luo2016vision}",
          "Article Title": "Vision-based extraction of spatial information in grape clusters for harvesting robots",
          "Times Cited, All Databases": "90",
          "Publication Year": "2016",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{luo2016vision,\n  title={Vision-based extraction of spatial information in grape clusters for harvesting robots},\n  author={Luo, Lufeng and Tang, Yunchao and Zou, Xiangjun and Ye, Min and Feng, Wenxian and Li, Guoqing},\n  journal={Biosystems Engineering},\n  volume={151},\n  pages={90--104},\n  year={2016},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "vision:localization",
          "fruit/veg": "grape",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Luo, LF; Tang, YC; Zou, XJ; Ye, M; Feng, WX; Li, GQ",
          "Abstract": "Grapes are likely to have collisions and be damaged by manipulations when harvesting grape clusters. To conduct an undamaged robotic harvesting, this paper focuses mainly on locating the spatial coordinates of the cutting points on a peduncle of grape clusters for the end-effector and determining the bounding volume of the grape clusters for the motion planner of the manipulator. A method for acquiring spatial information from grape clusters is presented based on binocular stereo vision. This method includes four steps: (1) calibrating the binocular cameras and rectifying the images, (2) detecting the cutting points on the peduncle and the centres of the grape berries, (3) extracting three-dimensional spatial coordinates of the points detected in step 2, and (4) calculating the bounding volume of the grape clusters. A total of 300 images were captured in the vineyard and were tested to validate the method for the cutting point detection, and the success rate was approximately 87%. The accuracy of the localisation of the cutting points was determined under outdoor conditions, and the accuracy in the Z and X directions was 12 mm and 9 mm, respectively. The acquired bounding volume of the grape cluster was compared with manual measurements, and errors in the height and maximum diameter were less than 17 mm and 19 mm, respectively. The elapsed time of the whole algorithm was less than 0.7 s. The demonstrated performance of this developed method indicated that it could be used on harvesting robots. (C) 2016 IAgrE. Published by Elsevier Ltd. All rights reserved.",
          "Keywords Plus": "AUTONOMOUS ROBOT; LOCALIZATION; ENVIRONMENT; DESIGN; WIRE"
        }
      },
      {
        "title": "A vision methodology for harvesting robot to detect cutting points on peduncles of double overlapping grape clusters in a vineyard",
        "year": "2018",
        "algorithm": "",
        "performance": "",
        "citation": "@article{luo2018vision,\n  title={A vision methodology for harvesting robot to detect cutting points on peduncles of double overlapping grape clusters in a vineyard},\n  author={Luo, Lufeng and Tang, Yunchao and Lu, Qinghua and Chen, Xiong and Zhang, Po and Zou, Xiangjun},\n  journal={Computers in industry},\n  volume={99},\n  pages={130--139},\n  year={2018},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "grape",
        "paper": {
          "relevant": "y",
          "": "\\cite{luo2018vision}",
          "Article Title": "A vision methodology for harvesting robot to detect cutting points on peduncles of double overlapping grape clusters in a vineyard",
          "Times Cited, All Databases": "82",
          "Publication Year": "2018",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{luo2018vision,\n  title={A vision methodology for harvesting robot to detect cutting points on peduncles of double overlapping grape clusters in a vineyard},\n  author={Luo, Lufeng and Tang, Yunchao and Lu, Qinghua and Chen, Xiong and Zhang, Po and Zou, Xiangjun},\n  journal={Computers in industry},\n  volume={99},\n  pages={130--139},\n  year={2018},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "vision: pick point",
          "fruit/veg": "grape",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Luo, LF; Tang, YC; Lu, QH; Chen, X; Zhang, P; Zou, XJ",
          "Abstract": "Reliable and robust vision algorithms to detect the cutting points on peduncles of overlapping grape clusters in the unstructured vineyard are essential for efficient use of a harvesting robot. In this study, we designed an approach to detect these cutting points in three main steps. First, the areas of pixels representing grape clusters in vineyard images were obtained using a segmentation algorithm based on k-means clustering and an effective color component. Next, the edge images of grape clusters were extracted, and then a geometric model was used to obtain the contour intersection points of double overlapping grape clusters. Profile analysis was used to separate the regional pixels of double grape clusters by a line connecting double intersection points. Finally, the region of interest of the peduncle for each grape clusters was determined based on the geometric information of each pixel region, and a computational method was used to determine the appropriate cutting point on the peduncle of each grape cluster by use of a geometric constraint method. Thirty vineyard images that were captured from different perspectives were tested to validate the performance of the presented approach in a complex environment. The average recognition accuracy was 88.33%, and the success rate of visual detection of the cutting point on the peduncle of double overlapping grape clusters was 81.66%. The demonstrated performance of this developed method indicated that it could be used by harvesting robots. (C) 2018 Elsevier B.V. All rights reserved.",
          "Keywords Plus": "FIELD-TEST; LOCALIZATION; ENVIRONMENT; DESIGN; IMAGES"
        }
      },
      {
        "title": "Real-Time Fruit Recognition and Grasping Estimation for Robotic Apple Harvesting",
        "year": "2020",
        "algorithm": "",
        "performance": "",
        "citation": "@article{kang2020real,\n  title={Real-time fruit recognition and grasping estimation for robotic apple harvesting},\n  author={Kang, Hanwen and Zhou, Hongyu and Wang, Xing and Chen, Chao},\n  journal={Sensors},\n  volume={20},\n  number={19},\n  pages={5670},\n  year={2020},\n  publisher={MDPI}\n}\n",
        "fruit_veg": "apples",
        "paper": {
          "relevant": "y",
          "": "\\cite{kang2020real}",
          "Article Title": "Real-Time Fruit Recognition and Grasping Estimation for Robotic Apple Harvesting",
          "Times Cited, All Databases": "75",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{kang2020real,\n  title={Real-time fruit recognition and grasping estimation for robotic apple harvesting},\n  author={Kang, Hanwen and Zhou, Hongyu and Wang, Xing and Chen, Chao},\n  journal={Sensors},\n  volume={20},\n  number={19},\n  pages={5670},\n  year={2020},\n  publisher={MDPI}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "recognition and grasp",
          "fruit/veg": "apples",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Kang, HW; Zhou, HY; Wang, X; Chen, C",
          "Abstract": "Robotic harvesting shows a promising aspect in future development of agricultural industry. However, there are many challenges which are still presented in the development of a fully functional robotic harvesting system. Vision is one of the most important keys among these challenges. Traditional vision methods always suffer from defects in accuracy, robustness, and efficiency in real implementation environments. In this work, a fully deep learning-based vision method for autonomous apple harvesting is developed and evaluated. The developed method includes a light-weight one-stage detection and segmentation network for fruit recognition and a PointNet to process the point clouds and estimate a proper approach pose for each fruit before grasping. Fruit recognition network takes raw inputs from RGB-D camera and performs fruit detection and instance segmentation on RGB images. The PointNet grasping network combines depth information and results from the fruit recognition as input and outputs the approach pose of each fruit for robotic arm execution. The developed vision method is evaluated on RGB-D image data which are collected from both laboratory and orchard environments. Robotic harvesting experiments in both indoor and outdoor conditions are also included to validate the performance of the developed harvesting system. Experimental results show that the developed vision method can perform highly efficient and accurate to guide robotic harvesting. Overall, the developed robotic harvesting system achieves 0.8 on harvesting success rate and cycle time is 6.5 s.",
          "Keywords Plus": "DEEP; PERCEPTION"
        }
      },
      {
        "title": "Colour-agnostic shape-based 3D fruit detection for crop harvesting robots",
        "year": "2016",
        "algorithm": "",
        "performance": "",
        "citation": "@article{barnea2016colour,\n  title={Colour-agnostic shape-based 3D fruit detection for crop harvesting robots},\n  author={Barnea, Ehud and Mairon, Rotem and Ben-Shahar, Ohad},\n  journal={Biosystems Engineering},\n  volume={146},\n  pages={57--70},\n  year={2016},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{e{barnea2016colour}",
          "Article Title": "Colour-agnostic shape-based 3D fruit detection for crop harvesting robots",
          "Times Cited, All Databases": "80",
          "Publication Year": "2016",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{barnea2016colour,\n  title={Colour-agnostic shape-based 3D fruit detection for crop harvesting robots},\n  author={Barnea, Ehud and Mairon, Rotem and Ben-Shahar, Ohad},\n  journal={Biosystems Engineering},\n  volume={146},\n  pages={57--70},\n  year={2016},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "color, shape detection",
          "fruit/veg": "",
          "Data Modality": "3D",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Barnea, E; Mairon, R; Ben-Shahar, O",
          "Abstract": "Most agricultural robots, fruit harvesting systems in particular, use computer vision to detect their fruit targets. Exploiting the uniqueness of fruit colour amidst the foliage, almost all of these computer vision systems rely on colour features to identify the fruit in the image. However, often the colour of fruit cannot be discriminated from its background, especially under unstable illumination conditions, thus rendering the detection and segmentation of the target highly sensitive or unfeasible in colour space. While multispectral signals, especially those outside the visible spectrum, may alleviate this difficulty, simpler, cheaper, and more accessible solutions are desired. Here exploiting both RGB and range data to analyse shape-related features of objects both in the image plane and 3D space is proposed. In particular, 3D surface normal features, 3D plane-reflective symmetry, and image plane highlights from elliptic surface points are combined to provide shape-based detection of fruits in 3D space regardless of their colour. Results are shown using a particularly challenging sweet pepper dataset with a significant degree of occlusions. (C) 2016 IAgrE. Published by Elsevier Ltd. All rights reserved.",
          "Keywords Plus": "REFLECTION COMPONENTS; RECOGNITION; SYSTEM"
        }
      },
      {
        "title": "Design of an eye-in-hand sensing and servo control framework for harvesting robotics in dense vegetation",
        "year": "2016",
        "algorithm": "",
        "performance": "",
        "citation": "@article{barth2016design,\n  title={Design of an eye-in-hand sensing and servo control framework for harvesting robotics in dense vegetation},\n  author={Barth, Ruud and Hemming, Jochen and van Henten, Eldert J},\n  journal={Biosystems Engineering},\n  volume={146},\n  pages={71--84},\n  year={2016},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{barth2016design}",
          "Article Title": "Design of an eye-in-hand sensing and servo control framework for harvesting robotics in dense vegetation",
          "Times Cited, All Databases": "81",
          "Publication Year": "2016",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{barth2016design,\n  title={Design of an eye-in-hand sensing and servo control framework for harvesting robotics in dense vegetation},\n  author={Barth, Ruud and Hemming, Jochen and van Henten, Eldert J},\n  journal={Biosystems Engineering},\n  volume={146},\n  pages={71--84},\n  year={2016},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Barth, R; Hemming, J; van Henten, EJ",
          "Abstract": "A modular software framework design that allows flexible implementation of eye-in-hand sensing and motion control for agricultural robotics in dense vegetation is reported. Harvesting robots in cultivars with dense vegetation require multiple viewpoints and on-line trajectory adjustments in order to reduce the amount of false negatives and correct for fruit movement. In contrast to specialised software, the framework proposed aims to support a wide variety of agricultural use cases, hardware and extensions. A set of Robotic Operating System (ROS) nodes was created to ensure modularity and separation of concems, implementing functionalities for application control, robot motion control, image acquisition, fruit detection, visual servo control and simultaneous localisation and mapping (SLAM) for monocular relative depth estimation and scene reconstruction. Coordination functionality was implemented by the application control node with a finite state machine. In order to provide visual servo control and simultaneous localisation and mapping functionalities, off-the-shelf libraries Visual Servoing Platform library (ViSP) and Large Scale Direct SLAM (LSD-SLAM) were wrapped in ROS nodes. The capabilities of the framework are demonstrated by an example implementation for use with a sweet-pepper crop, combined with hardware consisting of a Baxter robot and a colour camera placed on its end-effector. Qualitative tests were performed under laboratory conditions using an artificial dense vegetation sweet-pepper crop. Results indicated the framework can be implemented for sensing and robot motion control in sweet-pepper using visual information from the end-effector. Future research to apply the framework to other use-cases and validate the performance of its components in servo applications under real greenhouse conditions is suggested. (C) 2015 The Authors. Published by Elsevier Ltd on behalf of IAgrE.",
          "Keywords Plus": "LOCALIZATION"
        }
      },
      {
        "title": "Development of a tomato harvesting robot used in greenhouse",
        "year": "2017",
        "algorithm": "",
        "performance": "",
        "citation": "@article{lili2017development,\n  title={Development of a tomato harvesting robot used in greenhouse},\n  author={Lili, Wang and Bo, Zhao and Jinwei, Fan and Xiaoan, Hu and Shu, Wei and Yashuo, Li and Zhou, Qiangbing and Chongfeng, Wei},\n  journal={International Journal of Agricultural and Biological Engineering},\n  volume={10},\n  number={4},\n  pages={140--149},\n  year={2017}\n}\n",
        "fruit_veg": "tomato",
        "paper": {
          "relevant": "y",
          "": "\\cite{lili2017development}",
          "Article Title": "Development of a tomato harvesting robot used in greenhouse",
          "Times Cited, All Databases": "77",
          "Publication Year": "2017",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{lili2017development,\n  title={Development of a tomato harvesting robot used in greenhouse},\n  author={Lili, Wang and Bo, Zhao and Jinwei, Fan and Xiaoan, Hu and Shu, Wei and Yashuo, Li and Zhou, Qiangbing and Chongfeng, Wei},\n  journal={International Journal of Agricultural and Biological Engineering},\n  volume={10},\n  number={4},\n  pages={140--149},\n  year={2017}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "",
          "fruit/veg": "tomato",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Wang, LL; Zhao, B; Fan, JW; Hu, XA; Wei, S; Li, YS; Zhou, QB; Wei, CF",
          "Abstract": "A tomato harvesting robot was developed in this study, which consisted of a four-wheel independent steering system, a 5-DOF harvesting system, a navigation system, and a binocular stereo vision system. The four-wheel independent steering system was capable of providing a low-speed steering control of the robot based on Ackerman steering geometry. The proportional-integral-derivative (PID) algorithm was used in the laser navigation control system. The Otsu algorithm and the elliptic template method were used for the automatic recognition of ripe tomatoes, and obstacle avoidance strategies were proposed based on the C-space method. The maximum average absolute error between the set angle and the actual angle was about 0.14 degrees, and the maximum standard deviation was about 0.04 degrees. The laser navigation system was able to rapidly and accurately track the path, with the deviation being less than 8 cm. The load bearing capacity of the mechanical arm was about 1.5 kg. The success rate of the binocular vision system in the recognition of ripe tomatoes was 99.3%. When the distance was less than 600 mm, the positioning error was less than 10 mm. The time needed for recognition of ripe tomatoes and pitching was about 15 s per tomato, with a success rate of about 86%. This study provides some insights into the development and application of tomato harvesting robot used in the greenhouse.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Soft Grippers for Automatic Crop Harvesting: A Review",
        "year": "2021",
        "algorithm": "",
        "performance": "",
        "citation": "@article{navas2021soft,\n  title={Soft grippers for automatic crop harvesting: A review},\n  author={Navas, Eduardo and Fern{\\'a}ndez, Roemi and Sep{\\'u}lveda, Delia and Armada, Manuel and Gonzalez-de-Santos, Pablo},\n  journal={Sensors},\n  volume={21},\n  number={8},\n  pages={2689},\n  year={2021},\n  publisher={MDPI}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{navas2021soft}",
          "Article Title": "Soft Grippers for Automatic Crop Harvesting: A Review",
          "Times Cited, All Databases": "69",
          "Publication Year": "2021",
          "Highly Cited Status": "",
          "Publisher": "sensors",
          "Citation": "@article{navas2021soft,\n  title={Soft grippers for automatic crop harvesting: A review},\n  author={Navas, Eduardo and Fern{\\'a}ndez, Roemi and Sep{\\'u}lveda, Delia and Armada, Manuel and Gonzalez-de-Santos, Pablo},\n  journal={Sensors},\n  volume={21},\n  number={8},\n  pages={2689},\n  year={2021},\n  publisher={MDPI}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Navas, E; Fern¨¢ndez, R; Sep¨²lveda, D; Armada, M; Gonzalez-de-Santos, P",
          "Abstract": "Agriculture 4.0 is transforming farming livelihoods thanks to the development and adoption of technologies such as artificial intelligence, the Internet of Things and robotics, traditionally used in other productive sectors. Soft robotics and soft grippers in particular are promising approaches to lead to new solutions in this field due to the need to meet hygiene and manipulation requirements in unstructured environments and in operation with delicate products. This review aims to provide an in-depth look at soft end-effectors for agricultural applications, with a special emphasis on robotic harvesting. To that end, the current state of automatic picking tasks for several crops is analysed, identifying which of them lack automatic solutions, and which methods are commonly used based on the botanical characteristics of the fruits. The latest advances in the design and implementation of soft grippers are also presented and discussed, studying the properties of their materials, their manufacturing processes, the gripping technologies and the proposed control methods. Finally, the challenges that have to be overcome to boost its definitive implementation in the real world are highlighted. Therefore, this review intends to serve as a guide for those researchers working in the field of soft robotics for Agriculture 4.0, and more specifically, in the design of soft grippers for fruit harvesting robots.",
          "Keywords Plus": "OF-THE-ART; ROBOTIC GRIPPER; PERFORMANCE EVALUATION; MECHANICAL-PROPERTIES; FIELD-EVALUATION; LITCHI CLUSTERS; MACHINE VISION; DESIGN; FRUIT; RECOGNITION"
        }
      },
      {
        "title": "Fuzzy classification of pre-harvest tomatoes for ripeness estimation - An approach based on automatic rule learning using decision tree",
        "year": "2015",
        "algorithm": "fuzzy classification and desion tree",
        "performance": "",
        "citation": "@article{goel2015fuzzy,\n  title={Fuzzy classification of pre-harvest tomatoes for ripeness estimation--An approach based on automatic rule learning using decision tree},\n  author={Goel, Nidhi and Sehgal, Priti},\n  journal={Applied Soft Computing},\n  volume={36},\n  pages={45--56},\n  year={2015},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "tomato",
        "paper": {
          "relevant": "y",
          "": "\\cite{goel2015fuzzy}",
          "Article Title": "Fuzzy classification of pre-harvest tomatoes for ripeness estimation - An approach based on automatic rule learning using decision tree",
          "Times Cited, All Databases": "72",
          "Publication Year": "2015",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{goel2015fuzzy,\n  title={Fuzzy classification of pre-harvest tomatoes for ripeness estimation--An approach based on automatic rule learning using decision tree},\n  author={Goel, Nidhi and Sehgal, Priti},\n  journal={Applied Soft Computing},\n  volume={36},\n  pages={45--56},\n  year={2015},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "tomato ripeness estimation",
          "fruit/veg": "tomato",
          "Data Modality": "",
          "Learning Algorithm": "fuzzy classification and desion tree",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Goel, N; Sehgal, P",
          "Abstract": "Tomato (Solanum lycopersicum) ripeness estimation is an important process that affects its quality evaluation and marketing. However, the slow speed, subjectivity, time consumption associated with manual assessment has been forcing the agriculture industry to apply automation through robots. The vision system of harvesting robot is responsible for two-tasks. The first task is the recognition of object (tomato) and second is the classification of recognized objects (tomatoes). In this paper, Fuzzy Rule-Based Classification approach (FRBCS) has been proposed to estimate the ripeness of tomatoes based on color. The two color depictions: red-green color difference and red-green color ratio are derived from extracted RGB color information. These are then compared as a criterion for classification. Fuzzy partitioning of the feature space into linguistic variables is done by means of a learning algorithm. A rule set is automatically generated from the derived feature set using Decision Trees. Mamdani fuzzy inference system is adopted for building the fuzzy rule based classification system that classifies the tomatoes into six maturity stages. Dataset used for experiments has been created using the real images that were collected from a farm. 70% of the total images were used for training and 30% images of the total were used for testing the dataset respectively. Training dataset is divided into six classes representing the six different stages of tomato ripeness. Experimental results showed the system achieved the ripeness classification accuracy of 94.29% using proposed FRBCS. (C) 2015 Elsevier B.V. All rights reserved.",
          "Keywords Plus": "QUALITY EVALUATION; IMAGE; VISION; SYSTEM"
        }
      },
      {
        "title": "Intelligent robots for fruit harvesting: recent developments and future challenges",
        "year": "2022",
        "algorithm": "",
        "performance": "",
        "citation": "@article{zhou2022intelligent,\n  title={Intelligent robots for fruit harvesting: Recent developments and future challenges},\n  author={Zhou, Hongyu and Wang, Xing and Au, Wesley and Kang, Hanwen and Chen, Chao},\n  journal={Precision Agriculture},\n  volume={23},\n  number={5},\n  pages={1856--1907},\n  year={2022},\n  publisher={Springer}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{zhou2022intelligent}",
          "Article Title": "Intelligent robots for fruit harvesting: recent developments and future challenges",
          "Times Cited, All Databases": "72",
          "Publication Year": "2022",
          "Highly Cited Status": "Y",
          "Publisher": "Precision Agriculture",
          "Citation": "@article{zhou2022intelligent,\n  title={Intelligent robots for fruit harvesting: Recent developments and future challenges},\n  author={Zhou, Hongyu and Wang, Xing and Au, Wesley and Kang, Hanwen and Chen, Chao},\n  journal={Precision Agriculture},\n  volume={23},\n  number={5},\n  pages={1856--1907},\n  year={2022},\n  publisher={Springer}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Zhou, HY; Wang, X; Au, W; Kang, HW; Chen, C",
          "Abstract": "Intelligent robots for fruit harvesting have been actively developed over the past decades to bridge the increasing gap between feeding a rapidly growing population and limited labour resources. Despite significant advancements in this field, widespread use of harvesting robots in orchards is yet to be seen. To identify the challenges and formulate future research and development directions, this work reviews the state-of-the-art of intelligent fruit harvesting robots by comparing their system architectures, visual perception approaches, fruit detachment methods and system performances. The potential reasons behind the inadequate performance of existing harvesting robots are analysed and a novel map of challenges and potential research directions is created, considering both environmental factors and user requirements.",
          "Keywords Plus": "VISION-BASED CONTROL; FIELD-EVALUATION; PERFORMANCE EVALUATION; MECHANICAL DAMAGE; PICKING ROBOT; END-EFFECTOR; DESIGN; LOCALIZATION; SYSTEM; RECOGNITION"
        }
      },
      {
        "title": "Dual-arm cooperation and implementing for robotic harvesting tomato using binocular vision",
        "year": "2019",
        "algorithm": "",
        "performance": "",
        "citation": "@article{ling2019dual,\n  title={Dual-arm cooperation and implementing for robotic harvesting tomato using binocular vision},\n  author={Ling, Xiao and Zhao, Yuanshen and Gong, Liang and Liu, Chengliang and Wang, Tao},\n  journal={Robotics and Autonomous Systems},\n  volume={114},\n  pages={134--143},\n  year={2019},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "tomato",
        "paper": {
          "relevant": "y",
          "": "\\cite{ling2019dual}",
          "Article Title": "Dual-arm cooperation and implementing for robotic harvesting tomato using binocular vision",
          "Times Cited, All Databases": "75",
          "Publication Year": "2019",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{ling2019dual,\n  title={Dual-arm cooperation and implementing for robotic harvesting tomato using binocular vision},\n  author={Ling, Xiao and Zhao, Yuanshen and Gong, Liang and Liu, Chengliang and Wang, Tao},\n  journal={Robotics and Autonomous Systems},\n  volume={114},\n  pages={134--143},\n  year={2019},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "dual-arm",
          "fruit/veg": "tomato",
          "Data Modality": "bionocular",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Ling, X; Zhao, YS; Gong, L; Liu, CL; Wang, T",
          "Abstract": "Dual-arm cooperation is considered as an available approach to improve the poor efficiency by autonomous robotic harvesting. While, cooperating arm movements using visual information is a key challenge for harvesting robots working in non-structured environments. In this paper, we develop a dual-arm cooperative approach for a tomato harvesting robot using a binocular vision sensor. Firstly, a tomato detection algorithm combining AdaBoost classifier and color analysis is proposed and employed by the harvesting robot. Then, a fast three-dimensional scene reconstruction method is obtained in the simulation environment by using point clouds acquired from a stereo camera. Integration of tomato detection, target localization, motion planning and real-time control for dual-arm movements, the dual arm cooperation for robotic harvesting can be implemented. To validate the proposed approach, field experiments were conducted with the potted tomatoes in greenhouse. Over 96% of target tomatoes were correctly detected with the speed of about 10 fps. The positioning error of robot end-point of less than 10 mm was achieved for large scale direct positioning of the harvesting robot. With the vacuum cup grasping and wide-range cutting, the success rate of robotic harvesting achieved 87.5%. Meanwhile, the harvesting cycle time excluding cruise time was less than 30 s. These results indicate that the dual-arm cooperative approach is feasible and practical for robotic harvesting in non-structured environments. (C) 2019 Elsevier B.V. All rights reserved.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Robust Grape Cluster Detection in a Vineyard by Combining the AdaBoost Framework and Multiple Color Components",
        "year": "2016",
        "algorithm": "",
        "performance": "",
        "citation": "@article{luo2016robust,\n  title={Robust grape cluster detection in a vineyard by combining the AdaBoost framework and multiple color components},\n  author={Luo, Lufeng and Tang, Yunchao and Zou, Xiangjun and Wang, Chenglin and Zhang, Po and Feng, Wenxian},\n  journal={Sensors},\n  volume={16},\n  number={12},\n  pages={2098},\n  year={2016},\n  publisher={MDPI}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{luo2016robust}",
          "Article Title": "Robust Grape Cluster Detection in a Vineyard by Combining the AdaBoost Framework and Multiple Color Components",
          "Times Cited, All Databases": "69",
          "Publication Year": "2016",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{luo2016robust,\n  title={Robust grape cluster detection in a vineyard by combining the AdaBoost framework and multiple color components},\n  author={Luo, Lufeng and Tang, Yunchao and Zou, Xiangjun and Wang, Chenglin and Zhang, Po and Feng, Wenxian},\n  journal={Sensors},\n  volume={16},\n  number={12},\n  pages={2098},\n  year={2016},\n  publisher={MDPI}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Luo, LF; Tang, YC; Zou, XJ; Wang, CL; Zhang, P; Feng, WX",
          "Abstract": "The automatic fruit detection and precision picking in unstructured environments was always a difficult and frontline problem in the harvesting robots field. To realize the accurate identification of grape clusters in a vineyard, an approach for the automatic detection of ripe grape by combining the AdaBoost framework and multiple color components was developed by using a simple vision sensor. This approach mainly included three steps: (1) the dataset of classifier training samples was obtained by capturing the images from grape planting scenes using a color digital camera, extracting the effective color components for grape clusters, and then constructing the corresponding linear classification models using the threshold method; (2) based on these linear models and the dataset, a strong classifier was constructed by using the AdaBoost framework; and (3) all the pixels of the captured images were classified by the strong classifier, the noise was eliminated by the region threshold method and morphological filtering, and the grape clusters were finally marked using the enclosing rectangle method. Nine hundred testing samples were used to verify the constructed strong classifier, and the classification accuracy reached up to 96.56%, higher than other linear classification models. Moreover, 200 images captured under three different illuminations in the vineyard were selected as the testing images on which the proposed approach was applied, and the average detection rate was as high as 93.74%. The experimental results show that the approach can partly restrain the influence of the complex background such as the weather condition, leaves and changing illumination.",
          "Keywords Plus": "VISION SYSTEM; RGB IMAGES; LOCALIZATION; ENVIRONMENT; EXTRACTION; DESIGN; YIELD"
        }
      },
      {
        "title": "Robust Tomato Recognition for Robotic Harvesting Using Feature Images Fusion",
        "year": "2016",
        "algorithm": "images fusion",
        "performance": "",
        "citation": "@article{zhao2016robust,\n  title={Robust tomato recognition for robotic harvesting using feature images fusion},\n  author={Zhao, Yuanshen and Gong, Liang and Huang, Yixiang and Liu, Chengliang},\n  journal={Sensors},\n  volume={16},\n  number={2},\n  pages={173},\n  year={2016},\n  publisher={MDPI}\n}\n",
        "fruit_veg": "tomato",
        "paper": {
          "relevant": "y",
          "": "\\cite{zhao2016robust}",
          "Article Title": "Robust Tomato Recognition for Robotic Harvesting Using Feature Images Fusion",
          "Times Cited, All Databases": "75",
          "Publication Year": "2016",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{zhao2016robust,\n  title={Robust tomato recognition for robotic harvesting using feature images fusion},\n  author={Zhao, Yuanshen and Gong, Liang and Huang, Yixiang and Liu, Chengliang},\n  journal={Sensors},\n  volume={16},\n  number={2},\n  pages={173},\n  year={2016},\n  publisher={MDPI}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "recognition",
          "fruit/veg": "tomato",
          "Data Modality": "",
          "Learning Algorithm": "images fusion",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Zhao, YS; Gong, L; Huang, YX; Liu, CL",
          "Abstract": "Automatic recognition of mature fruits in a complex agricultural environment is still a challenge for an autonomous harvesting robot due to various disturbances existing in the background of the image. The bottleneck to robust fruit recognition is reducing influence from two main disturbances: illumination and overlapping. In order to recognize the tomato in the tree canopy using a low-cost camera, a robust tomato recognition algorithm based on multiple feature images and image fusion was studied in this paper. Firstly, two novel feature images, the a*-component image and the I-component image, were extracted from the L*a*b* color space and luminance, in-phase, quadrature-phase (YIQ) color space, respectively. Secondly, wavelet transformation was adopted to fuse the two feature images at the pixel level, which combined the feature information of the two source images. Thirdly, in order to segment the target tomato from the background, an adaptive threshold algorithm was used to get the optimal threshold. The final segmentation result was processed by morphology operation to reduce a small amount of noise. In the detection tests, 93% target tomatoes were recognized out of 200 overall samples. It indicates that the proposed tomato recognition method is available for robotic tomato harvesting in the uncontrolled environment with low cost.",
          "Keywords Plus": "CITRUS-FRUIT; DESIGN"
        }
      },
      {
        "title": "Robotics in Agriculture and Forestry",
        "year": "2016",
        "algorithm": "",
        "performance": "",
        "citation": "@article{billingsley2008robotics,\n  title={Robotics in Agriculture and Forestry.},\n  author={Billingsley, John and Visala, Arto and Dunn, Mark},\n  journal={Springer handbook of robotics},\n  volume={10},\n  pages={978--3},\n  year={2008}\n}",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "",
          "Article Title": "Robotics in Agriculture and Forestry",
          "Times Cited, All Databases": "71",
          "Publication Year": "2016",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{billingsley2008robotics,\n  title={Robotics in Agriculture and Forestry.},\n  author={Billingsley, John and Visala, Arto and Dunn, Mark},\n  journal={Springer handbook of robotics},\n  volume={10},\n  pages={978--3},\n  year={2008}\n}",
          "Document Type": "Article; Book Chapter",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Bergerman, M; Billingsley, J; Reid, J; van Henten, E",
          "Abstract": "Robotics for agriculture and forestry (A&F) represents the ultimate application of one of our society's latest and most advanced innovations to its most ancient and important industries. Over the course of history, mechanization and automation increased crop output several orders of magnitude, enabling a geometric growth in population and an increase in quality of life across the globe. Rapid population growth and rising incomes in developing countries, however, require ever larger amounts of A&F output. This chapter addresses robotics for A&F in the form of case studies where robotics is being successfully applied to solve well-identified problems. With respect to plant crops, the focus is on the in-field or in-farm tasks necessary to guarantee a quality crop and, generally speaking, end at harvest time. In the livestock domain, the focus is on breeding and nurturing, exploiting, harvesting, and slaughtering and processing. The chapter is organized in four main sections. The first one explains the scope, in particular, what aspects of robotics for A&F are dealt with in the chapter. The second one discusses the challenges and opportunities associated with the application of robotics to A&F. The third section is the core of the chapter, presenting twenty case studies that showcase (mostly) mature applications of robotics in various agricultural and forestry domains. The case studies are not meant to be comprehensive but instead to give the reader a general overview of how robotics has been applied to A&F in the last 10 years. The fourth section concludes the chapter with a discussion on specific improvements to current technology and paths to commercialization.",
          "Keywords Plus": "RUMEX-OBTUSIFOLIUS; AUTONOMOUS ROBOT; VISION; TREE; MECHANIZATION"
        }
      },
      {
        "title": "Collision-free path planning for a guava-harvesting robot based on recurrent deep reinforcement learning",
        "year": "2021",
        "algorithm": "",
        "performance": "",
        "citation": "@article{lin2021collision,\n  title={Collision-free path planning for a guava-harvesting robot based on recurrent deep reinforcement learning},\n  author={Lin, Guichao and Zhu, Lixue and Li, Jinhui and Zou, Xiangjun and Tang, Yunchao},\n  journal={Computers and Electronics in Agriculture},\n  volume={188},\n  pages={106350},\n  year={2021},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "guava",
        "paper": {
          "relevant": "y",
          "": "\\cite{lin2021collision}",
          "Article Title": "Collision-free path planning for a guava-harvesting robot based on recurrent deep reinforcement learning",
          "Times Cited, All Databases": "72",
          "Publication Year": "2021",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{lin2021collision,\n  title={Collision-free path planning for a guava-harvesting robot based on recurrent deep reinforcement learning},\n  author={Lin, Guichao and Zhu, Lixue and Li, Jinhui and Zou, Xiangjun and Tang, Yunchao},\n  journal={Computers and Electronics in Agriculture},\n  volume={188},\n  pages={106350},\n  year={2021},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "collision-free",
          "fruit/veg": "guava",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "path planning",
          "Performance": "",
          "challenges": "",
          "Authors": "Lin, GC; Zhu, LX; Li, JH; Zou, XJ; Tang, YC",
          "Abstract": "In unstructured orchard environments, picking a target fruit without colliding with neighboring branches is a significant challenge for guava-harvesting robots. This paper introduces a fast and robust collision-free pathplanning method based on deep reinforcement learning. A recurrent neural network is first adopted to remember and exploit the past states observed by the robot, then a deep deterministic policy gradient algorithm (DDPG) predicts a collision-free path from the states. A simulation environment is developed and its parameters are randomized during the training phase to enable recurrent DDPG to generalize to real-world scenarios. We also introduce an image processing method that uses a deep neural network to detect obstacles and uses many threedimensional line segments to approximate the obstacles. Simulations show that recurrent DDPG only needs 29 ms to plan a collision-free path with a success rate of 90.90%. Field tests show that recurrent DDPG can increase grasp, detachment, and harvest success rates by 19.43%, 9.11%, and 10.97%, respectively, compared to cases where no collision-free path-planning algorithm is implemented. Recurrent DDPG strikes a strong balance between efficiency and robustness and may be suitable for other fruits.",
          "Keywords Plus": "PICKING"
        }
      },
      {
        "title": "Real-Time Visual Localization of the Picking Points for a Ridge-Planting Strawberry Harvesting Robot",
        "year": "2020",
        "algorithm": "",
        "performance": "",
        "citation": "@article{yu2020real,\n  title={Real-time visual localization of the picking points for a ridge-planting strawberry harvesting robot},\n  author={Yu, Yang and Zhang, Kailiang and Liu, Hui and Yang, Li and Zhang, Dongxing},\n  journal={Ieee Access},\n  volume={8},\n  pages={116556--116568},\n  year={2020},\n  publisher={IEEE}\n}\n",
        "fruit_veg": "strawberry",
        "paper": {
          "relevant": "y",
          "": "\\cite{yu2020real}",
          "Article Title": "Real-Time Visual Localization of the Picking Points for a Ridge-Planting Strawberry Harvesting Robot",
          "Times Cited, All Databases": "72",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{yu2020real,\n  title={Real-time visual localization of the picking points for a ridge-planting strawberry harvesting robot},\n  author={Yu, Yang and Zhang, Kailiang and Liu, Hui and Yang, Li and Zhang, Dongxing},\n  journal={Ieee Access},\n  volume={8},\n  pages={116556--116568},\n  year={2020},\n  publisher={IEEE}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "vision: pick point",
          "fruit/veg": "strawberry",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Yu, Y; Zhang, KL; Liu, H; Yang, L; Zhang, DX",
          "Abstract": "At present, the primary technical deterrent to the use of strawberry harvesting robots is the low harvest rate, and there is a need to improve the accuracy and real-time performance of the localization algorithms to detect the picking point on the strawberry stem. The pose estimation of the fruit target (the direction of the fruit axis) can improve the accuracy of the localization algorithm. This study proposes a novel harvesting robot for the ridge-planted strawberries as well as a fruit pose estimator called rotated YOLO (R-YOLO), which significantly improves the localization precision of the picking points. First, the lightweight network Mobilenet-V1 was used to replace the convolution neural network as the backbone network for feature extraction. The simplified network structure substantially increased the operating speed. Second, the rotation angle parameter alpha was used to label the training set and set the anchors; the rotation of the bounding boxes of the target fruits was predicted using logistic regression with the rotated anchors. The test results of a set of 100 strawberry images showed that the proposed model's average recognition rate to be 94.43% and the recall rate to be 93.46%. Eighteen frames per second (FPS) were processed on the embedded controller of the robot, demonstrating good real-time performance. Compared with several other target detection methods used for the fruit harvesting robots, the proposed model exhibited better performance in terms of real-time detection and localization accuracy of the picking points. Field test results showed that the harvesting success rate reached 84.35% in modified situations. The results of this study provide technical support for improving the target detection of the embedded controller of harvesting robots.",
          "Keywords Plus": "CONVOLUTIONAL NETWORKS; FIELD-EVALUATION; APPLE DETECTION; VISION; FRUIT; RECOGNITION; SYSTEM"
        }
      },
      {
        "title": "An automated fruit harvesting robot by using deep learning",
        "year": "2019",
        "algorithm": "deep learning",
        "performance": "",
        "citation": "@article{onishi2019automated,\n  title={An automated fruit harvesting robot by using deep learning},\n  author={Onishi, Yuki and Yoshida, Takeshi and Kurita, Hiroki and Fukao, Takanori and Arihara, Hiromu and Iwai, Ayako},\n  journal={Robomech Journal},\n  volume={6},\n  number={1},\n  pages={1--8},\n  year={2019},\n  publisher={Springer}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{onishi2019automated}",
          "Article Title": "An automated fruit harvesting robot by using deep learning",
          "Times Cited, All Databases": "67",
          "Publication Year": "2019",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{onishi2019automated,\n  title={An automated fruit harvesting robot by using deep learning},\n  author={Onishi, Yuki and Yoshida, Takeshi and Kurita, Hiroki and Fukao, Takanori and Arihara, Hiromu and Iwai, Ayako},\n  journal={Robomech Journal},\n  volume={6},\n  number={1},\n  pages={1--8},\n  year={2019},\n  publisher={Springer}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "deep learning",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Onishi, Y; Yoshida, T; Kurita, H; Fukao, T; Arihara, H; Iwai, A",
          "Abstract": "Automation and labor saving in agriculture have been required recently. However, mechanization and robots for growing fruits have not been advanced. This study proposes a method of detecting fruits and automated harvesting using a robot arm. A highly fast and accurate method with a Single Shot MultiBox Detector is used herein to detect the position of fruit, and a stereo camera is used to detect the three-dimensional position. After calculating the angles of the joints at the detected position by inverse kinematics, the robot arm is moved to the target fruit's position. The robot then harvests the fruit by twisting the hand axis. The experimental results showed that more than 90% of the fruits were detected. Moreover, the robot could harvest a fruit in 16 s.",
          "Keywords Plus": "NUMBER; APPLES"
        }
      },
      {
        "title": "A Proposal for Automatic Fruit Harvesting by Combining a Low Cost Stereovision Camera and a Robotic Arm",
        "year": "2014",
        "algorithm": "",
        "performance": "",
        "citation": "@article{font2014proposal,\n  title={A proposal for automatic fruit harvesting by combining a low cost stereovision camera and a robotic arm},\n  author={Font, Davinia and Pallej{\\`a}, Tom{\\`a}s and Tresanchez, Marcel and Runcan, David and Moreno, Javier and Mart{\\'\\i}nez, Dani and Teixid{\\'o}, Merc{\\`e} and Palac{\\'\\i}n, Jordi},\n  journal={Sensors},\n  volume={14},\n  number={7},\n  pages={11557--11579},\n  year={2014},\n  publisher={MDPI}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{font2014proposal}",
          "Article Title": "A Proposal for Automatic Fruit Harvesting by Combining a Low Cost Stereovision Camera and a Robotic Arm",
          "Times Cited, All Databases": "70",
          "Publication Year": "2014",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{font2014proposal,\n  title={A proposal for automatic fruit harvesting by combining a low cost stereovision camera and a robotic arm},\n  author={Font, Davinia and Pallej{\\`a}, Tom{\\`a}s and Tresanchez, Marcel and Runcan, David and Moreno, Javier and Mart{\\'\\i}nez, Dani and Teixid{\\'o}, Merc{\\`e} and Palac{\\'\\i}n, Jordi},\n  journal={Sensors},\n  volume={14},\n  number={7},\n  pages={11557--11579},\n  year={2014},\n  publisher={MDPI}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "3D",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Font, D; Pallej¨¤, T; Tresanchez, M; Runcan, D; Moreno, J; Mart¨ªnez, D; Teixid¨®, M; Palac¨ªn, J",
          "Abstract": "This paper proposes the development of an automatic fruit harvesting system by combining a low cost stereovision camera and a robotic arm placed in the gripper tool. The stereovision camera is used to estimate the size, distance and position of the fruits whereas the robotic arm is used to mechanically pickup the fruits. The low cost stereovision system has been tested in laboratory conditions with a reference small object, an apple and a pear at 10 different intermediate distances from the camera. The average distance error was from 4% to 5%, and the average diameter error was up to 30% in the case of a small object and in a range from 2% to 6% in the case of a pear and an apple. The stereovision system has been attached to the gripper tool in order to obtain relative distance, orientation and size of the fruit. The harvesting stage requires the initial fruit location, the computation of the inverse kinematics of the robotic arm in order to place the gripper tool in front of the fruit, and a final pickup approach by iteratively adjusting the vertical and horizontal position of the gripper tool in a closed visual loop. The complete system has been tested in controlled laboratory conditions with uniform illumination applied to the fruits. As a future work, this system will be tested and improved in conventional outdoor farming conditions.",
          "Keywords Plus": "SYSTEM; MACHINE; SENSOR"
        }
      },
      {
        "title": "Robotic Aubergine Harvesting Using Dual-Arm Manipulation",
        "year": "2020",
        "algorithm": "",
        "performance": "",
        "citation": "@article{sepulveda2020robotic,\n  title={Robotic aubergine harvesting using dual-arm manipulation},\n  author={Sep{\\'u}Lveda, Delia and Fern{\\'a}ndez, Roemi and Navas, Eduardo and Armada, Manuel and Gonz{\\'a}lez-De-Santos, Pablo},\n  journal={IEEE Access},\n  volume={8},\n  pages={121889--121904},\n  year={2020},\n  publisher={IEEE}\n}\n",
        "fruit_veg": "aubergine",
        "paper": {
          "relevant": "y",
          "": "\\cite{sepulveda2020robotic}",
          "Article Title": "Robotic Aubergine Harvesting Using Dual-Arm Manipulation",
          "Times Cited, All Databases": "67",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{sepulveda2020robotic,\n  title={Robotic aubergine harvesting using dual-arm manipulation},\n  author={Sep{\\'u}Lveda, Delia and Fern{\\'a}ndez, Roemi and Navas, Eduardo and Armada, Manuel and Gonz{\\'a}lez-De-Santos, Pablo},\n  journal={IEEE Access},\n  volume={8},\n  pages={121889--121904},\n  year={2020},\n  publisher={IEEE}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "dual-arm",
          "fruit/veg": "aubergine",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Sep¨²lveda, D; Fern¨¢ndez, R; Navas, E; Armada, M; Gonz¨¢lez-De-Santos, P",
          "Abstract": "Interest in agricultural automation has increased considerably in recent decades due to benefits such as improving productivity or reducing the labor force. However, there are some current problems associated with unstructured environments make developing a robotic harvester a challenge. This article presents a dual-arm aubergine harvesting robot consisting of two robotic arms configured in an anthropomorphic manner to optimize the dual workspace. To detect and locate the aubergines automatically, we implemented an algorithm based on a support vector machine (SVM) classifier and designed a planning algorithm for scheduling efficient fruit harvesting that coordinates the two arms throughout the harvesting process. Finally, we propose a novel algorithm for dealing with occlusions using the capabilities of the dual-arm robot for coordinate work. Therefore, the main contribution of this study is the implementation and validation of a dual-arm harvesting robot with planning and control algorithms, which, depending on the locations of the fruits and the configuration of the arms, enables the following: (i) the simultaneous harvesting of two aubergines; (ii) the harvesting of a single aubergine with a single arm; or (iii) a collaborative behavior between the arms to solve occlusions. This cooperative operation mimics complex human harvesting motions such as using one arm to push leaves aside while the other arm picks the fruit. The performance of the proposed harvester is evaluated through laboratory tests that simulate the most common real-world scenarios. The results show that the robotic harvester has a success rate of 91.67% and an average cycle time of 26 s/fruit.",
          "Keywords Plus": "SEGMENTATION; LOCALIZATION; VISION; DESIGN; SYSTEM; IMAGE"
        }
      },
      {
        "title": "Recognition of Bloom/Yield in Crop Images Using Deep Learning Models for Smart Agriculture: A Review",
        "year": "2021",
        "algorithm": "",
        "performance": "",
        "citation": "@article{darwin2021recognition,\n  title={Recognition of bloom/yield in crop images using deep learning models for smart agriculture: A review},\n  author={Darwin, Bini and Dharmaraj, Pamela and Prince, Shajin and Popescu, Daniela Elena and Hemanth, Duraisamy Jude},\n  journal={Agronomy},\n  volume={11},\n  number={4},\n  pages={646},\n  year={2021},\n  publisher={MDPI}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{darwin2021recognition}",
          "Article Title": "Recognition of Bloom/Yield in Crop Images Using Deep Learning Models for Smart Agriculture: A Review",
          "Times Cited, All Databases": "65",
          "Publication Year": "2021",
          "Highly Cited Status": "",
          "Publisher": "Agronomy",
          "Citation": "@article{darwin2021recognition,\n  title={Recognition of bloom/yield in crop images using deep learning models for smart agriculture: A review},\n  author={Darwin, Bini and Dharmaraj, Pamela and Prince, Shajin and Popescu, Daniela Elena and Hemanth, Duraisamy Jude},\n  journal={Agronomy},\n  volume={11},\n  number={4},\n  pages={646},\n  year={2021},\n  publisher={MDPI}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Darwin, B; Dharmaraj, P; Prince, S; Popescu, DE; Hemanth, DJ",
          "Abstract": "Precision agriculture is a crucial way to achieve greater yields by utilizing the natural deposits in a diverse environment. The yield of a crop may vary from year to year depending on the variations in climate, soil parameters and fertilizers used. Automation in the agricultural industry moderates the usage of resources and can increase the quality of food in the post-pandemic world. Agricultural robots have been developed for crop seeding, monitoring, weed control, pest management and harvesting. Physical counting of fruitlets, flowers or fruits at various phases of growth is labour intensive as well as an expensive procedure for crop yield estimation. Remote sensing technologies offer accuracy and reliability in crop yield prediction and estimation. The automation in image analysis with computer vision and deep learning models provides precise field and yield maps. In this review, it has been observed that the application of deep learning techniques has provided a better accuracy for smart farming. The crops taken for the study are fruits such as grapes, apples, citrus, tomatoes and vegetables such as sugarcane, corn, soybean, cucumber, maize, wheat. The research works which are carried out in this research paper are available as products for applications such as robot harvesting, weed detection and pest infestation. The methods which made use of conventional deep learning techniques have provided an average accuracy of 92.51%. This paper elucidates the diverse automation approaches for crop yield detection techniques with virtual analysis and classifier approaches. Technical hitches in the deep learning techniques have progressed with limitations and future investigations are also surveyed. This work highlights the machine vision and deep learning models which need to be explored for improving automated precision farming expressly during this pandemic.",
          "Keywords Plus": "YIELD ESTIMATION; FRUIT DETECTION; NEURAL-NETWORKS; DISEASE DETECTION; SEGMENTATION; COLOR; CLASSIFICATION; TOMATOES; FEATURES; SYSTEM"
        }
      },
      {
        "title": "Identification of fruit and branch in natural scenes for citrus harvesting robot using machine vision and support vector machine",
        "year": "2014",
        "algorithm": "",
        "performance": "",
        "citation": "@article{qiang2014identification,\n  title={Identification of fruit and branch in natural scenes for citrus harvesting robot using machine vision and support vector machine},\n  author={Qiang, L{\\\"u} and Jianrong, Cai and Bin, Liu and Lie, Deng and Yajing, Zhang},\n  journal={International Journal of Agricultural and Biological Engineering},\n  volume={7},\n  number={2},\n  pages={115--121},\n  year={2014}\n}",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{qiang2014identification}",
          "Article Title": "Identification of fruit and branch in natural scenes for citrus harvesting robot using machine vision and support vector machine",
          "Times Cited, All Databases": "71",
          "Publication Year": "2014",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{qiang2014identification,\n  title={Identification of fruit and branch in natural scenes for citrus harvesting robot using machine vision and support vector machine},\n  author={Qiang, L{\\\"u} and Jianrong, Cai and Bin, Liu and Lie, Deng and Yajing, Zhang},\n  journal={International Journal of Agricultural and Biological Engineering},\n  volume={7},\n  number={2},\n  pages={115--121},\n  year={2014}\n}",
          "Document Type": "Article",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Qiang, L; Cai, JR; Bin, L; Lie, D; Zhang, YJ",
          "Abstract": "With the decrease of agricultural labor and the increase of production cost, the researches on citrus harvesting robot (CHR) have received more and more attention in recent years. For the success of robotic harvesting and the safety of robot, the identification of mature citrus fruit and obstacle is the priority of robotic harvesting. In this work, a machine vision system, which consisted of a color CCD camera and a computer, was developed to achieve these tasks. Images of citrus trees were captured under sunny and cloudy conditions. Due to varying degrees of lightness and position randomness of fruits and branches, red, green, and blue values of objects in these images are changed dramatically. The traditional threshold segmentation is not efficient to solve these problems. Multi-class support vector machine (SVM), which succeeds by morphological operation, was used to simultaneously segment the fruits and branches in this study. The recognition rate of citrus fruit was 92.4%, and the branch of which diameter was more than 5 pixels, could be recognized. The results showed that the algorithm could be used to detect the fruits and branches for CHR.",
          "Keywords Plus": "COMPUTER VISION; RECOGNITION; SEGMENTATION"
        }
      },
      {
        "title": "Stem localization of sweet-pepper plants using the support wire as a visual cue",
        "year": "2014",
        "algorithm": "",
        "performance": "",
        "citation": "@article{bac2014stem,\n  title={Stem localization of sweet-pepper plants using the support wire as a visual cue},\n  author={Bac, C Wouter and Hemming, Jochen and Van Henten, Eldert J},\n  journal={Computers and electronics in agriculture},\n  volume={105},\n  pages={111--120},\n  year={2014},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "sweet pepper",
        "paper": {
          "relevant": "y",
          "": "\\cite{bac2014stem}",
          "Article Title": "Stem localization of sweet-pepper plants using the support wire as a visual cue",
          "Times Cited, All Databases": "76",
          "Publication Year": "2014",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{bac2014stem,\n  title={Stem localization of sweet-pepper plants using the support wire as a visual cue},\n  author={Bac, C Wouter and Hemming, Jochen and Van Henten, Eldert J},\n  journal={Computers and electronics in agriculture},\n  volume={105},\n  pages={111--120},\n  year={2014},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "vision: pick point",
          "fruit/veg": "sweet pepper",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Bac, CW; Hemming, J; van Henten, EJ",
          "Abstract": "A robot arm should avoid collisions with the plant stem when it approaches a candidate sweet-pepper for harvesting. This study therefore aims at stem localization, a topic so far only studied under controlled lighting conditions. Objectives were to develop an algorithm capable of stem localization, using detection of the support wire that is twisted around the stem; to quantitatively evaluate performance of wire detection and stem localization under varying lighting conditions; to determine depth accuracy of stereo-vision under lab and greenhouse conditions. A single colour camera was mounted on a pneumatic slide to record image pairs with a small baseline of 1 cm. Artificial lighting was developed to mitigate disturbances caused by natural lighting conditions. An algorithm consisting of five steps was developed and includes novel components such as adaptive thresholding, use of support wires as a visual cue, use of object-based and 3D features and use of minimum expected stem distance. Wire detection rates (true-positive/scaled false-positive) were more favourable under moderate irradiance (94/5%) than under strong irradiance (74/26%). Error of stem localization was measured, in the horizontal plane, by Euclidean distance. Error was smaller for interpolated segments (0.8 cm), where a support wire was detected, than for extrapolated segments (1.5 cm), where a support wire was not detected. Error increased under strong irradiance. Accuracy of the stereo-vision system (+/- 0.4 cm) met the requirements (+/- 1 cm) in the lab, but not in the greenhouse (+/- 4.5 cm) due to plant movement during recording. The algorithm is probably capable to construct a useful collision map for robotic harvesting, if the issue of inaccurate stereo-vision can be resolved by directions proposed for future work. This is the first study regarding stem localization under varying lighting conditions, and can be useful for future applications in crops that grow along a support wire. (C) 2014 Elsevier B.V. All rights reserved.",
          "Keywords Plus": "AUTONOMOUS ROBOT; STEREO; VISION; IMAGES; SYSTEM; RGB"
        }
      },
      {
        "title": "In-field citrus detection and localisation based on RGB-D image analysis",
        "year": "2019",
        "algorithm": "",
        "performance": "",
        "citation": "@article{lin2019field,\n  title={In-field citrus detection and localisation based on RGB-D image analysis},\n  author={Lin, Guichao and Tang, Yunchao and Zou, Xiangjun and Li, Jinhui and Xiong, Juntao},\n  journal={Biosystems Engineering},\n  volume={186},\n  pages={34--44},\n  year={2019},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "citrus",
        "paper": {
          "relevant": "y",
          "": "\\cite{lin2019field}",
          "Article Title": "In-field citrus detection and localisation based on RGB-D image analysis",
          "Times Cited, All Databases": "67",
          "Publication Year": "2019",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{lin2019field,\n  title={In-field citrus detection and localisation based on RGB-D image analysis},\n  author={Lin, Guichao and Tang, Yunchao and Zou, Xiangjun and Li, Jinhui and Xiong, Juntao},\n  journal={Biosystems Engineering},\n  volume={186},\n  pages={34--44},\n  year={2019},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "detection and localization",
          "fruit/veg": "citrus",
          "Data Modality": "RGB-D",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Lin, GC; Tang, YC; Zou, XJ; Li, JH; Xiong, JT",
          "Abstract": "In-field citrus detection and localisation are highly challenging tasks due to varying illumination conditions, partial occlusion of citrus, and the colour variation of citrus at different stages of maturity. A reliable algorithm based on red-green-blue-depth (RGB-D) images was developed to detect and locate citrus in real, outdoor orchard environments for robotic harvesting. A depth filter and a Bayes-classifier-based image segmentation method were first developed to exclude as many backgrounds as possible. A density clustering method was then used to group adjacent points in the filtered RGB-D images into clusters, where each cluster represents a possible citrus. A colour, gradient, and geometry feature-based support vector machine classifier was trained to remove false positives. To test the method, a dataset with 506 RGB-D images was acquired in a citrus orchard on sunny and cloudy days. Results showed that the proposed algorithm was robust with an F1 score of 0.9197; the positioning errors in the x, y and z directions were 7.0 +/- 2.5 mm, -4.0 +/- 3.0 mm and 13.0 +/- 3.0 mm, respectively, and the sizing error was -1.0 +/- 4.0 mm. These excellent performance values demonstrate that the proposed method could be used to guide a citrus-harvesting robot. (C) 2019 IAgrE. Published by Elsevier Ltd. All rights reserved.",
          "Keywords Plus": "FRUIT DETECTION; COLOR; RECOGNITION"
        }
      },
      {
        "title": "Apple harvesting robot under information technology: A review",
        "year": "2020",
        "algorithm": "",
        "performance": "",
        "citation": "@article{jia2020apple,\n  title={Apple harvesting robot under information technology: A review},\n  author={Jia, Weikuan and Zhang, Yan and Lian, Jian and Zheng, Yuanjie and Zhao, Dean and Li, Chengjiang},\n  journal={International Journal of Advanced Robotic Systems},\n  volume={17},\n  number={3},\n  pages={1729881420925310},\n  year={2020},\n  publisher={SAGE Publications Sage UK: London, England}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{jia2020apple}",
          "Article Title": "Apple harvesting robot under information technology: A review",
          "Times Cited, All Databases": "64",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "International Journal of Advanced Robotic Systems",
          "Citation": "@article{jia2020apple,\n  title={Apple harvesting robot under information technology: A review},\n  author={Jia, Weikuan and Zhang, Yan and Lian, Jian and Zheng, Yuanjie and Zhao, Dean and Li, Chengjiang},\n  journal={International Journal of Advanced Robotic Systems},\n  volume={17},\n  number={3},\n  pages={1729881420925310},\n  year={2020},\n  publisher={SAGE Publications Sage UK: London, England}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Jia, WK; Zhang, Y; Lian, J; Zheng, YJ; Zhao, D; Li, CJ",
          "Abstract": "It has been more than 30 years since the French pioneered the research of the apple harvesting robot, and with the joint efforts of scholars all around the world, a variety of prototypes have been developed. However, the existing apple harvesting robot prototype is still in the experimental research stage because of its low harvesting efficiency. With the help of information technology, the related research has ushered in a milestone development, and it is full of opportunities and challenges for apple harvesting robotic researchers. In this article, it briefly introduced the development history, structure, and composition of apple harvesting robots and the operation process, which makes readers have a clear understanding of apple harvesting robot and its harvesting principle. Then systematically summarizing the research results of apple harvesting robots both at domestic and at foreign, we carried out in following three aspects: rapid and accurate recognition and positioning of target fruit, all-weather operation mode, and application of intelligent computing theory in apple harvesting robots, and it analyzes the research progress of apple harvesting robot in detail. The results show that improving the harvesting efficiency is the key and hot spot for the research on apple harvesting robots. Under the impetus of information technology, how to achieve fast and accurate recognition of the fruits of multienvironment and multiple information, obtain reasonable path planning, and further optimization of control strategies are all important research directions.",
          "Keywords Plus": "VISION-BASED CONTROL; FRUIT DETECTION; FEATURE-EXTRACTION; NEURAL-NETWORK; IMAGE-ANALYSIS; LOCALIZATION; RECOGNITION; NIGHT; CLASSIFICATION; SEGMENTATION"
        }
      },
      {
        "title": "Kiwifruit recognition at nighttime using artificial lighting based on machine vision",
        "year": "2015",
        "algorithm": "",
        "performance": "",
        "citation": "@article{longsheng2015kiwifruit,\n  title={Kiwifruit recognition at nighttime using artificial lighting based on machine vision},\n  author={Longsheng, Fu and Bin, Wang and Yongjie, Cui and Shuai, Su and Gejima, Yoshinori and Kobayashi, Taiichi},\n  journal={International Journal of Agricultural and Biological Engineering},\n  volume={8},\n  number={4},\n  pages={52--59},\n  year={2015}\n}\n",
        "fruit_veg": "kiwifruit",
        "paper": {
          "relevant": "y",
          "": "\\cite{longsheng2015kiwifruit}",
          "Article Title": "Kiwifruit recognition at nighttime using artificial lighting based on machine vision",
          "Times Cited, All Databases": "72",
          "Publication Year": "2015",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{longsheng2015kiwifruit,\n  title={Kiwifruit recognition at nighttime using artificial lighting based on machine vision},\n  author={Longsheng, Fu and Bin, Wang and Yongjie, Cui and Shuai, Su and Gejima, Yoshinori and Kobayashi, Taiichi},\n  journal={International Journal of Agricultural and Biological Engineering},\n  volume={8},\n  number={4},\n  pages={52--59},\n  year={2015}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "vison:artifical light at night",
          "fruit/veg": "kiwifruit",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Fu, LS; Wang, B; Cui, YJ; Su, S; Gejima, Y; Kobayashi, T",
          "Abstract": "Most researches involved so far in kiwifruit harvesting robot suggest the scenario of harvesting in daytime for taking advantage of sunlight. A robot operating at nighttime can overcome the problem of low work efficiency and would help to minimize fruit damage. In addition, artificial lights can be used to ensure constant illumination instead of the variable natural sunlight for image capturing. This paper aims to study the kiwifruit recognition at nighttime using artificial lighting based on machine vision. Firstly, an RGB camera was placed underneath the canopy so that clusters of kiwifruits could be included in the images. Next, the images were segmented using an R-G color model. Finally, a group of image processing conventional methods, such as Canny operator were applied to detect the fruits. The image processing results showed that this capturing method could reduce the background noise and overcome any target overlapping. The experimental results showed that the optimal artificial lighting ranged approximately between 30-50 lx. The developed algorithm detected 88.3% of the fruits successfully.",
          "Keywords Plus": "SYSTEM; DESIGN; ROBOT"
        }
      },
      {
        "title": "Vine trunk detector for a reliable robot localization system",
        "year": "2016",
        "algorithm": "",
        "performance": "",
        "citation": "@inproceedings{mendes2016vine,\n  title={Vine trunk detector for a reliable robot localization system},\n  author={Mendes, Jorge and Dos Santos, Filipe Neves and Ferraz, Nuno and Couto, Pedro and Morais, Raul},\n  booktitle={2016 International Conference on Autonomous Robot Systems and Competitions (ICARSC)},\n  pages={1--6},\n  year={2016},\n  organization={IEEE}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{mendes2016vine}",
          "Article Title": "Vine trunk detector for a reliable robot localization system",
          "Times Cited, All Databases": "64",
          "Publication Year": "2016",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@inproceedings{mendes2016vine,\n  title={Vine trunk detector for a reliable robot localization system},\n  author={Mendes, Jorge and Dos Santos, Filipe Neves and Ferraz, Nuno and Couto, Pedro and Morais, Raul},\n  booktitle={2016 International Conference on Autonomous Robot Systems and Competitions (ICARSC)},\n  pages={1--6},\n  year={2016},\n  organization={IEEE}\n}\n",
          "Document Type": "Proceedings Paper",
          "Main Contribution": "vision: pick point",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Mendes, J; dos Santos, FN; Ferraz, N; Couto, P; Morais, R",
          "Abstract": "Develop ground robots for crop monitoring and harvesting in steep slope vineyards is a complex challenge due to two main reasons: harsh condition of the terrain and unstable localization accuracy got from Global Positioning Systems (GPS). For this context, a reliable localization system requires a high density of natural/artificial features and an accurate detector. This paper presents a novel visual detector for Vineyards Trunks and Masts (ViTruDe). The ViTruDe detector was developed considering the constrains of a cost-effective robot to carry-out crop monitoring tasks in steep slope vineyard environment. The obtained results with real data shows an accuracy higher than 95% for all tested configurations. The training and test data are made public for future research work. This approach is a contribution for an accurate and reliable localization system that is GPS-free.",
          "Keywords Plus": "SLAM"
        }
      },
      {
        "title": "Technology progress in mechanical harvest of fresh market apples",
        "year": "2020",
        "algorithm": "",
        "performance": "",
        "citation": "@article{zhang2020technology,\n  title={Technology progress in mechanical harvest of fresh market apples},\n  author={Zhang, Zhao and Igathinathane, C and Li, J and Cen, Haiyan and Lu, Y and Flores, Paulo},\n  journal={Computers and Electronics in Agriculture},\n  volume={175},\n  pages={105606},\n  year={2020},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{zhang2020technology}",
          "Article Title": "Technology progress in mechanical harvest of fresh market apples",
          "Times Cited, All Databases": "64",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "Computers and Electronics in Agriculture",
          "Citation": "@article{zhang2020technology,\n  title={Technology progress in mechanical harvest of fresh market apples},\n  author={Zhang, Zhao and Igathinathane, C and Li, J and Cen, Haiyan and Lu, Y and Flores, Paulo},\n  journal={Computers and Electronics in Agriculture},\n  volume={175},\n  pages={105606},\n  year={2020},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Zhang, Z; Igathinathane, C; Li, J; Cen, H; Lu, Y; Flores, P",
          "Abstract": "This article reviews the research and development progress of mechanical harvest technologies for fresh market apples over the past decades with a focus on the predominant technologies of shake-and-catch, robots, and harvest-assist platform methods. In addition, based on the review it points out the bottlenecks and future trends of these three technology categories. Major progress in the shake-and-catch method is related to theoretical studies on the effective removal of apples and catching mechanisms to minimize bruising. The unacceptable bruising conditions hinder the shake-and-catch method from commercial application. Two startups of apple harvesting robots are in the stage of commercializing their products based on vacuum and three-finger end-effectors, respectively. Economic benefits, as well as technology reliability and robustness of both robots, are pending for validation before they are on the market. In addition, a key obstacle faced by both robots before commercial use is to find a solution to pick apples grown in clusters. Harvest-assist platforms are gradually adopted by apple growers, but at a very low rate due to their doubts on economic benefits. Validation of harvest-assist platforms' economic benefits and incorporation with more functions (e.g., sorting) would enhance their adoption. With the rapid development of sensing and automation technologies, such as novel sensors, embedded systems, and machine learning algorithms, and the progress in new tree canopy structures that are friendlier for fruit visibility and accessibility, it is believed the robots for fresh market apple harvest would be realized and commercialized in the near future. Currently, more efforts should be invested in analyzing and validating the economic benefits of harvest-assist platforms, as well as adding more functions to the harvest-assist platforms, to increase their application rate for the benefit of the apple industry.",
          "Keywords Plus": "ECONOMIC-EVALUATION; CITRUS DETECTION; FRUIT DETECTION; GREEN APPLES; MACHINE; DESIGN; VISION; COLOR; RGB; FEATURES"
        }
      },
      {
        "title": "Automatic cucumber recognition algorithm for harvesting robots in the natural environment using deep learning and multi-feature fusion",
        "year": "2020",
        "algorithm": "deep learning",
        "performance": "",
        "citation": "@article{mao2020automatic,\n  title={Automatic cucumber recognition algorithm for harvesting robots in the natural environment using deep learning and multi-feature fusion},\n  author={Mao, Shihan and Li, Yuhua and Ma, You and Zhang, Baohua and Zhou, Jun and Wang, Kai},\n  journal={Computers and Electronics in Agriculture},\n  volume={170},\n  pages={105254},\n  year={2020},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "cucumber",
        "paper": {
          "relevant": "y",
          "": "\\cite{mao2020automatic}",
          "Article Title": "Automatic cucumber recognition algorithm for harvesting robots in the natural environment using deep learning and multi-feature fusion",
          "Times Cited, All Databases": "59",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{mao2020automatic,\n  title={Automatic cucumber recognition algorithm for harvesting robots in the natural environment using deep learning and multi-feature fusion},\n  author={Mao, Shihan and Li, Yuhua and Ma, You and Zhang, Baohua and Zhou, Jun and Wang, Kai},\n  journal={Computers and Electronics in Agriculture},\n  volume={170},\n  pages={105254},\n  year={2020},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "recognition",
          "fruit/veg": "cucumber",
          "Data Modality": "",
          "Learning Algorithm": "deep learning",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Mao, SH; Li, YH; Ma, Y; Zhang, BH; Zhou, J; Wang, K",
          "Abstract": "Mechanical harvesting requires agricultural robot to detect fruits automatically. However, effective and accurate detection of cucumber by computer vision system is still a challenge due to similarity between cucumber color and that of branches and leaves, shape irregularity and complex growing environment. To improve the practicability and accuracy of the automatic recognition models, this paper proposed a novel cucumber region detection method using multi-path convolutional neural network (MPCNN), combined with color component selection and support vector machine (SVM). In this method, the cucumber image was transformed into color space to obtain 15 color components and the weight information of relevant features was analyzed by I-RELIEF. In parallel, to remove part of the background area, the OSTU algorithm was applied to segment the G component and Maximally Stable Extremal Regions (MSER) was used to obtain the mask image. In order to maximize the differences between cucumber and leaf, promoting the classification accuracy of SVM, the top three components of the weight were input into the deep learning module to extract and fuse features. In final, cucumber recognition was realized by combining SVM classification with mask image. The recognition results show that more than 90% pixels of cucumber images are correctly classified, and the misidentified pixels are less than 22%. The ratio between the two indicators is over 4, demonstrating the satisfactory performance of the proposed method and highlighting its promising applications in mechanical cucumber harvesting.",
          "Keywords Plus": "FRUIT-PICKING; APPLES; STEM"
        }
      },
      {
        "title": "Fruit Localization and Environment Perception for Strawberry Harvesting Robots",
        "year": "2019",
        "algorithm": "",
        "performance": "",
        "citation": "@article{ge2019fruit,\n  title={Fruit localization and environment perception for strawberry harvesting robots},\n  author={Ge, Yuanyue and Xiong, Ya and Tenorio, Gabriel Lins and From, P{\\aa}l Johan},\n  journal={IEEE Access},\n  volume={7},\n  pages={147642--147652},\n  year={2019},\n  publisher={IEEE}\n}\n",
        "fruit_veg": "strawberry",
        "paper": {
          "relevant": "y",
          "": "\\cite{ge2019fruit}",
          "Article Title": "Fruit Localization and Environment Perception for Strawberry Harvesting Robots",
          "Times Cited, All Databases": "58",
          "Publication Year": "2019",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{ge2019fruit,\n  title={Fruit localization and environment perception for strawberry harvesting robots},\n  author={Ge, Yuanyue and Xiong, Ya and Tenorio, Gabriel Lins and From, P{\\aa}l Johan},\n  journal={IEEE Access},\n  volume={7},\n  pages={147642--147652},\n  year={2019},\n  publisher={IEEE}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "detection and localization",
          "fruit/veg": "strawberry",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Ge, YY; Xiong, Y; Tenorio, GL; From, PJ",
          "Abstract": "This work presents a machine vision system for the localization of strawberries and environment perception in a strawberry-harvesting robot for use in table-top strawberry production. A deep convolutional neural network for segmentation is utilized to detect the strawberries. Segmented strawberries are localized through coordinate transformation, density base point clustering and the proposed location approximation method. To avoid collisions between the gripper and fixed obstacles, the safe manipulation region is limited to the space in front of the table and underneath the strap. Therefore, a safe region classification algorithm, based on Hough Transform algorithm, is proposed to segment the strap masks into a belt region in order to identify the pickable strawberries located underneath the strap. Similarly, a safe region classification algorithm is proposed for the table, to calculate its points in 3D and fit the points onto a 3D plane based on the 3D point cloud, so that pickable strawberries in front of the table can be identified. Experimental tests showed that the algorithm could accurately classify ripe and unripe strawberries and could identify whether the strawberries are within the safe region for harvesting. Furthermore, harvester robot's optimized localization method could accurately locate the strawberry targets with a picking accuracy rate of 74.1% in modified situations.",
          "Keywords Plus": "AWARENESS; CAMERA"
        }
      },
      {
        "title": "A pattern recognition strategy for visual grape bunch detection in vineyards",
        "year": "2018",
        "algorithm": "",
        "performance": "",
        "citation": "@article{perez2018pattern,\n  title={A pattern recognition strategy for visual grape bunch detection in vineyards},\n  author={P{\\'e}rez-Zavala, Rodrigo and Torres-Torriti, Miguel and Cheein, Fernando Auat and Troni, Giancarlo},\n  journal={Computers and Electronics in Agriculture},\n  volume={151},\n  pages={136--149},\n  year={2018},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "grape",
        "paper": {
          "relevant": "y",
          "": "\\cite{perez2018pattern}",
          "Article Title": "A pattern recognition strategy for visual grape bunch detection in vineyards",
          "Times Cited, All Databases": "64",
          "Publication Year": "2018",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{perez2018pattern,\n  title={A pattern recognition strategy for visual grape bunch detection in vineyards},\n  author={P{\\'e}rez-Zavala, Rodrigo and Torres-Torriti, Miguel and Cheein, Fernando Auat and Troni, Giancarlo},\n  journal={Computers and Electronics in Agriculture},\n  volume={151},\n  pages={136--149},\n  year={2018},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "detection",
          "fruit/veg": "grape",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "P¨¦rez-Zavala, R; Torres-Torriti, M; Cheein, FA; Troni, G",
          "Abstract": "Automating grapevine growth monitoring, spraying, leaf thinning and harvesting tasks, as well as improving yield estimation and plant phenotyping, requires reliable methods for detecting grape bunches across different vineyard environmental and plant variety conditions, in which illumination, occlusions, colors and contrast are the main challenges to computer vision techniques. This work presents a method that employs visible spectrum cameras for robust grape berries recognition and grape bunch detection that does not require artificial illumination nor is limited to red or purple grape varieties. The proposed approach relies on shape and texture information together with a strategy to separate regions of clustered pixels into grape bunches. The approach employs histograms of oriented gradients (HOG) as shape descriptor and local binary patterns (LBP) to obtain texture information. A review of the existing methods and comparative analysis of different feature vectors (DAISY, DSIFT, HOG, LBP) and support vector classifiers (SVM-RBF, SVDD) is also presented. Datasets from four countries containing 163 images of different grapevine varieties acquired under different vineyard illumination and occlusion levels were employed to assess the approach. Grapes bunches are detected with an average precision of 88.61% and average recall of 80.34%. Single berries are detected with precision rates above 99% and recall rates between 84.0% and 92.5% on average. The proposed approach should facilitate the estimation of yield, crop thinning measurements and the computation of leaf removal indicators, as well as the implementation guidance strategies for precise robotic harvesters.",
          "Keywords Plus": "CLUSTER YIELD COMPONENTS; IMAGE-ANALYSIS; POINTS; SCALE"
        }
      },
      {
        "title": "Development of a Robot for Harvesting Strawberries",
        "year": "2018",
        "algorithm": "",
        "performance": "",
        "citation": "@article{de2018development,\n  title={Development of a robot for harvesting strawberries},\n  author={De Preter, Andreas and Anthonis, Jan and De Baerdemaeker, Josse},\n  journal={IFAC-PapersOnLine},\n  volume={51},\n  number={17},\n  pages={14--19},\n  year={2018},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{de2018development}",
          "Article Title": "Development of a Robot for Harvesting Strawberries",
          "Times Cited, All Databases": "57",
          "Publication Year": "2018",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{de2018development,\n  title={Development of a robot for harvesting strawberries},\n  author={De Preter, Andreas and Anthonis, Jan and De Baerdemaeker, Josse},\n  journal={IFAC-PapersOnLine},\n  volume={51},\n  number={17},\n  pages={14--19},\n  year={2018},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Proceedings Paper",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "De Preter, A; Anthonis, J; De Baerdemaeker, J",
          "Abstract": "The lack of available workforce threatens the sustainability and preservation of the agricultural industry in developed countries. The raise of salaries will additionally have a negative impact on the viability of this industry. European farms risk to disappear from markets if no solution is found for the shortage of affordable operating resources. Labor is the most important component in the production cost of a strawberry. In order to preserve the strawberry sites in Europe, the cultivation must become more cost-efficient. The agricultural R&D-company Octinion develops a picking robot that harvests strawberries on tabletop cultivation systems. The robot is a complete, fully autonomous system: it detects the ripe fruits, picks them with no damage, and puts them in a punnet (box in which strawberries are put). The picking system is mounted on a mobile platform that navigates autonomously through the greenhouse. The current prototype of the robot is able to pick a strawberry in 4 seconds. It is therefore a viable alternative for costly human pickers who will be less available in the near future. (C) 2018, IFAC (International Federation of Automatic Control) Hosting by Elsevier Ltd. All rights reserved.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Deep Learning Based Improved Classification System for Designing Tomato Harvesting Robot",
        "year": "2018",
        "algorithm": "deep learning",
        "performance": "",
        "citation": "@article{zhang2018deep,\n  title={Deep learning based improved classification system for designing tomato harvesting robot},\n  author={Zhang, Li and Jia, Jingdun and Gui, Guan and Hao, Xia and Gao, Wanlin and Wang, Minjuan},\n  journal={IEEE Access},\n  volume={6},\n  pages={67940--67950},\n  year={2018},\n  publisher={IEEE}\n}\n",
        "fruit_veg": "tomato",
        "paper": {
          "relevant": "y",
          "": "\\cite{zhang2018deep}",
          "Article Title": "Deep Learning Based Improved Classification System for Designing Tomato Harvesting Robot",
          "Times Cited, All Databases": "60",
          "Publication Year": "2018",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{zhang2018deep,\n  title={Deep learning based improved classification system for designing tomato harvesting robot},\n  author={Zhang, Li and Jia, Jingdun and Gui, Guan and Hao, Xia and Gao, Wanlin and Wang, Minjuan},\n  journal={IEEE Access},\n  volume={6},\n  pages={67940--67950},\n  year={2018},\n  publisher={IEEE}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "fruit classification",
          "fruit/veg": "tomato",
          "Data Modality": "",
          "Learning Algorithm": "deep learning",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Zhang, L; Jia, JD; Gui, G; Ha, X; Gao, WL; Wang, MJ",
          "Abstract": "Maturity level-based classification system plays an essential role in the design of tomato harvesting robot. Traditional knowledge-based systems are unable to meet the current production management requirements of precision picking, because they are time-consuming and have low accuracy. Our research proposes an improved deep learning-based classification method that improves the accuracy and scalability of tomato ripeness with a small amount of training data. This study was on the relationship between different dataset augmentation methods and prediction results of final classification task. We implemented classification systems based on convolutional neural network (CNN), by training and validating the model on different augmented datasets and tried to choose an optimal augmentation method for datasets. The experimental results showed an average accuracy of 91.9% with a less than 0.01-s prediction time. Compared to the existing methods, our solution achieved better prediction results both in terms of accuracy and time consumption. Moreover, this is a versatile method and can be extended to other related fields.",
          "Keywords Plus": "CONVOLUTIONAL NEURAL-NETWORK"
        }
      },
      {
        "title": "Semantic Segmentation of Litchi Branches Using DeepLabV3+Model",
        "year": "2020",
        "algorithm": "deeplabV3",
        "performance": "",
        "citation": "@article{peng2020semantic,\n  title={Semantic segmentation of litchi branches using DeepLabV3+ model},\n  author={Peng, Hongxing and Xue, Chao and Shao, Yuanyuan and Chen, Keyin and Xiong, Juntao and Xie, Zhihua and Zhang, Liuhong},\n  journal={Ieee Access},\n  volume={8},\n  pages={164546--164555},\n  year={2020},\n  publisher={IEEE}\n}\n",
        "fruit_veg": "Litch",
        "paper": {
          "relevant": "y",
          "": "\\cite{peng2020semantic}",
          "Article Title": "Semantic Segmentation of Litchi Branches Using DeepLabV3+Model",
          "Times Cited, All Databases": "61",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{peng2020semantic,\n  title={Semantic segmentation of litchi branches using DeepLabV3+ model},\n  author={Peng, Hongxing and Xue, Chao and Shao, Yuanyuan and Chen, Keyin and Xiong, Juntao and Xie, Zhihua and Zhang, Liuhong},\n  journal={Ieee Access},\n  volume={8},\n  pages={164546--164555},\n  year={2020},\n  publisher={IEEE}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "semantic segmentation",
          "fruit/veg": "Litch",
          "Data Modality": "",
          "Learning Algorithm": "deeplabV3",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Peng, HX; Xue, C; Shao, YY; Chen, KY; Xiong, JT; Xie, ZH; Zhang, LH",
          "Abstract": "Litchi is often harvested by clamping and cutting the branches, which are small and can easily be damaged by the picking robot. Therefore, the detection of litchi branches is particularly significant. In this article, an fully convolutional neural network-based semantic segmentation algorithm is proposed to semantically segment the litchi branches. First, the DeepLabV3+ semantic segmentation model is combined with the Xception depth separable convolution feature. Second, transfer learning and data enhancement are used to accelerate the convergence and improve the robustness of the model. Third, a coding and a decoding structure are adopted to reduce the number of network parameters. The decoding structure uses upsampling and the shallow features to fuse, and the same weight is assigned to ensure that the shallow feature semantics and the deep feature semantics are evenly distributed. Fourth, using atrous spatial pyramid pooling, we can better extract the semantic pixel position information without increasing the number of weight parameters. Finally, different sizes of hole convolution are used to ensure the prediction accuracy of small targets. Experiment results demonstrated that the DeepLabV3+ model using the Xception_65 feature extraction network obtained the best results, achieving a mean intersection over union (MIoU) of 0.765, which is 0.144 higher than the MIoU of 0.621 of the original DeepLabV3+ model. Meanwhile, the DeepLabV3+ model using the Xception_65 network has greater robustness, far exceeding the PSPNet_101 and ICNet in detection accuracy. The aforementioned results indicated that the proposed model produced better detection results. It can provide powerful technical support for the gripper picking robot to find fruit branches and provide a new solution for the problem of aim detection and recognition in agricultural automation.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Task-based agricultural mobile robots in arable farming: A review",
        "year": "2017",
        "algorithm": "",
        "performance": "",
        "citation": "@article{aravind2017task,\n  title={Task-based agricultural mobile robots in arable farming: A review},\n  author={Aravind, Krishnaswamy R and Raja, Purushothaman and P{\\'e}rez-Ruiz, Manuel},\n  journal={Spanish journal of agricultural research},\n  volume={15},\n  number={1},\n  pages={e02R01--e02R01},\n  year={2017}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{aravind2017task}",
          "Article Title": "Task-based agricultural mobile robots in arable farming: A review",
          "Times Cited, All Databases": "63",
          "Publication Year": "2017",
          "Highly Cited Status": "",
          "Publisher": "Spanish journal of agricultural research",
          "Citation": "@article{aravind2017task,\n  title={Task-based agricultural mobile robots in arable farming: A review},\n  author={Aravind, Krishnaswamy R and Raja, Purushothaman and P{\\'e}rez-Ruiz, Manuel},\n  journal={Spanish journal of agricultural research},\n  volume={15},\n  number={1},\n  pages={e02R01--e02R01},\n  year={2017}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Aravind, KR; Raja, P; P¨¦rez-Ruiz, M",
          "Abstract": "In agriculture (in the context of this paper, the terms agriculture and farming refer to only the farming of crops and exclude the farming of animals), smart farming and automated agricultural technology have emerged as promising methodologies for increasing the crop productivity without sacrificing produce quality. The emergence of various robotics technologies has facilitated the application of these techniques in agricultural processes. However, incorporating this technology in farms has proven to be challenging because of the large variations in shape, size, rate and type of growth, type of produce, and environmental requirements for different types of crops. Agricultural processes are chains of systematic, repetitive, and time-dependent tasks. However, some agricultural processes differ based on the type of farming, namely permanent crop farming and arable farming. Permanent crop farming includes permanent crops or woody plants such as orchards and vineyards whereas arable farming includes temporary crops such as wheat and rice. Major operations in open arable farming include tilling, soil analysis, seeding, transplanting, crop scouting, pest control, weed removal and harvesting where robots can assist in performing all of these tasks. Each specific operation requires axillary devices and sensors with specific functions. This article reviews the latest advances in the application of mobile robots in these agricultural operations for open arable farming and provide an overview of the systems and techniques that are used. This article also discusses various challenges for future improvements in using reliable mobile robots for arable farming.",
          "Keywords Plus": "WEED-CONTROL; MACHINE VISION; SYSTEM; ACCURACY; VEHICLES; PLATFORM; DESIGN"
        }
      },
      {
        "title": "Characterizing apple picking patterns for robotic harvesting",
        "year": "2016",
        "algorithm": "",
        "performance": "",
        "citation": "@article{li2016characterizing,\n  title={Characterizing apple picking patterns for robotic harvesting},\n  author={Li, Jun and Karkee, Manoj and Zhang, Qin and Xiao, Kehui and Feng, Tao},\n  journal={Computers and Electronics in Agriculture},\n  volume={127},\n  pages={633--640},\n  year={2016},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "apples",
        "paper": {
          "relevant": "y",
          "": "\\cite{li2016characterizing}",
          "Article Title": "Characterizing apple picking patterns for robotic harvesting",
          "Times Cited, All Databases": "65",
          "Publication Year": "2016",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{li2016characterizing,\n  title={Characterizing apple picking patterns for robotic harvesting},\n  author={Li, Jun and Karkee, Manoj and Zhang, Qin and Xiao, Kehui and Feng, Tao},\n  journal={Computers and Electronics in Agriculture},\n  volume={127},\n  pages={633--640},\n  year={2016},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "",
          "fruit/veg": "apples",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Li, J; Karkee, M; Zhang, Q; Xiao, KH; Feng, T",
          "Abstract": "Fruit detachment is one of the essential tasks in apple harvest. The resistance of detaching an apple from the tree is largely influenced by picking patterns. This research aimed at gaining an understanding of fruit detachment process under different picking patterns, focused on characterizing those processes using a few key detaching parameters. It also aimed at identifying an effective robotic picking pattern using a three-finger gripper. To accomplish this goal, one manual and three robotic apple checking patterns were studied, by measuring and analyzing the minimal grasping pressure required to remove a fruit from the tree. The corresponding damage level on removed fruit was also analyzed. The results revealed that manual picking could create a bending moment which helped to reduce the required grasping pressure for fruit detachment, and resulted in no picking-induced fruit bruising on all collected samples. Results obtained from all three robotic picking patterns indicated that the use of a three-finger gripper required higher grasping pressure to detach apples, which resulted in higher percentages of picking-induced fruit bruising. It was found that one of the studied robotic patterns could offer a more manual-like performance than the other two robotic picking patterns. Further investigation assessing potentials and limitations of this identified robotic picking pattern on a more comprehensive scale to gain a deeper understanding of how this pattern works is recommended before it can be used as the base pattern for developing effective and efficient apple picking robots. (C) 2016 Elsevier B.V. All rights reserved.",
          "Keywords Plus": "FRUIT; HORTICULTURE; GRIPPER; DESIGN"
        }
      },
      {
        "title": "A novel green apple segmentation algorithm based on ensemble U-Net under complex orchard environment",
        "year": "2021",
        "algorithm": "ensember U-net",
        "performance": "",
        "citation": "@article{li2021novel,\n  title={A novel green apple segmentation algorithm based on ensemble U-Net under complex orchard environment},\n  author={Li, Qianwen and Jia, Weikuan and Sun, Meili and Hou, Sujuan and Zheng, Yuanjie},\n  journal={Computers and Electronics in Agriculture},\n  volume={180},\n  pages={105900},\n  year={2021},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "apples",
        "paper": {
          "relevant": "y",
          "": "\\cite{li2021novel}",
          "Article Title": "A novel green apple segmentation algorithm based on ensemble U-Net under complex orchard environment",
          "Times Cited, All Databases": "59",
          "Publication Year": "2021",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{li2021novel,\n  title={A novel green apple segmentation algorithm based on ensemble U-Net under complex orchard environment},\n  author={Li, Qianwen and Jia, Weikuan and Sun, Meili and Hou, Sujuan and Zheng, Yuanjie},\n  journal={Computers and Electronics in Agriculture},\n  volume={180},\n  pages={105900},\n  year={2021},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "detection and segmentation",
          "fruit/veg": "apples",
          "Data Modality": "",
          "Learning Algorithm": "ensember U-net",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Li, QW; Jia, WK; Sun, ML; Hou, SJ; Zheng, YJ",
          "Abstract": "Fruit segmentation is a critical step for fruit recognition, and key to the efficiency of a robotic vision system for harvesting and accurate orchard yield estimation. Due to the unstructured characteristics of orchard environment, the segmentation performance of traditional segmentation algorithms is insufficient for handling green target fruits in a complex environment. Deep learning algorithms bring a fresh perspective to target segmentation. This study proposes an ensemble U-Net segmentation model suitable for small sample datasets. Edge structures are designed by integrating residual blocks and gated convolutions to obtain the boundary semantic information of the target image; atrous convolutions are applied to resolve the contradiction between the resolution of the feature map and the receiving field, retain more multiscale context information and achieve target fruit segmentation. An atrous spatial pyramid pooling (ASPP) structure is applied to merge the edge features and the high-level features of U-Net. The experimental results show that the proposed method effectively improves the segmentation accuracy of the target fruit and the generalization ability of the model. The proposed method extends the application scope of the harvesting robot and orchard yield measurements, thereby providing a theoretical reference for other fruit and vegetable target fruit segmentation efforts.",
          "Keywords Plus": "CROWD EVACUATION; RECOGNITION; MODEL; VISION"
        }
      },
      {
        "title": "3D-vision based detection, localization, and sizing of broccoli heads in the field",
        "year": "2017",
        "algorithm": "",
        "performance": "",
        "citation": "@article{kusumam20173d,\n  title={3D-vision based detection, localization, and sizing of broccoli heads in the field},\n  author={Kusumam, Keerthy and Krajn{\\'\\i}k, Tom{\\'a}{\\v{s}} and Pearson, Simon and Duckett, Tom and Cielniak, Grzegorz},\n  journal={Journal of Field Robotics},\n  volume={34},\n  number={8},\n  pages={1505--1518},\n  year={2017},\n  publisher={Wiley Online Library}\n}\n",
        "fruit_veg": "broccoli head",
        "paper": {
          "relevant": "y",
          "": "\\cite{kusumam20173d}",
          "Article Title": "3D-vision based detection, localization, and sizing of broccoli heads in the field",
          "Times Cited, All Databases": "56",
          "Publication Year": "2017",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{kusumam20173d,\n  title={3D-vision based detection, localization, and sizing of broccoli heads in the field},\n  author={Kusumam, Keerthy and Krajn{\\'\\i}k, Tom{\\'a}{\\v{s}} and Pearson, Simon and Duckett, Tom and Cielniak, Grzegorz},\n  journal={Journal of Field Robotics},\n  volume={34},\n  number={8},\n  pages={1505--1518},\n  year={2017},\n  publisher={Wiley Online Library}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "",
          "fruit/veg": "broccoli head",
          "Data Modality": "3D",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Kusumam, K; Krajn¨ªk, T; Pearson, S; Duckett, T; Cielniak, G",
          "Abstract": "This paper describes a 3D vision system for robotic harvesting of broccoli using low-cost RGB-D sensors, which was developed and evaluated using sensory data collected under real-world field conditions in both the UK and Spain. The presented method addresses the tasks of detecting mature broccoli heads in the field and providing their 3D locations relative to the vehicle. The paper evaluates different 3D features, machine learning, and temporal filtering methods for detection of broccoli heads. Our experiments show that a combination of Viewpoint Feature Histograms, Support Vector Machine classifier, and a temporal filter to track the detected heads results in a system that detects broccoli heads with high precision. We also show that the temporal filtering can be used to generate a 3D map of the broccoli head positions in the field. Additionally, we present methods for automatically estimating the size of the broccoli heads, to determine when a head is ready for harvest. All of the methods were evaluated using ground-truth data from both the UK and Spain, which we also make available to the research community for subsequent algorithm development and result comparison. Cross-validation of the system trained on the UK dataset on the Spanish dataset, and vice versa, indicated good generalization capabilities of the system, confirming the strong potential of low-cost 3D imaging for commercial broccoli harvesting.",
          "Keywords Plus": "MACHINE VISION; RECOGNITION; PLANTS"
        }
      },
      {
        "title": "An Overview of Cooperative Robotics in Agriculture",
        "year": "2021",
        "algorithm": "",
        "performance": "",
        "citation": "@article{lytridis2021overview,\n  title={An overview of cooperative robotics in agriculture},\n  author={Lytridis, Chris and Kaburlasos, Vassilis G and Pachidis, Theodore and Manios, Michalis and Vrochidou, Eleni and Kalampokas, Theofanis and Chatzistamatis, Stamatis},\n  journal={Agronomy},\n  volume={11},\n  number={9},\n  pages={1818},\n  year={2021},\n  publisher={MDPI}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{lytridis2021overview}",
          "Article Title": "An Overview of Cooperative Robotics in Agriculture",
          "Times Cited, All Databases": "56",
          "Publication Year": "2021",
          "Highly Cited Status": "",
          "Publisher": "Agronomy",
          "Citation": "@article{lytridis2021overview,\n  title={An overview of cooperative robotics in agriculture},\n  author={Lytridis, Chris and Kaburlasos, Vassilis G and Pachidis, Theodore and Manios, Michalis and Vrochidou, Eleni and Kalampokas, Theofanis and Chatzistamatis, Stamatis},\n  journal={Agronomy},\n  volume={11},\n  number={9},\n  pages={1818},\n  year={2021},\n  publisher={MDPI}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Lytridis, C; Kaburlasos, VG; Pachidis, T; Manios, M; Vrochidou, E; Kalampokas, T; Chatzistamatis, S",
          "Abstract": "Agricultural robotics has been a popular subject in recent years from an academic as well as a commercial point of view. This is because agricultural robotics addresses critical issues such as seasonal shortages in manual labor, e.g., during harvest, as well as the increasing concern regarding environmentally friendly practices. On one hand, several individual agricultural robots have already been developed for specific tasks (e.g., for monitoring, spraying, harvesting, transport, etc.) with varying degrees of effectiveness. On the other hand, the use of cooperative teams of agricultural robots in farming tasks is not as widespread; yet, it is an emerging trend. This paper presents a comprehensive overview of the work carried out so far in the area of cooperative agricultural robotics and identifies the state-of-the-art. This paper also outlines challenges to be addressed in fully automating agricultural production; the latter is promising for sustaining an increasingly vast human population, especially in cases of pandemics such as the recent COVID-19 pandemic.",
          "Keywords Plus": "PATH PLANNING METHOD; PRECISION AGRICULTURE; UNMANNED AERIAL; AREA COVERAGE; SYSTEM; FLEETS; UAV; COLLABORATION; OPTIMIZATION; VEHICLES"
        }
      },
      {
        "title": "Fruit Image Classification Based on MobileNetV2 with Transfer Learning Technique",
        "year": "2019",
        "algorithm": "mobileNetV2 transfer learning",
        "performance": "",
        "citation": "@inproceedings{xiang2019fruit,\n  title={Fruit image classification based on Mobilenetv2 with transfer learning technique},\n  author={Xiang, Qian and Wang, Xiaodan and Li, Rui and Zhang, Guoling and Lai, Jie and Hu, Qingshuang},\n  booktitle={Proceedings of the 3rd international conference on computer science and application engineering},\n  pages={1--7},\n  year={2019}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{xiang2019fruit}",
          "Article Title": "Fruit Image Classification Based on MobileNetV2 with Transfer Learning Technique",
          "Times Cited, All Databases": "53",
          "Publication Year": "2019",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@inproceedings{xiang2019fruit,\n  title={Fruit image classification based on Mobilenetv2 with transfer learning technique},\n  author={Xiang, Qian and Wang, Xiaodan and Li, Rui and Zhang, Guoling and Lai, Jie and Hu, Qingshuang},\n  booktitle={Proceedings of the 3rd international conference on computer science and application engineering},\n  pages={1--7},\n  year={2019}\n}\n",
          "Document Type": "Proceedings Paper",
          "Main Contribution": "fruit classification",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "mobileNetV2 transfer learning",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Xiang, Q; Wang, XD; Li, R; Zhang, GL; Lai, J; Hu, QS",
          "Abstract": "Fruit image classification is the key technology for robotic picking which can tremendously save costs and effectively improve fruit producer's competitiveness in the international fruit market. In the image classification field, deep learning technologies especially DCNNs are state-of-the-art technologies and have achieved remarkable success. But the requirements of high computation and storage resources prohibit the usages of DCNNs on resource-limited environments such as automatic harvesting robots. Therefore, we need to choose a lightweight neural network to achieve the balance of resource limitations and recognition accuracy. In this paper, a fruit image classification method based on a lightweight neural network MobileNetV2 with transfer learning technique was used to recognize fruit images. We used a MobileNetV2 network pre-trained by ImageNet dataset as a base network and then replace the top layer of the base network with a conventional convolution layer and a Softmax classifier. We applied dropout to the new-added conv2d at the same time to reduce overfitting. The pre-trained MobileNetV2 was used to extract features and the Softmax classifier was used to classify features. We trained this new model in two stages using Adam optimizer of different learning rate. This method finally achieved a classification accuracy of 85.12% in our fruit image dataset including 3670 images of 5 fruits. Compared with other network such as MobileNetVl, InceptionV3 and DenseNet121, this hybrid network implemented by Google open source deep learning framework Tensorflow can make a good compromise between accuracy and speed. Since MobileNetV2 is a lightweight neural network, the method in this paper can be deployed in low-power and limited-computing devices such as mobile phone.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Towards an Efficient Tomato Harvesting Robot: 3D Perception, Manipulation, and End-Effector",
        "year": "2021",
        "algorithm": "",
        "performance": "",
        "citation": "@article{jun2021towards,\n  title={Towards an efficient tomato harvesting robot: 3d perception, manipulation, and end-effector},\n  author={Jun, Jongpyo and Kim, Jeongin and Seol, Jaehwi and Kim, Jeongeun and Son, Hyoung Il},\n  journal={IEEE access},\n  volume={9},\n  pages={17631--17640},\n  year={2021},\n  publisher={IEEE}\n}\n",
        "fruit_veg": "tomato",
        "paper": {
          "relevant": "y",
          "": "\\cite{jun2021towards}",
          "Article Title": "Towards an Efficient Tomato Harvesting Robot: 3D Perception, Manipulation, and End-Effector",
          "Times Cited, All Databases": "57",
          "Publication Year": "2021",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{jun2021towards,\n  title={Towards an efficient tomato harvesting robot: 3d perception, manipulation, and end-effector},\n  author={Jun, Jongpyo and Kim, Jeongin and Seol, Jaehwi and Kim, Jeongeun and Son, Hyoung Il},\n  journal={IEEE access},\n  volume={9},\n  pages={17631--17640},\n  year={2021},\n  publisher={IEEE}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "efficiency",
          "fruit/veg": "tomato",
          "Data Modality": "3D",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Jun, J; Kim, J; Seol, J; Kim, J; Il Son, H",
          "Abstract": "Fruit and vegetable harvesting robots have been widely studied and developed in recent years. However, despite extensive research commercial tomato harvesting robots still remain a challenge. In this paper, we propose an efficient tomato harvesting robot that combines the principle of 3D perception, Manipulation, and an End-effector. For this robot, tomatoes are detected based on deep learning, after which 3D coordinates of the target crop are extracted and motion control of the manipulator based on 3D coordination. In addition, a suction pad featuring the kirigami pattern, which is a part of the suction gripper, was developed to grip individual tomatoes in clusters. A scissor-shaped cutting module with an assist unit, which is used to overcome structural limitations and implement effective cutting, was also desinged and tested. The proposed tomato harvesting robot was validated and evaluated on a laboratory testbed basd on the performance of each component. Therefore, in this study, we propose and verify a new robot design for the effective harvesting of tomatoes.",
          "Keywords Plus": "FIELD-EVALUATION; GRIPPER; VISION; DESIGN"
        }
      },
      {
        "title": "Localization and Mapping for Robots in Agriculture and Forestry: A Survey",
        "year": "2020",
        "algorithm": "",
        "performance": "",
        "citation": "@article{aguiar2020localization,\n  title={Localization and mapping for robots in agriculture and forestry: A survey},\n  author={Aguiar, Andr{\\'e} Silva and Dos Santos, Filipe Neves and Cunha, Jos{\\'e} Boaventura and Sobreira, H{\\'e}ber and Sousa, Armando Jorge},\n  journal={Robotics},\n  volume={9},\n  number={4},\n  pages={97},\n  year={2020},\n  publisher={MDPI}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{aguiar2020localization}",
          "Article Title": "Localization and Mapping for Robots in Agriculture and Forestry: A Survey",
          "Times Cited, All Databases": "57",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "Robotics",
          "Citation": "@article{aguiar2020localization,\n  title={Localization and mapping for robots in agriculture and forestry: A survey},\n  author={Aguiar, Andr{\\'e} Silva and Dos Santos, Filipe Neves and Cunha, Jos{\\'e} Boaventura and Sobreira, H{\\'e}ber and Sousa, Armando Jorge},\n  journal={Robotics},\n  volume={9},\n  number={4},\n  pages={97},\n  year={2020},\n  publisher={MDPI}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Aguiar, AS; dos Santos, FN; Cunha, JB; Sobreira, H; Sousa, AJ",
          "Abstract": "Research and development of autonomous mobile robotic solutions that can perform several active agricultural tasks (pruning, harvesting, mowing) have been growing. Robots are now used for a variety of tasks such as planting, harvesting, environmental monitoring, supply of water and nutrients, and others. To do so, robots need to be able to perform online localization and, if desired, mapping. The most used approach for localization in agricultural applications is based in standalone Global Navigation Satellite System-based systems. However, in many agricultural and forest environments, satellite signals are unavailable or inaccurate, which leads to the need of advanced solutions independent from these signals. Approaches like simultaneous localization and mapping and visual odometry are the most promising solutions to increase localization reliability and availability. This work leads to the main conclusion that, few methods can achieve simultaneously the desired goals of scalability, availability, and accuracy, due to the challenges imposed by these harsh environments. In the near future, novel contributions to this field are expected that will help one to achieve the desired goals, with the development of more advanced techniques, based on 3D localization, and semantic and topological mapping. In this context, this work proposes an analysis of the current state-of-the-art of localization and mapping approaches in agriculture and forest environments. Additionally, an overview about the available datasets to develop and test these approaches is performed. Finally, a critical analysis of this research field is done, with the characterization of the literature using a variety of metrics.",
          "Keywords Plus": "DATA ASSOCIATION; SLAM; SYSTEM; DATASET; MAPS"
        }
      },
      {
        "title": "Using depth cameras to extract structural parameters to assess the growth state and yield of cauliflower crops",
        "year": "2016",
        "algorithm": "",
        "performance": "",
        "citation": "@article{andujar2016using,\n  title={Using depth cameras to extract structural parameters to assess the growth state and yield of cauliflower crops},\n  author={Andujar, Dionisio and Ribeiro, Angela and Fern{\\'a}ndez-Quintanilla, C{\\'e}sar and Dorado, Jose},\n  journal={Computers and Electronics in Agriculture},\n  volume={122},\n  pages={67--73},\n  year={2016},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "cauliflower",
        "paper": {
          "relevant": "y",
          "": "\\cite{andujar2016using}",
          "Article Title": "Using depth cameras to extract structural parameters to assess the growth state and yield of cauliflower crops",
          "Times Cited, All Databases": "57",
          "Publication Year": "2016",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{andujar2016using,\n  title={Using depth cameras to extract structural parameters to assess the growth state and yield of cauliflower crops},\n  author={Andujar, Dionisio and Ribeiro, Angela and Fern{\\'a}ndez-Quintanilla, C{\\'e}sar and Dorado, Jose},\n  journal={Computers and Electronics in Agriculture},\n  volume={122},\n  pages={67--73},\n  year={2016},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "growth state",
          "fruit/veg": "cauliflower",
          "Data Modality": "3D",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "And¨²jar, D; Ribeiro, A; Fern¨¢ndez-Quintanilla, C; Dorado, J",
          "Abstract": "The use of robotic systems for horticultural crops is widely known. However, the use of these systems in cruciferous vegetables remains a challenge. The case of cauliflower crops is of special relevance because it is a hand-harvested crop for which the cutting time is visually chosen. This methodology leads to a yield reduction, as some inflorescences are cut before ripening because the leaves hide their real state of maturity. This work proposes the use of depth cameras instead of visual estimation. Using Kinect Fusion algorithms, depth cameras create a 3D point cloud from the depth video stream and consequently generate solid 3D models, which have been compared to the actual structural parameters of cauliflower plants. The results show good consistency among depth image models and ground truth from the actual structural parameters. In addition, the best time for individual fruit cutting could be detected using these models, which enabled the optimization of harvesting and increased yields. The accuracy of the models deviated from the ground truth by less than 2 cm in diameter/height, whereas the fruit volume estimation showed an error below 0.6% overestimation. Analysis of the structural parameters revealed a significant correlation between estimated and actual values of the volume of plants and fruit weight. These results show the potential of depth cameras to be used as a precise tool in estimating the degree of ripeness during the harvesting of cauliflower and thereby optimizing the crop profitability. (C) 2016 Elsevier B.V. All rights reserved.",
          "Keywords Plus": "PLANT; METHODOLOGY; PHOTOGRAPHY; SENSORS"
        }
      },
      {
        "title": "The Smart Image Recognition Mechanism for Crop Harvesting System in Intelligent Agriculture",
        "year": "2020",
        "algorithm": "",
        "performance": "",
        "citation": "@article{horng2019smart,\n  title={The smart image recognition mechanism for crop harvesting system in intelligent agriculture},\n  author={Horng, Gwo-Jiun and Liu, Min-Xiang and Chen, Chao-Chun},\n  journal={IEEE Sensors Journal},\n  volume={20},\n  number={5},\n  pages={2766--2781},\n  year={2019},\n  publisher={IEEE}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{horng2019smart}",
          "Article Title": "The Smart Image Recognition Mechanism for Crop Harvesting System in Intelligent Agriculture",
          "Times Cited, All Databases": "52",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{horng2019smart,\n  title={The smart image recognition mechanism for crop harvesting system in intelligent agriculture},\n  author={Horng, Gwo-Jiun and Liu, Min-Xiang and Chen, Chao-Chun},\n  journal={IEEE Sensors Journal},\n  volume={20},\n  number={5},\n  pages={2766--2781},\n  year={2019},\n  publisher={IEEE}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "recognition",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Horng, GJ; Liu, MX; Chen, CC",
          "Abstract": "This study proposed a harvesting system based on the Internet of Things technology and smart image recognition. Farming decisions require extensive experience; with the proposed system, crop maturity can be determined through object detection by training neural network models, and mature crops can then be harvested using robotic arms. Keras was used to construct a multilayer perceptron machine learning model and to predict multiaxial robotic arm movements and position. Following the execution of object detection on images, the pixel coordinates of the central point of the target crop in the image were used as neural network input, whereas the robotic arms were regarded as the output side. A MobileNet version 2 convolutional neural network was then used as the image feature extraction model, which was combined with a single shot multibox detector model as the posterior layer to form an object detection model. The model then performed crop detection by collecting and tagging images. Empirical evidence shows that the proposed model training had a mean average precision (mAP) of 84%, which was higher than that of other models; a mAP of 89% was observed from the arm picking results.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "A robust fruit image segmentation algorithm against varying illumination for vision system of fruit harvesting robot",
        "year": "2017",
        "algorithm": "",
        "performance": "",
        "citation": "@article{wang2017robust,\n  title={A robust fruit image segmentation algorithm against varying illumination for vision system of fruit harvesting robot},\n  author={Wang, Chenglin and Tang, Yunchao and Zou, Xiangjun and SiTu, Weiming and Feng, Wenxian},\n  journal={Optik},\n  volume={131},\n  pages={626--631},\n  year={2017},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{wang2017robust}",
          "Article Title": "A robust fruit image segmentation algorithm against varying illumination for vision system of fruit harvesting robot",
          "Times Cited, All Databases": "52",
          "Publication Year": "2017",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{wang2017robust,\n  title={A robust fruit image segmentation algorithm against varying illumination for vision system of fruit harvesting robot},\n  author={Wang, Chenglin and Tang, Yunchao and Zou, Xiangjun and SiTu, Weiming and Feng, Wenxian},\n  journal={Optik},\n  volume={131},\n  pages={626--631},\n  year={2017},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "detection and segmentation",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Wang, CL; Tang, YC; Zou, XJ; SiTu, WM; Feng, WX",
          "Abstract": "Vision system is the crucial component of fruit harvesting robot for recognising fruit, however, which is seriously affected by varying illumination when the robot works in real natural environment. A robust fruit segmentation algorithm against varying illumination for vision system was proposed with the aim of effectively extracting fruit object in the natural environment. The method involved the application of improved wavelet transform to fruit image to normalise illumination of object surface. Then Retinex-based image enhancement algorithm was used to highlight fruit object of illumination normalised image. Finally fruit image was segmented by implementing K-means clustering. Three kinds of fruit images of different colour under sunny and cloudy days were segment using the proposed method respectively and the experimental results showed that the proposed algorithm could be robust against the influence of varying illumination and precisely segment different colour fruits. (C) 2016 Elsevier GmbH. All rights reserved.",
          "Keywords Plus": "MACHINE"
        }
      },
      {
        "title": "Detecting citrus fruits and occlusion recovery under natural illumination conditions",
        "year": "2015",
        "algorithm": "",
        "performance": "",
        "citation": "@article{lu2015detecting,\n  title={Detecting citrus fruits and occlusion recovery under natural illumination conditions},\n  author={Lu, Jun and Sang, Nong},\n  journal={Computers and Electronics in Agriculture},\n  volume={110},\n  pages={121--130},\n  year={2015},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "citrus",
        "paper": {
          "relevant": "y",
          "": "\\cite{lu2015detecting}",
          "Article Title": "Detecting citrus fruits and occlusion recovery under natural illumination conditions",
          "Times Cited, All Databases": "59",
          "Publication Year": "2015",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{lu2015detecting,\n  title={Detecting citrus fruits and occlusion recovery under natural illumination conditions},\n  author={Lu, Jun and Sang, Nong},\n  journal={Computers and Electronics in Agriculture},\n  volume={110},\n  pages={121--130},\n  year={2015},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "detection and occlusion recovery",
          "fruit/veg": "citrus",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Lu, J; Sang, N",
          "Abstract": "A method based on color information and contour fragments was developed to identify citrus fruits in variable illumination conditions within tree canopy, in order to guide the robots for harvesting citrus fruits. The color properties of fruit targets within citrus-grove scene were analyzed, a preliminary segmentation method was put forward by fusing the chromatic aberration information and normalized RGB model. The set of contour fragments was constructed by detecting the significant edges of chromatic aberration map and the corners within these edges. The valid subset was chosen out by three indicators of every fragment: length, bending degree, and concavity or convexity. The combination analysis was done for these valid contour fragments, and the ellipse fitting was used for every subset of valid fragments to recover the occluded fruits. The partial order relationship was derived based on the distribution of the edge within the overlapped area. The results showed that occluded fruits were effectively recovered under natural outdoor light conditions using the proposed method, and the relative error was 5.27%. The partial order relation of fruit targets provide key cues for path planning of harvesting robot. (C) 2014 Elsevier B.V. All rights reserved.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "DESIGN AND FIELD EVALUATION OF A ROBOTIC APPLE HARVESTING SYSTEM WITH A 3D-PRINTED SOFT-ROBOTIC END-EFFECTOR",
        "year": "2019",
        "algorithm": "",
        "performance": "",
        "citation": "@article{hohimer2019design,\n  title={Design and field evaluation of a robotic apple harvesting system with a 3D-printed soft-robotic end-effector},\n  author={Hohimer, Cameron J and Wang, Heng and Bhusal, Santosh and Miller, John and Mo, Changki and Karkee, Manoj},\n  journal={Transactions of the ASABE},\n  volume={62},\n  number={2},\n  pages={405--414},\n  year={2019},\n  publisher={American Society of Agricultural and Biological Engineers}\n}\n",
        "fruit_veg": "apples",
        "paper": {
          "relevant": "y",
          "": "\\cite{hohimer2019design}",
          "Article Title": "DESIGN AND FIELD EVALUATION OF A ROBOTIC APPLE HARVESTING SYSTEM WITH A 3D-PRINTED SOFT-ROBOTIC END-EFFECTOR",
          "Times Cited, All Databases": "58",
          "Publication Year": "2019",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{hohimer2019design,\n  title={Design and field evaluation of a robotic apple harvesting system with a 3D-printed soft-robotic end-effector},\n  author={Hohimer, Cameron J and Wang, Heng and Bhusal, Santosh and Miller, John and Mo, Changki and Karkee, Manoj},\n  journal={Transactions of the ASABE},\n  volume={62},\n  number={2},\n  pages={405--414},\n  year={2019},\n  publisher={American Society of Agricultural and Biological Engineers}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "end-effector",
          "fruit/veg": "apples",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "end-effector",
          "Performance": "",
          "challenges": "",
          "Authors": "Hohimer, CJ; Wang, H; Bhusal, S; Miller, J; Mo, C; Karkee, M",
          "Abstract": "Fresh market apple harvesting is a difficult task that relies entirely on manual labor. Much research has been done on the development of mechanical harvesting techniques. Several selective harvesting robots have been developed for research studies, but there are no commercially available robotic systems. This article discusses the design and development of a novel pneumatic 3D-printed soft-robotic end-effector to facilitate apple separation. The end-effector was integrated into a robotic system with five degrees of freedom that was designed to simplify the picking sequence and reduce costs compared to previous versions. Apples were successfully harvested using the low-cost robotic system in a commercial orchard during the fall 2017 harvest. A detachment success rate on attempted apples of 67% was achieved, with an average time of 7.3 s per fruit from separation to storage bin. By conducting this study in an orchard where problematic apples were not removed to increase the detachment success rate, current pruning and thinning practices were assessed to help lay the foundation for future studies and develop strategies for successfully harvesting apples that are difficult to detach.",
          "Keywords Plus": "FRUIT; PATTERNS"
        }
      },
      {
        "title": "THE DEVELOPMENT OF MECHANICAL APPLE HARVESTING TECHNOLOGY: A REVIEW",
        "year": "2016",
        "algorithm": "",
        "performance": "",
        "citation": "@article{zhang2016development,\n  title={The development of mechanical apple harvesting technology: A review},\n  author={Zhang, Zhao and Heinemann, Paul H and Liu, Jude and Baugher, Tara A and Schupp, James R},\n  journal={Transactions of the ASABE},\n  volume={59},\n  number={5},\n  pages={1165--1180},\n  year={2016},\n  publisher={American Society of Agricultural and Biological Engineers}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{zhang2016development}",
          "Article Title": "THE DEVELOPMENT OF MECHANICAL APPLE HARVESTING TECHNOLOGY: A REVIEW",
          "Times Cited, All Databases": "54",
          "Publication Year": "2016",
          "Highly Cited Status": "",
          "Publisher": "Transactions of the ASABE",
          "Citation": "@article{zhang2016development,\n  title={The development of mechanical apple harvesting technology: A review},\n  author={Zhang, Zhao and Heinemann, Paul H and Liu, Jude and Baugher, Tara A and Schupp, James R},\n  journal={Transactions of the ASABE},\n  volume={59},\n  number={5},\n  pages={1165--1180},\n  year={2016},\n  publisher={American Society of Agricultural and Biological Engineers}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Zhang, Z; Heinemann, PH; Liu, J; Baugher, TA; Schupp, JR",
          "Abstract": "Harvest is one of the most challenging production tasks in apple production. It is a labor-intensive operation, and mechanization has been slow in development for a myriad of reasons. With increasing labor costs and decreasing availability of harvest employees, researchers have been focusing on mechanical apple harvest technologies. Studies on semi-automatic harvesters, harvest robots, and harvest-assist platforms target development of mechanical apple harvesting. Semi-automatic harvesters for fresh market apples have not been commercialized due to unacceptably high incidence of apple bruising. Harvest robots have not been commercialized due to their high costs and low efficiency. Apple harvest-assist platforms are a technology that has the potential to be more widely commercialized based on the current research.",
          "Keywords Plus": "TREE FRUIT HARVESTER; IMPULSE TRUNK SHAKER; THE-ROW HARVESTER; AUTOMATIC RECOGNITION; DESIGN; ORCHARD; QUALITY; PROTOTYPE; DAMAGE; PERFORMANCE"
        }
      },
      {
        "title": "Fruit Detectability Analysis for Different Camera Positions in Sweet-Pepper",
        "year": "2014",
        "algorithm": "",
        "performance": "",
        "citation": "@article{hemming2014fruit,\n  title={Fruit detectability analysis for different camera positions in sweet-pepper},\n  author={Hemming, Jochen and Ruizendaal, Jos and Hofstee, Jan Willem and Van Henten, Eldert J},\n  journal={Sensors},\n  volume={14},\n  number={4},\n  pages={6032--6044},\n  year={2014},\n  publisher={MDPI}\n}\n",
        "fruit_veg": "sweet pepper",
        "paper": {
          "relevant": "y",
          "": "\\cite{hemming2014fruit}",
          "Article Title": "Fruit Detectability Analysis for Different Camera Positions in Sweet-Pepper",
          "Times Cited, All Databases": "53",
          "Publication Year": "2014",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{hemming2014fruit,\n  title={Fruit detectability analysis for different camera positions in sweet-pepper},\n  author={Hemming, Jochen and Ruizendaal, Jos and Hofstee, Jan Willem and Van Henten, Eldert J},\n  journal={Sensors},\n  volume={14},\n  number={4},\n  pages={6032--6044},\n  year={2014},\n  publisher={MDPI}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "detection",
          "fruit/veg": "sweet pepper",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Hemming, J; Ruizendaal, J; Hofstee, JW; van Henten, EJ",
          "Abstract": "For robotic harvesting of sweet-pepper fruits in greenhouses a sensor system is required to detect and localize the fruits on the plants. Due to the complex structure of the plant, most fruits are (partially) occluded when an image is taken from one viewpoint only. In this research the effect of multiple camera positions and viewing angles on fruit visibility and detectability was investigated. A recording device was built which allowed to place the camera under different azimuth and zenith angles and to move the camera horizontally along the crop row. Fourteen camera positions were chosen and the fruit visibility in the recorded images was manually determined for each position. For images taken from one position only with the criterion of maximum 50% occlusion per fruit, the fruit detectability (FD) was in no case higher than 69%. The best single positions were the front views and looking with a zenith angle of 60 degrees upwards. The FD increased when a combination was made of multiple viewpoint positions. With a combination of five favourite positions the maximum FD was 90%.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Deep Learning Techniques for Grape Plant Species Identification in Natural Images",
        "year": "2019",
        "algorithm": "deep learning",
        "performance": "",
        "citation": "@article{pereira2019deep,\n  title={Deep learning techniques for grape plant species identification in natural images},\n  author={Pereira, Carlos S and Morais, Raul and Reis, Manuel JCS},\n  journal={Sensors},\n  volume={19},\n  number={22},\n  pages={4850},\n  year={2019},\n  publisher={MDPI}\n}\n",
        "fruit_veg": "grape",
        "paper": {
          "relevant": "y",
          "": "\\cite{pereira2019deep}",
          "Article Title": "Deep Learning Techniques for Grape Plant Species Identification in Natural Images",
          "Times Cited, All Databases": "51",
          "Publication Year": "2019",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{pereira2019deep,\n  title={Deep learning techniques for grape plant species identification in natural images},\n  author={Pereira, Carlos S and Morais, Raul and Reis, Manuel JCS},\n  journal={Sensors},\n  volume={19},\n  number={22},\n  pages={4850},\n  year={2019},\n  publisher={MDPI}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "species id",
          "fruit/veg": "grape",
          "Data Modality": "",
          "Learning Algorithm": "deep learning",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Pereira, CS; Morais, R; Reis, MJCS",
          "Abstract": "Frequently, the vineyards in the Douro Region present multiple grape varieties per parcel and even per row. An automatic algorithm for grape variety identification as an integrated software component was proposed that can be applied, for example, to a robotic harvesting system. However, some issues and constraints in its development were highlighted, namely, the images captured in natural environment, low volume of images, high similarity of the images among different grape varieties, leaf senescence, and significant changes on the grapevine leaf and bunch images in the harvest seasons, mainly due to adverse climatic conditions, diseases, and the presence of pesticides. In this paper, the performance of the transfer learning and fine-tuning techniques based on AlexNet architecture were evaluated when applied to the identification of grape varieties. Two natural vineyard image datasets were captured in different geographical locations and harvest seasons. To generate different datasets for training and classification, some image processing methods, including a proposed four-corners-in-one image warping algorithm, were used. The experimental results, obtained from the application of an AlexNet-based transfer learning scheme and trained on the image dataset pre-processed through the four-corners-in-one method, achieved a test accuracy score of 77.30%. Applying this classifier model, an accuracy of 89.75% on the popular Flavia leaf dataset was reached. The results obtained by the proposed approach are promising and encouraging in helping Douro wine growers in the automatic task of identifying grape varieties.",
          "Keywords Plus": "EXTRACTION; CLASSIFICATION"
        }
      },
      {
        "title": "Analysis of a motion planning problem for sweet-pepper harvesting in a dense obstacle environment",
        "year": "2016",
        "algorithm": "",
        "performance": "",
        "citation": "@article{bac2016analysis,\n  title={Analysis of a motion planning problem for sweet-pepper harvesting in a dense obstacle environment},\n  author={Bac, C Wouter and Roorda, Tim and Reshef, Roi and Berman, Sigal and Hemming, Jochen and van Henten, Eldert J},\n  journal={Biosystems engineering},\n  volume={146},\n  pages={85--97},\n  year={2016},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "sweet pepper",
        "paper": {
          "relevant": "y",
          "": "\\cite{bac2016analysis}",
          "Article Title": "Analysis of a motion planning problem for sweet-pepper harvesting in a dense obstacle environment",
          "Times Cited, All Databases": "51",
          "Publication Year": "2016",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{bac2016analysis,\n  title={Analysis of a motion planning problem for sweet-pepper harvesting in a dense obstacle environment},\n  author={Bac, C Wouter and Roorda, Tim and Reshef, Roi and Berman, Sigal and Hemming, Jochen and van Henten, Eldert J},\n  journal={Biosystems engineering},\n  volume={146},\n  pages={85--97},\n  year={2016},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "planning path",
          "fruit/veg": "sweet pepper",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "path planning",
          "Performance": "",
          "challenges": "",
          "Authors": "Bac, CW; Roorda, T; Reshef, R; Berman, S; Hemming, J; van Henten, EJ",
          "Abstract": "To reach a fruit in an obstacle-dense crop environment, robotic fruit harvesting requires a collision-free motion of the manipulator and end-effector. A novel two-part analysis was conducted of a sweet-pepper harvesting robot based on data of fruit (N = 158) and stem locations collected from a greenhouse. The first part of the analysis compared two methods of selecting the azimuth angle of the end-effector. The new 'constrained-azimuth' method avoided risky paths and achieved a motion planning success similar to the 'full-azimuth' method. In the second part, a sensitivity analysis was conducted for five parameters specifying the crop (stem spacing and fruit location), the robot (end-effector dimensions and robot position) and the planning algorithm, to evaluate their effect on successfully finding a collision-free goal configuration and path. Reducing end-effector dimensions and widening stem spacing are promising research directions because they significantly improved goal configuration success, from 63% to 84%. However, the fruit location at the stem is the strongest influencing parameter and therefore provides an incentive to train or breed plants that develop more fruit at the front side of the plant stem. The two analyses may serve as useful tools to study motion planning problems in a dense obstacle environment. (C) 2015 IAgrE. Published by Elsevier Ltd. All rights reserved.",
          "Keywords Plus": "SENSITIVITY-ANALYSIS; ROBOT; MANIPULATORS; DESIGN"
        }
      },
      {
        "title": "Robust visual servo control in the presence of fruit motion for robotic citrus harvesting",
        "year": "2016",
        "algorithm": "",
        "performance": "",
        "citation": "@article{mehta2016robust,\n  title={Robust visual servo control in the presence of fruit motion for robotic citrus harvesting},\n  author={Mehta, Siddhartha S and MacKunis, William and Burks, Thomas F},\n  journal={Computers and Electronics in Agriculture},\n  volume={123},\n  pages={362--375},\n  year={2016},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "citrus",
        "paper": {
          "relevant": "y",
          "": "\\cite{mehta2016robust}",
          "Article Title": "Robust visual servo control in the presence of fruit motion for robotic citrus harvesting",
          "Times Cited, All Databases": "50",
          "Publication Year": "2016",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{mehta2016robust,\n  title={Robust visual servo control in the presence of fruit motion for robotic citrus harvesting},\n  author={Mehta, Siddhartha S and MacKunis, William and Burks, Thomas F},\n  journal={Computers and Electronics in Agriculture},\n  volume={123},\n  pages={362--375},\n  year={2016},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "servo control",
          "fruit/veg": "citrus",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Mehta, SS; MacKunis, W; Burks, TF",
          "Abstract": "Unknown fruit motion due to exogenous disturbances such as wind gust, canopy unloading, and particularly, fruit detachment forces can reduce overall harvesting efficiency in robotic fruit harvesting. Existing approaches relying on high-gain controllers to compensate for fruit motion are inherently susceptible to measurement noise and can lead to instability. The contribution of this paper is in the development of a robust image-based visual servo controller to regulate a robotic manipulator to a target fruit in the presence of unknown fruit motion. The robust feedback elements included in the control structure compensate for non-vanishing nonlinear disturbances without the need for high-gain feedback. Lyapunov-based stability analysis guarantees uniformly ultimately bounded regulation of the robot end-effector to a target fruit. In addition, finite-time convergence analysis is presented to show that the controller gains can be chosen to achieve the desired fruit removal rate, or cycle time. Numerical simulations with varying fruit displacement of {35, 70, 105, 140, 175,210} mm verify the feasibility of the developed controller while the performance is evaluated on a seven degrees-of-freedom kinematically redundant manipulator using an artificial citrus fruit moving with 120 mm displacement. The developed robust controller demonstrates stable closed-loop operation of the system. Further, the effect of uncertainties in field conditions such as illumination variations and partial fruit occlusion, overlapping fruit, and obstacles on the developed controller, are discussed. (C) 2016 Elsevier B.V. All rights reserved.",
          "Keywords Plus": "VISION-BASED CONTROL; PICKING; TRACKING; OBJECT; SYSTEM"
        }
      },
      {
        "title": "Improvements to and large-scale evaluation of a robotic kiwifruit harvester",
        "year": "2020",
        "algorithm": "",
        "performance": "",
        "citation": "@article{williams2020improvements,\n  title={Improvements to and large-scale evaluation of a robotic kiwifruit harvester},\n  author={Williams, Henry and Ting, Canaan and Nejati, Mahla and Jones, Mark Hedley and Penhall, Nicky and Lim, JongYoon and Seabright, Matthew and Bell, Jamie and Ahn, Ho Seok and Scarfe, Alistair and others},\n  journal={Journal of Field Robotics},\n  volume={37},\n  number={2},\n  pages={187--201},\n  year={2020},\n  publisher={Wiley Online Library}\n}\n",
        "fruit_veg": "kiwifruit",
        "paper": {
          "relevant": "y",
          "": "\\cite{williams2020improvements}",
          "Article Title": "Improvements to and large-scale evaluation of a robotic kiwifruit harvester",
          "Times Cited, All Databases": "53",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{williams2020improvements,\n  title={Improvements to and large-scale evaluation of a robotic kiwifruit harvester},\n  author={Williams, Henry and Ting, Canaan and Nejati, Mahla and Jones, Mark Hedley and Penhall, Nicky and Lim, JongYoon and Seabright, Matthew and Bell, Jamie and Ahn, Ho Seok and Scarfe, Alistair and others},\n  journal={Journal of Field Robotics},\n  volume={37},\n  number={2},\n  pages={187--201},\n  year={2020},\n  publisher={Wiley Online Library}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "",
          "fruit/veg": "kiwifruit",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Williams, H; Ting, C; Nejati, M; Jones, MH; Penhall, N; Lim, J; Seabright, M; Bell, J; Ahn, HS; Scarfe, A; Duke, M; MacDonald, B",
          "Abstract": "The growing popularity of kiwifruit orchards in New Zealand is increasing the already high demand for seasonal labourers. A novel robotic kiwifruit harvester has been designed and built to help meet this demand [H. A. Williams et al. Biosystems Eng. 181 (2019), pp. 140-156]. Early evaluations of the platform have shown good results with the system capable of harvesting 51.0% of 1,456 kiwifruit in four bays with an average cycle-time of 5.5 s/fruit. However, the harvester's high cycle-time, and high fruit loss rate at 23.4%, prevent it from being commercially viable. This paper presents two new developments for the harvesting system, a new vision system and two new gripper variations designed to overcome the harvester's previous limitations. Furthermore, we have designed and conducted the largest real-world evaluation of a robotic fruit harvesting system published to date with over 12,000 kiwifruit involved. The results of this trial demonstrated that our kiwifruit harvester is one of the most effective selective fruit harvesters in the world capable of successfully harvesting 86.0% of reachable fruit, and 55.8% of all kiwifruit with a cycle-time of 2.78 s/fruit.",
          "Keywords Plus": "FIELD-EVALUATION; FRUIT DETECTION; SYSTEMS; DESIGN"
        }
      },
      {
        "title": "Indoor Coverage Path Planning: Survey, Implementation, Analysis",
        "year": "2018",
        "algorithm": "",
        "performance": "",
        "citation": "@inproceedings{bormann2018indoor,\n  title={Indoor coverage path planning: Survey, implementation, analysis},\n  author={Bormann, Richard and Jordan, Florian and Hampp, Joshua and H{\\\"a}gele, Martin},\n  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},\n  pages={1718--1725},\n  year={2018},\n  organization={IEEE}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{bormann2018indoor}",
          "Article Title": "Indoor Coverage Path Planning: Survey, Implementation, Analysis",
          "Times Cited, All Databases": "50",
          "Publication Year": "2018",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@inproceedings{bormann2018indoor,\n  title={Indoor coverage path planning: Survey, implementation, analysis},\n  author={Bormann, Richard and Jordan, Florian and Hampp, Joshua and H{\\\"a}gele, Martin},\n  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},\n  pages={1718--1725},\n  year={2018},\n  organization={IEEE}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Bormann, R; Jordan, F; Hampp, J; H?gele, M",
          "Abstract": "Coverage Path Planning (CPP) describes the process of generating robot trajectories that fully cover an area or volume. Applications are, amongst many others, mobile cleaning robots, lawn mowing robots or harvesting machines in agriculture. Many approaches and facets of this problem have been discussed in literature but despite the availability of several surveys on the topic there is little work on quantitative assessment and comparison of different coverage path planning algorithms. This paper analyzes six popular off-line coverage path planning methods, applicable to previously recorded maps, in the setting of indoor coverage path planning on room-sized units. The implemented algorithms are thoroughly compared on a large dataset of over 550 rooms with and without furniture.",
          "Keywords Plus": "DECOMPOSITIONS; ALGORITHMS"
        }
      },
      {
        "title": "Sweet Pepper Pose Detection and Grasping for Automated Crop Harvesting",
        "year": "2016",
        "algorithm": "",
        "performance": "",
        "citation": "@inproceedings{lehnert2016sweet,\n  title={Sweet pepper pose detection and grasping for automated crop harvesting},\n  author={Lehnert, Christopher and Sa, Inkyu and McCool, Christopher and Upcroft, Ben and Perez, Tristan},\n  booktitle={2016 IEEE international conference on robotics and automation (ICRA)},\n  pages={2428--2434},\n  year={2016},\n  organization={IEEE}\n}\n",
        "fruit_veg": "sweet pepper",
        "paper": {
          "relevant": "y",
          "": "\\cite{lehnert2016sweet}",
          "Article Title": "Sweet Pepper Pose Detection and Grasping for Automated Crop Harvesting",
          "Times Cited, All Databases": "51",
          "Publication Year": "2016",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@inproceedings{lehnert2016sweet,\n  title={Sweet pepper pose detection and grasping for automated crop harvesting},\n  author={Lehnert, Christopher and Sa, Inkyu and McCool, Christopher and Upcroft, Ben and Perez, Tristan},\n  booktitle={2016 IEEE international conference on robotics and automation (ICRA)},\n  pages={2428--2434},\n  year={2016},\n  organization={IEEE}\n}\n",
          "Document Type": "Proceedings Paper",
          "Main Contribution": "pose detection and grasp",
          "fruit/veg": "sweet pepper",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Lehnert, C; Sa, I; McCool, C; Uperoft, B; Perez, T",
          "Abstract": "This paper presents a method for estimating the 6DOF pose of sweet-pepper (capsicum) crops for autonomous harvesting via a robotic manipulator. The method uses the Kinect Fusion algorithm to robustly fuse RGB-D data from an eye-in-hand camera combined with a colour segmentation and clustering step to extract an accurate representation of the crop. The 6DOF pose of the sweet peppers is then estimated via a nonlinear least squares optimisation by fitting a superellipsoid to the segmented sweet pepper. The performance of the method is demonstrated on a real 6DOF manipulator with a custom gripper. The method is shown to estimate the 6DOF pose successfully enabling the manipulator to grasp sweet peppers for a range of different orientations. The results obtained improve largely on the performance of grasping when compared to a naive approach, which does not estimate the orientation of the crop.",
          "Keywords Plus": "CLOSED-FORM SOLUTION"
        }
      },
      {
        "title": "Optimised computer vision system for automatic pre-grading of citrus fruit in the field using a mobile platform",
        "year": "2014",
        "algorithm": "",
        "performance": "",
        "citation": "@article{cubero2014optimised,\n  title={Optimised computer vision system for automatic pre-grading of citrus fruit in the field using a mobile platform},\n  author={Cubero, Sergio and Aleixos, Nuria and Albert, Francisco and Torregrosa, Antonio and Ortiz, Coral and Garc{\\'\\i}a-Navarrete, O and Blasco, Jos{\\'e}},\n  journal={Precision agriculture},\n  volume={15},\n  pages={80--94},\n  year={2014},\n  publisher={Springer}\n}\n",
        "fruit_veg": "citrus",
        "paper": {
          "relevant": "y",
          "": "\\cite{cubero2014optimised}",
          "Article Title": "Optimised computer vision system for automatic pre-grading of citrus fruit in the field using a mobile platform",
          "Times Cited, All Databases": "52",
          "Publication Year": "2014",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{cubero2014optimised,\n  title={Optimised computer vision system for automatic pre-grading of citrus fruit in the field using a mobile platform},\n  author={Cubero, Sergio and Aleixos, Nuria and Albert, Francisco and Torregrosa, Antonio and Ortiz, Coral and Garc{\\'\\i}a-Navarrete, O and Blasco, Jos{\\'e}},\n  journal={Precision agriculture},\n  volume={15},\n  pages={80--94},\n  year={2014},\n  publisher={Springer}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "vision",
          "fruit/veg": "citrus",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Cubero, S; Aleixos, N; Albert, F; Torregrosa, A; Ortiz, C; Garc¨ªa-Navarrete, O; Blasco, J",
          "Abstract": "The mechanisation and automation of citrus harvesting is considered to be one of the best options to reduce production costs. Computer vision technology has been shown to be a useful tool for fresh fruit and vegetable inspection, and is currently used in post-harvest fruit and vegetable automated grading systems in packing houses. Although computer vision technology has been used in some harvesting robots, it is not commonly utilised in fruit grading during harvesting due to the difficulties involved in adapting it to field conditions. Carrying out fruit inspection before arrival at the packing lines could offer many advantages, such as having an accurate fruit assessment in order to decide among different fruit treatments or savings in the cost of transport and marketing non-commercial fruit. This work presents a computer vision system, mounted on a mobile platform where workers place the harvested fruits, that was specially designed for sorting fruit in the field. Due to the specific field conditions, an efficient and robust lighting system, very low-power image acquisition and processing hardware, and a reduced inspection chamber had to be developed. The equipment is capable of analysing fruit colour and size at a speed of eight fruits per second. The algorithms developed achieved prediction accuracy with an R-2 coefficient of 0.993 for size estimation and an R-2 coefficient of 0.918 for the colour index.",
          "Keywords Plus": "MACHINE; RECOGNITION"
        }
      },
      {
        "title": "An Extensive Review of Mobile Agricultural Robotics for Field Operations: Focus on Cotton Harvesting",
        "year": "2020",
        "algorithm": "",
        "performance": "",
        "citation": "@article{fue2020extensive,\n  title={An extensive review of mobile agricultural robotics for field operations: focus on cotton harvesting},\n  author={Fue, Kadeghe G and Porter, Wesley M and Barnes, Edward M and Rains, Glen C},\n  journal={AgriEngineering},\n  volume={2},\n  number={1},\n  year={2020},\n  publisher={MDPI}\n}",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{fue2020extensive}",
          "Article Title": "An Extensive Review of Mobile Agricultural Robotics for Field Operations: Focus on Cotton Harvesting",
          "Times Cited, All Databases": "50",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "AgriEngineering",
          "Citation": "@article{fue2020extensive,\n  title={An extensive review of mobile agricultural robotics for field operations: focus on cotton harvesting},\n  author={Fue, Kadeghe G and Porter, Wesley M and Barnes, Edward M and Rains, Glen C},\n  journal={AgriEngineering},\n  volume={2},\n  number={1},\n  year={2020},\n  publisher={MDPI}\n}",
          "Document Type": "Review",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Fue, KG; Porter, WM; Barnes, EM; Rains, GC",
          "Abstract": "In this review, we examine opportunities and challenges for 21st-century robotic agricultural cotton harvesting research and commercial development. The paper reviews opportunities present in the agricultural robotics industry, and a detailed analysis is conducted for the cotton harvesting robot industry. The review is divided into four sections: (1) general agricultural robotic operations, where we check the current robotic technologies in agriculture; (2) opportunities and advances in related robotic harvesting fields, which is focused on investigating robotic harvesting technologies; (3) status and progress in cotton harvesting robot research, which concentrates on the current research and technology development in cotton harvesting robots; and (4) challenges in commercial deployment of agricultural robots, where challenges to commercializing and using these robots are reviewed. Conclusions are drawn about cotton harvesting robot research and the potential of multipurpose robotic operations in general. The development of multipurpose robots that can do multiple operations on different crops to increase the value of the robots is discussed. In each of the sections except the conclusion, the analysis is divided into four robotic system categories; mobility and steering, sensing and localization, path planning, and robotic manipulation.",
          "Keywords Plus": "GREEN CITRUS-FRUIT; MACHINE VISION; NEURAL-NETWORK; IN-FIELD; DESIGN; SYSTEMS; ROW; INSPECTION; ALGORITHM; GRIPPERS"
        }
      },
      {
        "title": "A method of segmenting apples at night based on color and position information",
        "year": "2016",
        "algorithm": "",
        "performance": "",
        "citation": "@article{liu2016method,\n  title={A method of segmenting apples at night based on color and position information},\n  author={Liu, Xiaoyang and Zhao, Dean and Jia, Weikuan and Ruan, Chengzhi and Tang, Shuping and Shen, Tian},\n  journal={Computers and Electronics in Agriculture},\n  volume={122},\n  pages={118--123},\n  year={2016},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "apples",
        "paper": {
          "relevant": "y",
          "": "\\cite{liu2016method}",
          "Article Title": "A method of segmenting apples at night based on color and position information",
          "Times Cited, All Databases": "51",
          "Publication Year": "2016",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{liu2016method,\n  title={A method of segmenting apples at night based on color and position information},\n  author={Liu, Xiaoyang and Zhao, Dean and Jia, Weikuan and Ruan, Chengzhi and Tang, Shuping and Shen, Tian},\n  journal={Computers and Electronics in Agriculture},\n  volume={122},\n  pages={118--123},\n  year={2016},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "detection and segmentation",
          "fruit/veg": "apples",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Liu, XY; Zhao, D; Jia, WK; Ruan, CZ; Tang, SP; Shen, T",
          "Abstract": "This paper proposes a method to segment apples on trees at night for apple-harvesting robots based on color and position of pixels. Images of apples acquired under artificial light with low illumination at night include less color information than daytime images, so it is necessary to take position of pixels into consideration. The new method has two main steps. Firstly, color components of sampled pixels in RGB and HSI color space are used to train a neural network model to segment the apples. However, the segmentation results are incomplete and not able to guide apple-harvesting robots accurately, because partial edge regions of apples are dark in shadows and difficult to be recognized due to uneven illumination. Secondly, the color and position of pixels around segmented regions and pixels on the boundary of segmented regions are taken into consideration to segment the edge regions of apples. The union of two segmentation results is the final result. The complete recognition can increase the accuracy of location by about 6.5%, which verified the validity and feasibility of the method. (C) 2016 Elsevier B.V. All rights reserved.",
          "Keywords Plus": "IMAGES; FRUITS"
        }
      },
      {
        "title": "Internet-of-Things (IoT)-based smart agriculture: Toward making the fields talk",
        "year": "2019",
        "algorithm": "",
        "performance": "",
        "citation": "@article{ayaz2019internet,\n  title={Internet-of-Things (IoT)-based smart agriculture: Toward making the fields talk},\n  author={Ayaz, Muhammad and Ammad-Uddin, Mohammad and Sharif, Zubair and Mansour, Ali and Aggoune, El-Hadi M},\n  journal={IEEE access},\n  volume={7},\n  pages={129551--129583},\n  year={2019},\n  publisher={IEEE}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{ayaz2019internet}",
          "Article Title": "Internet-of-Things (IoT)-based smart agriculture: Toward making the fields talk",
          "Times Cited, All Databases": "626",
          "Publication Year": "2019",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{ayaz2019internet,\n  title={Internet-of-Things (IoT)-based smart agriculture: Toward making the fields talk},\n  author={Ayaz, Muhammad and Ammad-Uddin, Mohammad and Sharif, Zubair and Mansour, Ali and Aggoune, El-Hadi M},\n  journal={IEEE access},\n  volume={7},\n  pages={129551--129583},\n  year={2019},\n  publisher={IEEE}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "IoT",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "Despite the perception people may have regarding the agricultural process, the reality is that today's agriculture industry is data-centered, precise, and smarter than ever. The rapid emergence of the Internet-of-Things (IoT) based technologies redesigned almost every industry including 'smart agriculture' which moved the industry from statistical to quantitative approaches. Such revolutionary changes are shaking the existing agriculture methods and creating new opportunities along a range of challenges. This article highlights the potential of wireless sensors and IoT in agriculture, as well as the challenges expected to be faced when integrating this technology with the traditional farming practices. IoT devices and communication techniques associated with wireless sensors encountered in agriculture applications are analyzed in detail. What sensors are available for specific agriculture application, like soil preparation, crop status, irrigation, insect and pest detection are listed. How this technology helping the growers throughout the crop stages, from sowing until harvesting, packing and transportation is explained. Furthermore, the use of unmanned aerial vehicles for crop surveillance and other favorable applications such as optimizing crop yield is considered in this article. State-of-the-art IoT-based architectures and platforms used in agriculture are also highlighted wherever suitable. Finally, based on this thorough review, we identify current and future trends of IoT in agriculture and highlight potential research challenges. ? 2013 IEEE.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Automation in Agriculture by Machine and Deep Learning Techniques: A Review of Recent Developments",
        "year": "2021",
        "algorithm": "",
        "performance": "",
        "citation": "@article{saleem2021automation,\n  title={Automation in agriculture by machine and deep learning techniques: A review of recent developments},\n  author={Saleem, Muhammad Hammad and Potgieter, Johan and Arif, Khalid Mahmood},\n  journal={Precision Agriculture},\n  volume={22},\n  number={6},\n  pages={2053--2091},\n  year={2021},\n  publisher={Springer}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{saleem2021automation}",
          "Article Title": "Automation in Agriculture by Machine and Deep Learning Techniques: A Review of Recent Developments",
          "Times Cited, All Databases": "160",
          "Publication Year": "2021",
          "Highly Cited Status": "",
          "Publisher": "Precision Agriculture",
          "Citation": "@article{saleem2021automation,\n  title={Automation in agriculture by machine and deep learning techniques: A review of recent developments},\n  author={Saleem, Muhammad Hammad and Potgieter, Johan and Arif, Khalid Mahmood},\n  journal={Precision Agriculture},\n  volume={22},\n  number={6},\n  pages={2053--2091},\n  year={2021},\n  publisher={Springer}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "Recently, agriculture has gained much attention regarding automation by artificial intelligence techniques and robotic systems. Particularly, with the advancements in machine learning (ML) concepts, significant improvements have been observed in agricultural tasks. The ability of automatic feature extraction creates an adaptive nature in deep learning (DL), specifically convolutional neural networks to achieve human-level accuracy in various agricultural applications, prominent among which are plant disease detection and classification, weed/crop discrimination, fruit counting, land cover classification, and crop/plant recognition. This review presents the performance of recent uses in agricultural robots by the implementation of ML and DL algorithms/architectures during the last decade. Performance plots are drawn to study the effectiveness of deep learning over traditional machine learning models for certain agricultural operations. The analysis of prominent studies highlighted that the DL-based models, like RCNN (Region-based Convolutional Neural Network), achieve a higher plant disease/pest detection rate (82.51%) than the well-known ML algorithms, including Multi-Layer Perceptron (64.9%) and K-nearest Neighbour (63.76%). The famous DL architecture named ResNet-18 attained more accurate Area Under the Curve (94.84%), and outperformed ML-based techniques, including Random Forest (RF) (70.16%) and Support Vector Machine (SVM) (60.6%), for crop/weed discrimination. Another DL model called FCN (Fully Convolutional Networks) recorded higher accuracy (83.9%) than SVM (67.6%) and RF (65.6%) algorithms for the classification of agricultural land covers. Finally, some important research gaps from the previous studies and innovative future directions are also noted to help propel automation in agriculture up to the next level. ? 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Cleaning the River Ganga: Impact of lockdown on water quality and future implications on river rejuvenation strategies",
        "year": "2020",
        "algorithm": "",
        "performance": "",
        "citation": "@article{dutta2020cleaning,\n  title={Cleaning the River Ganga: Impact of lockdown on water quality and future implications on river rejuvenation strategies},\n  author={Dutta, Venkatesh and Dubey, Divya and Kumar, Saroj},\n  journal={Science of the Total Environment},\n  volume={743},\n  pages={140756},\n  year={2020},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{dutta2020cleaning}",
          "Article Title": "Cleaning the River Ganga: Impact of lockdown on water quality and future implications on river rejuvenation strategies",
          "Times Cited, All Databases": "114",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{dutta2020cleaning,\n  title={Cleaning the River Ganga: Impact of lockdown on water quality and future implications on river rejuvenation strategies},\n  author={Dutta, Venkatesh and Dubey, Divya and Kumar, Saroj},\n  journal={Science of the Total Environment},\n  volume={743},\n  pages={140756},\n  year={2020},\n  publisher={Elsevier}\n}\n",
          "Document Type": "",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "Clean rivers and healthy aquatic life symbolize that the ecosystem is functioning well. The Ganga River has shown signs of rejuvenation and a significant improvement on many parameters, following the eight-week nationwide lockdown due to coronavirus pandemic. Since industrial units and commercial establishments were closed, water was not being lifted by them with a negligible discharge of industrial wastewater. It was observed that during the lockdown period most of the districts falling under the Ganga basin observed 60% excess rainfall than the normal, which led to increased discharge in the river, further contributing towards the dilution of pollutants. Further, data analysis of live storages in the Ganga Basin revealed that the storage during the beginning of the third phase of lockdown was almost double than the storage during the same period the previous year. Analysis of the storage data of the last ten years revealed that the storage till May 6, 2020 was 82.83% more than the average of the previous ten years, which meant that more water was available for the river during the lockdown period. The impact could be seen in terms of increased dissolved oxygen (DO) and reduced biological oxygen demand (BOD), Faecal coliform, Total coliform and nitrate (NO3-) concentration. A declining trend in nitrate concentration was observed in most of the locations due to limited industrial activities and reduction in agricultural run-off due to harvesting season. The gradual transformation in the quality of the water has given a sign of optimism from the point of restoration. Yet, it is believed that this improvement in water quality is ¡®short-lived¡¯ and quality would deteriorate once the normal industrial activities are resumed, indicating a strong influence of untreated commercial¨Cindustrial wastewater. The paper concludes that the river can be rejuvenated if issues of wastewater and adequate flow releases are addressed. ? 2020 Elsevier B.V.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Fast implementation of real-time fruit detection in apple orchards using deep learning",
        "year": "2020",
        "algorithm": "deep learning",
        "performance": "",
        "citation": "@article{kang2020fast,\n  title={Fast implementation of real-time fruit detection in apple orchards using deep learning},\n  author={Kang, Hanwen and Chen, Chao},\n  journal={Computers and Electronics in Agriculture},\n  volume={168},\n  pages={105108},\n  year={2020},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "apples",
        "paper": {
          "relevant": "y",
          "": "\\cite{kang2020fast}\n\n",
          "Article Title": "Fast implementation of real-time fruit detection in apple orchards using deep learning",
          "Times Cited, All Databases": "175",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{kang2020fast,\n  title={Fast implementation of real-time fruit detection in apple orchards using deep learning},\n  author={Kang, Hanwen and Chen, Chao},\n  journal={Computers and Electronics in Agriculture},\n  volume={168},\n  pages={105108},\n  year={2020},\n  publisher={Elsevier}\n}\n",
          "Document Type": "",
          "Main Contribution": "",
          "fruit/veg": "apples",
          "Data Modality": "",
          "Learning Algorithm": "deep learning",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "To perform robust and efficient fruit detection in orchards is challenging since there are a number of variances in the working environments. Recently, deep-learning have shown a promising performance in many visual-guided agriculture applications. However, deep-learning based approaches requires labelling on training data, which is a labour-intensive and time-consuming task. In this study, a fast implementation framework of a deep-learning based fruit detector for apple harvesting is developed. The developed framework comprises an auto label generation module and a deep-learning-based fruit detector ¡®LedNet¡¯. The Label Generation algorithm utilises the multi-scale pyramid and clustering classifier to assist fast labelling of training data. LedNet adopts feature pyramid network and atrous spatial pyramid pooling to improve the detection performance of the model. A light-weight backbone is also developed and utilised to improve computational efficiency. From the experimental results, LedNet achieves 0.821 and 0.853 on recall and accuracy on apple detection in orchards, and its weights size and inference time are 7.4 M and 28 ms, respectively. The experimental results show that LedNet can perform real-time apple detection in orchards robustly and efficiently. ? 2019 Elsevier B.V.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Remote sensing in agriculture¡ªaccomplishments, limitations, and opportunities",
        "year": "2020",
        "algorithm": "",
        "performance": "",
        "citation": "@article{khanal2020remote,\n  title={Remote sensing in agriculture¡ªaccomplishments, limitations, and opportunities},\n  author={Khanal, Sami and Kc, Kushal and Fulton, John P and Shearer, Scott and Ozkan, Erdal},\n  journal={Remote Sensing},\n  volume={12},\n  number={22},\n  pages={3783},\n  year={2020},\n  publisher={MDPI}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{khanal2020remote}",
          "Article Title": "Remote sensing in agriculture¡ªaccomplishments, limitations, and opportunities",
          "Times Cited, All Databases": "144",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{khanal2020remote,\n  title={Remote sensing in agriculture¡ªaccomplishments, limitations, and opportunities},\n  author={Khanal, Sami and Kc, Kushal and Fulton, John P and Shearer, Scott and Ozkan, Erdal},\n  journal={Remote Sensing},\n  volume={12},\n  number={22},\n  pages={3783},\n  year={2020},\n  publisher={MDPI}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "Remote sensing (RS) technologies provide a diagnostic tool that can serve as an early warning system, allowing the agricultural community to intervene early on to counter potential problems before they spread widely and negatively impact crop productivity. With the recent advancements in sensor technologies, data management and data analytics, currently, several RS options are available to the agricultural community. However, the agricultural sector is yet to implement RS technologies fully due to knowledge gaps on their sufficiency, appropriateness and techno-economic feasibilities. This study reviewed the literature between 2000 to 2019 that focused on the application of RS technologies in production agriculture, ranging from field preparation, planting, and in-season applications to harvesting, with the objective of contributing to the scientific understanding on the potential for RS technologies to support decision-making within different production stages. We found an increasing trend in the use of RS technologies in agricultural production over the past 20 years, with a sharp increase in applications of unmanned aerial systems (UASs) after 2015. The largest number of scientific papers related to UASs originated from Europe (34%), followed by the United States (20%) and China (11%). Most of the prior RS studies have focused on soil moisture and in-season crop health monitoring, and less in areas such as soil compaction, subsurface drainage, and crop grain quality monitoring. In summary, the literature highlighted that RS technologies can be used to support site-specific management decisions at various stages of crop production, helping to optimize crop production while addressing environmental quality, profitability, and sustainability. ? 2020 by the authors. Licensee MDPI, Basel, Switzerland.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Internet of Things for the Future of Smart Agriculture: A Comprehensive Survey of Emerging Technologies",
        "year": "2021",
        "algorithm": "",
        "performance": "",
        "citation": "@article{friha2021internet,\n  title={Internet of things for the future of smart agriculture: A comprehensive survey of emerging technologies},\n  author={Friha, Othmane and Ferrag, Mohamed Amine and Shu, Lei and Maglaras, Leandros and Wang, Xiaochan},\n  journal={IEEE/CAA Journal of Automatica Sinica},\n  volume={8},\n  number={4},\n  pages={718--752},\n  year={2021},\n  publisher={IEEE/CAA Journal of Automatica Sinica}\n}",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{friha2021internet}",
          "Article Title": "Internet of Things for the Future of Smart Agriculture: A Comprehensive Survey of Emerging Technologies",
          "Times Cited, All Databases": "282",
          "Publication Year": "2021",
          "Highly Cited Status": "",
          "Publisher": "IEEE/CAA Journal of Automatica Sinica",
          "Citation": "@article{friha2021internet,\n  title={Internet of things for the future of smart agriculture: A comprehensive survey of emerging technologies},\n  author={Friha, Othmane and Ferrag, Mohamed Amine and Shu, Lei and Maglaras, Leandros and Wang, Xiaochan},\n  journal={IEEE/CAA Journal of Automatica Sinica},\n  volume={8},\n  number={4},\n  pages={718--752},\n  year={2021},\n  publisher={IEEE/CAA Journal of Automatica Sinica}\n}",
          "Document Type": "Review",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "This paper presents a comprehensive review of emerging technologies for the internet of things (IoT)-based smart agriculture. We begin by summarizing the existing surveys and describing emergent technologies for the agricultural IoT, such as unmanned aerial vehicles, wireless technologies, open-source IoT platforms, software defined networking (SDN), network function virtualization (NFV) technologies, cloud/fog computing, and middleware platforms. We also provide a classification of IoT applications for smart agriculture into seven categories: including smart monitoring, smart water management, agrochemicals applications, disease management, smart harvesting, supply chain management, and smart agricultural practices. Moreover, we provide a taxonomy and a side-by-side comparison of the state-of-the-art methods toward supply chain management based on the blockchain technology for agricultural IoTs. Furthermore, we present real projects that use most of the aforementioned technologies, which demonstrate their great performance in the field of smart agriculture. Finally, we highlight open research challenges and discuss possible future research directions for agricultural IoTs.  ? 2014 Chinese Association of Automation.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "State-of-the-art robotic grippers, grasping and control strategies, as well as their applications in agricultural robots: A review",
        "year": "2020",
        "algorithm": "",
        "performance": "",
        "citation": "@article{zhang2020state,\n  title={State-of-the-art robotic grippers, grasping and control strategies, as well as their applications in agricultural robots: A review},\n  author={Zhang, Baohua and Xie, Yuanxin and Zhou, Jun and Wang, Kai and Zhang, Zhen},\n  journal={Computers and Electronics in Agriculture},\n  volume={177},\n  pages={105694},\n  year={2020},\n  publisher={Elsevier}\n}",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{zhang2020state}",
          "Article Title": "State-of-the-art robotic grippers, grasping and control strategies, as well as their applications in agricultural robots: A review",
          "Times Cited, All Databases": "228",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "Computers and Electronics in Agriculture",
          "Citation": "@article{zhang2020state,\n  title={State-of-the-art robotic grippers, grasping and control strategies, as well as their applications in agricultural robots: A review},\n  author={Zhang, Baohua and Xie, Yuanxin and Zhou, Jun and Wang, Kai and Zhang, Zhen},\n  journal={Computers and Electronics in Agriculture},\n  volume={177},\n  pages={105694},\n  year={2020},\n  publisher={Elsevier}\n}",
          "Document Type": "Review",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "Grasping, carrying and placing of objects are the fundamental capabilities and common operations for robots and robotic manipulators. Grippers are the most essential components of robots and play an important role in many manipulation tasks, since they serve as the end-of-arm tools, as well as the mechanical interface between robots and environments/grasped objects. Gripper developments are motivated by the great number of different requirements, diverse workpieces and the desire for well adapted and reliable systems. Grippers provide temporary contact with the grasped objects in manipulations. Secure grasping not only requires contacting the objects, but also avoiding the risk of potential slip and damage while the objects are picked and placed. To offer secure grasping for objects with a wide variety of shapes, sizes and materials, various sensors and control strategies are also needed. With the developments of technologies, labor shortage caused by the population aging, as well as the requirements of high automation degree, agricultural robots will find their increasing applications in agricultural and food industries. As the end-of-arm tools for the robots, grippers can be seen as the hands of robots, almost all automatic manipulations are conducted directly by robotic grippers. This paper gives a detailed summary about the state-of-the-art robotic grippers, grasping and sensor-based control methods, as well as their applications in robotic agricultural tasks and food industries. Different from workpiece in industrial environment, agricultural products are fragile and damageable. The requirement for grasping agricultural products is higher than that of grasping of industrial workpieces, various sensors are needed to be installed to the grippers to make them less aggressive, and more flexible and controllable. Therefore, particular attention has been paid to the sensors that used in the grippers to improve their sensing and grasping capabilities. The advantages and disadvantages of the grippers are discussed and summarized. Finally, the challenges and potential future trends of grippers in agricultural robots are reported. ? 2020 Elsevier B.V.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Multi-modal deep learning for Fuji apple detection using RGB-D cameras and their radiometric capabilities",
        "year": "2019",
        "algorithm": "",
        "performance": "",
        "citation": "@article{gene2019multi,\n  title={Multi-modal deep learning for Fuji apple detection using RGB-D cameras and their radiometric capabilities},\n  author={Gen{\\'e}-Mola, Jordi and Vilaplana, Ver{\\'o}nica and Rosell-Polo, Joan R and Morros, Josep-Ramon and Ruiz-Hidalgo, Javier and Gregorio, Eduard},\n  journal={Computers and Electronics in Agriculture},\n  volume={162},\n  pages={689--698},\n  year={2019},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{gene2019multi}",
          "Article Title": "Multi-modal deep learning for Fuji apple detection using RGB-D cameras and their radiometric capabilities",
          "Times Cited, All Databases": "119",
          "Publication Year": "2019",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{gene2019multi,\n  title={Multi-modal deep learning for Fuji apple detection using RGB-D cameras and their radiometric capabilities},\n  author={Gen{\\'e}-Mola, Jordi and Vilaplana, Ver{\\'o}nica and Rosell-Polo, Joan R and Morros, Josep-Ramon and Ruiz-Hidalgo, Javier and Gregorio, Eduard},\n  journal={Computers and Electronics in Agriculture},\n  volume={162},\n  pages={689--698},\n  year={2019},\n  publisher={Elsevier}\n}\n",
          "Document Type": "",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "Fruit detection and localization will be essential for future agronomic management of fruit crops, with applications in yield prediction, yield mapping and automated harvesting. RGB-D cameras are promising sensors for fruit detection given that they provide geometrical information with color data. Some of these sensors work on the principle of time-of-flight (ToF) and, besides color and depth, provide the backscatter signal intensity. However, this radiometric capability has not been exploited for fruit detection applications. This work presents the KFuji RGB-DS database, composed of 967 multi-modal images containing a total of 12,839 Fuji apples. Compilation of the database allowed a study of the usefulness of fusing RGB-D and radiometric information obtained with Kinect v2 for fruit detection. To do so, the signal intensity was range corrected to overcome signal attenuation, obtaining an image that was proportional to the reflectance of the scene. A registration between RGB, depth and intensity images was then carried out. The Faster R-CNN model was adapted for use with five-channel input images: color (RGB), depth (D) and range-corrected intensity signal (S). Results show an improvement of 4.46% in F1-score when adding depth and range-corrected intensity channels, obtaining an F1-score of 0.898 and an AP of 94.8% when all channels are used. From our experimental results, it can be concluded that the radiometric capabilities of ToF sensors give valuable information for fruit detection. ? 2019 Elsevier B.V.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Machine Learning Applications for Precision Agriculture: A Comprehensive Review",
        "year": "2021",
        "algorithm": "",
        "performance": "",
        "citation": "@article{sharma2020machine,\n  title={Machine learning applications for precision agriculture: A comprehensive review},\n  author={Sharma, Abhinav and Jain, Arpit and Gupta, Prateek and Chowdary, Vinay},\n  journal={IEEE Access},\n  volume={9},\n  pages={4843--4873},\n  year={2020},\n  publisher={IEEE}\n}",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{sharma2020machine}",
          "Article Title": "Machine Learning Applications for Precision Agriculture: A Comprehensive Review",
          "Times Cited, All Databases": "347",
          "Publication Year": "2021",
          "Highly Cited Status": "",
          "Publisher": "IEEE Access",
          "Citation": "@article{sharma2020machine,\n  title={Machine learning applications for precision agriculture: A comprehensive review},\n  author={Sharma, Abhinav and Jain, Arpit and Gupta, Prateek and Chowdary, Vinay},\n  journal={IEEE Access},\n  volume={9},\n  pages={4843--4873},\n  year={2020},\n  publisher={IEEE}\n}",
          "Document Type": "Review",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "Agriculture plays a vital role in the economic growth of any country. With the increase of population, frequent changes in climatic conditions and limited resources, it becomes a challenging task to fulfil the food requirement of the present population. Precision agriculture also known as smart farming have emerged as an innovative tool to address current challenges in agricultural sustainability. The mechanism that drives this cutting edge technology is machine learning (ML). It gives the machine ability to learn without being explicitly programmed. ML together with IoT (Internet of Things) enabled farm machinery are key components of the next agriculture revolution. In this article, authors present a systematic review of ML applications in the field of agriculture. The areas that are focused are prediction of soil parameters such as organic carbon and moisture content, crop yield prediction, disease and weed detection in crops and species detection. ML with computer vision are reviewed for the classification of a different set of crop images in order to monitor the crop quality and yield assessment. This approach can be integrated for enhanced livestock production by predicting fertility patterns, diagnosing eating disorders, cattle behaviour based on ML models using data collected by collar sensors, etc. Intelligent irrigation which includes drip irrigation and intelligent harvesting techniques are also reviewed that reduces human labour to a great extent. This article demonstrates how knowledge-based agriculture can improve the sustainable productivity and quality of the product. ? 2013 IEEE.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Fruit detection and segmentation for apple harvesting using visual sensor in orchards",
        "year": "2019",
        "algorithm": "",
        "performance": "",
        "citation": "@article{kang2019fruit,\n  title={Fruit detection and segmentation for apple harvesting using visual sensor in orchards},\n  author={Kang, Hanwen and Chen, Chao},\n  journal={Sensors},\n  volume={19},\n  number={20},\n  pages={4599},\n  year={2019},\n  publisher={MDPI}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{kang2019fruit}",
          "Article Title": "Fruit detection and segmentation for apple harvesting using visual sensor in orchards",
          "Times Cited, All Databases": "93",
          "Publication Year": "2019",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{kang2019fruit,\n  title={Fruit detection and segmentation for apple harvesting using visual sensor in orchards},\n  author={Kang, Hanwen and Chen, Chao},\n  journal={Sensors},\n  volume={19},\n  number={20},\n  pages={4599},\n  year={2019},\n  publisher={MDPI}\n}\n",
          "Document Type": "",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "Autonomous harvesting shows a promising prospect in the future development of the agriculture industry, while the vision system is one of the most challenging components in the autonomous harvesting technologies. This work proposes a multi-function network to perform the real-time detection and semantic segmentation of apples and branches in orchard environments by using the visual sensor. The developed detection and segmentation network utilises the atrous spatial pyramid pooling and the gate feature pyramid network to enhance feature extraction ability of the network. To improve the real-time computation performance of the network model, a lightweight backbone network based on the residual network architecture is developed. From the experimental results, the detection and segmentation network with ResNet-101 backbone outperformed on the detection and segmentation tasks, achieving an F1 score of 0.832 on the detection of apples and 87.6% and 77.2% on the semantic segmentation of apples and branches, respectively. The network model with lightweight backbone showed the best computation efficiency in the results. It achieved an F1 score of 0.827 on the detection of apples and 86.5% and 75.7% on the segmentation of apples and branches, respectively. The weights size and computation time of the network model with lightweight backbone were 12.8 M and 32 ms, respectively. The experimental results show that the detection and segmentation network can effectively perform the real-time detection and segmentation of apples and branches in orchards. ? 2019 by the authors. Licensee MDPI, Basel, Switzerland.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Identifying the spatiotemporal changes of annual harvesting areas for three staple crops in China by integrating multi-data sources",
        "year": "2020",
        "algorithm": "",
        "performance": "",
        "citation": "@article{luo2020identifying,\n  title={Identifying the spatiotemporal changes of annual harvesting areas for three staple crops in China by integrating multi-data sources},\n  author={Luo, Yuchuan and Zhang, Zhao and Li, Ziyue and Chen, Yi and Zhang, Liangliang and Cao, Juan and Tao, Fulu},\n  journal={Environmental Research Letters},\n  volume={15},\n  number={7},\n  pages={074003},\n  year={2020},\n  publisher={IOP Publishing}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{luo2020identifying}",
          "Article Title": "Identifying the spatiotemporal changes of annual harvesting areas for three staple crops in China by integrating multi-data sources",
          "Times Cited, All Databases": "85",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{luo2020identifying,\n  title={Identifying the spatiotemporal changes of annual harvesting areas for three staple crops in China by integrating multi-data sources},\n  author={Luo, Yuchuan and Zhang, Zhao and Li, Ziyue and Chen, Yi and Zhang, Liangliang and Cao, Juan and Tao, Fulu},\n  journal={Environmental Research Letters},\n  volume={15},\n  number={7},\n  pages={074003},\n  year={2020},\n  publisher={IOP Publishing}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "Reliable and continuous information on major crop harvesting areas is fundamental to investigate land surface dynamics and make policies affecting agricultural production, land use, and sustainable development. However, there is currently no spatially explicit and time-continuous crop harvesting area information with a high resolution for China. The spatiotemporal patterns of major crop harvesting areas at a national scale have rarely been investigated. In this study, we proposed a new crop phenology-based crop mapping approach to generate a 1 km harvesting area dataset for three staple crops (i.e. rice, wheat, and maize) in China from 2000 to 2015 based on GLASS leaf area index (LAI) products. First, we retrieved key phenological dates of the three staple crops by combining the inflexion- and threshold-based methods. Then, we determined the grids cultivated for a certain crop if its three key phenological dates could be simultaneously identified. Finally, we developed crop classification maps and a dataset of annual harvesting areas (ChinaCropArea1 km), comprehensively considering the characteristics of crop phenology and the references of drylands and paddy fields. Compared with the county-level agricultural statistical data, the crop classification had a high accuracy, with R2 values consistently greater than 0.8. The spatiotemporal patterns of major crop harvesting areas during the period were further analyzed. The results showed that paddy rice harvesting areas had expanded aggressively in northeastern China but decreased in southern China. Maize harvesting areas expanded substantially in major maize cultivation areas across China. Wheat harvesting areas declined overall, although they increased notably in their major production areas. The spatiotemporal patterns could be ascribed to various anthropogenic, biophysical, and social-economic drivers, including urbanization, reduced cropping intensity in southern China, frequent disasters from climate change, and large areas of abandoned farmland in northern and southwestern China. The resultant dataset can be applied for many purposes, including land surface modeling, agro-ecosystem modeling, agricultural production and land use policy-making. ? 2020 The Author(s). Published by IOP Publishing Ltd.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Automated Systems Based on Machine Vision for Inspecting Citrus Fruits from the Field to Postharvest¡ªa Review",
        "year": "2016",
        "algorithm": "",
        "performance": "",
        "citation": "@article{cubero2016automated,\n  title={Automated systems based on machine vision for inspecting citrus fruits from the field to postharvest¡ªa review},\n  author={Cubero, Sergio and Lee, Won Suk and Aleixos, Nuria and Albert, Francisco and Blasco, Jose},\n  journal={Food and Bioprocess Technology},\n  volume={9},\n  pages={1623--1639},\n  year={2016},\n  publisher={Springer}\n}",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{cubero2016automated}",
          "Article Title": "Automated Systems Based on Machine Vision for Inspecting Citrus Fruits from the Field to Postharvest¡ªa Review",
          "Times Cited, All Databases": "93",
          "Publication Year": "2016",
          "Highly Cited Status": "",
          "Publisher": "Food and Bioprocess Technology",
          "Citation": "@article{cubero2016automated,\n  title={Automated systems based on machine vision for inspecting citrus fruits from the field to postharvest¡ªa review},\n  author={Cubero, Sergio and Lee, Won Suk and Aleixos, Nuria and Albert, Francisco and Blasco, Jose},\n  journal={Food and Bioprocess Technology},\n  volume={9},\n  pages={1623--1639},\n  year={2016},\n  publisher={Springer}\n}",
          "Document Type": "Review",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "Computer vision systems are becoming a scientific but also a commercial tool for food quality assessment. In the field, these systems can be used to predict yield, as well as for robotic harvesting or the early detection of potentially dangerous diseases. In postharvest handling, it is mostly used for the automated inspection of the external quality of the fruits and for sorting them into commercial categories at very high speed. More recently, the use of hyperspectral imaging is allowing the detection of not only defects in the skin of the fruits but also their association to certain diseases of particular importance. In the research works that use this technology, wavelengths that play a significant role in detecting some of these dangerous diseases are found, leading to the development of multispectral imaging systems that can be used in industry. This article reviews recent works that use colour and non-standard computer vision systems for the automated inspection of citrus. It explains the different technologies available to acquire the images and their use for the non-destructive inspection of internal and external features of these fruits. Particular attention is paid to inspection for the early detection of some dangerous diseases like citrus canker, black spot, decay or citrus Huanglongbing. ? 2016, Springer Science+Business Media New York.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Internet of Things Empowered Smart Greenhouse Farming",
        "year": "2020",
        "algorithm": "",
        "performance": "",
        "citation": "@article{rayhana2020internet,\n  title={Internet of things empowered smart greenhouse farming},\n  author={Rayhana, Rakiba and Xiao, Gaozhi and Liu, Zheng},\n  journal={IEEE journal of radio frequency identification},\n  volume={4},\n  number={3},\n  pages={195--211},\n  year={2020},\n  publisher={IEEE}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{rayhana2020internet}",
          "Article Title": "Internet of Things Empowered Smart Greenhouse Farming",
          "Times Cited, All Databases": "99",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{rayhana2020internet,\n  title={Internet of things empowered smart greenhouse farming},\n  author={Rayhana, Rakiba and Xiao, Gaozhi and Liu, Zheng},\n  journal={IEEE journal of radio frequency identification},\n  volume={4},\n  number={3},\n  pages={195--211},\n  year={2020},\n  publisher={IEEE}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "The rapid change of climate, population explosion, and reduction of arable lands are calling for new approaches to ensure sustainable agriculture and food supply for the future. Greenhouse agriculture is considered to be a viable alternative and sustainable solution, which can combat the future food crisis by controlling the local environment and growing crops all year round, even in harsh outdoor conditions. However, greenhouse farms persist many challenges for efficient operation and management. The evolving Internet of Things (IoT) technologies, which encompass the smart sensors, devices, network topologies, big data analytics, and intelligent decision is believed to be the solution in addressing the key challenges facing the greenhouse farming, such as greenhouse local climate control, crop growth monitoring, crop harvesting and etc. This paper reviews the current greenhouse cultivation technologies as well as the state-of-the-art of IoT technologies for smart greenhouse farms. The paper also highlights the major challenges that need to be addressed.  ? 2017 IEEE.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Blockchain and smart contract for IoT enabled smart agriculture",
        "year": "2021",
        "algorithm": "",
        "performance": "",
        "citation": "@article{pranto2021blockchain,\n  title={Blockchain and smart contract for IoT enabled smart agriculture},\n  author={Pranto, Tahmid Hasan and Noman, Abdulla All and Mahmud, Atik and Haque, AKM Bahalul},\n  journal={PeerJ Computer Science},\n  volume={7},\n  pages={e407},\n  year={2021},\n  publisher={PeerJ Inc.}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{pranto2021blockchain}",
          "Article Title": "Blockchain and smart contract for IoT enabled smart agriculture",
          "Times Cited, All Databases": "98",
          "Publication Year": "2021",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{pranto2021blockchain,\n  title={Blockchain and smart contract for IoT enabled smart agriculture},\n  author={Pranto, Tahmid Hasan and Noman, Abdulla All and Mahmud, Atik and Haque, AKM Bahalul},\n  journal={PeerJ Computer Science},\n  volume={7},\n  pages={e407},\n  year={2021},\n  publisher={PeerJ Inc.}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "The agricultural sector is still lagging behind from all other sectors in terms of using the newest technologies. For production, the latest machines are being introduced and adopted. However, pre-harvest and post-harvest processing are still done by following traditional methodologies while tracing, storing, and publishing agricultural data. As a result, farmers are not getting deserved payment, consumers are not getting enough information before buying their product, and intermediate person/processors are increasing retail prices. Using blockchain, smart contracts, and IoT devices, we can fully automate the process while establishing absolute trust among all these parties. In this research, we explored the different aspects of using blockchain and smart contracts with the integration of IoT devices in pre-harvesting and post-harvesting segments of agriculture. We proposed a system that uses blockchain as the backbone while IoT devices collect data from the field level, and smart contracts regulate the interaction among all these contributing parties. The system implementation has been shown in diagrams and with proper explanations. Gas costs of every operation have also been attached for a better understanding of the costs. We also analyzed the system in terms of challenges and advantages. The overall impact of this research was to show the immutable, available, transparent, and robustly secure characteristics of blockchain in the field of agriculture while also emphasizing the vigorous mechanism that the collaboration of blockchain, smart contract, and IoT presents. ? 2021 Pranto et al. All Rights Reserved.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Research Progress Analysis of Robotic Harvesting Technologies in Greenhouse",
        "year": "2017",
        "algorithm": "",
        "performance": "",
        "citation": "@article{liu2017research,\n  title={Research progress analysis of robotic harvesting technologies in greenhouse},\n  author={Liu, JZ},\n  journal={Trans. Chin. Soc. Agric. Mach},\n  volume={48},\n  number={12},\n  pages={1--18},\n  year={2017}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{liu2017research}",
          "Article Title": "Research Progress Analysis of Robotic Harvesting Technologies in Greenhouse",
          "Times Cited, All Databases": "77",
          "Publication Year": "2017",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{liu2017research,\n  title={Research progress analysis of robotic harvesting technologies in greenhouse},\n  author={Liu, JZ},\n  journal={Trans. Chin. Soc. Agric. Mach},\n  volume={48},\n  number={12},\n  pages={1--18},\n  year={2017}\n}\n",
          "Document Type": "",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "As harvesting operation is with the most labor cost, however, the most difficult to realize mechanized work in greenhouse crop production, robotic harvesting technology has been developing rapidly in recent decades. Especially, research on robotic harvesting technology for main crops such as tomatoes, strawberries has been conducted continuously and deeply in Japan. Since the beginning of the new century, research in China in this field has become the most active. Development process of research on robotic harvesting technology for different greenhouse crops in Japan, China, the European Union, South Korea and so on were analyzed, and global main research units and specialists, key technical characteristics, level of development and course of evolution in this field were summarized. In conclusion, the flexibility to complex non-structural environment and deep integration of complex robot system were regarded as two keys to stimulate the development of greenhouse picking robot technology. Finally, it was believed that operation environment factorization, robot configuration standardization, multi-robot cooperation and man-robot collaboration would become the development trend in future, thereby to accelerate the development of robotic harvesting technology and its application in greenhouse industry. ? 2017, Chinese Society of Agricultural Machinery. All right reserved.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Apple fruit size estimation using a 3D machine vision system",
        "year": "2018",
        "algorithm": "",
        "performance": "",
        "citation": "@article{gongal2018apple,\n  title={Apple fruit size estimation using a 3D machine vision system},\n  author={Gongal, A and Karkee, M and Amatya, S},\n  journal={Information Processing in Agriculture},\n  volume={5},\n  number={4},\n  pages={498--503},\n  year={2018},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{gongal2018apple}",
          "Article Title": "Apple fruit size estimation using a 3D machine vision system",
          "Times Cited, All Databases": "83",
          "Publication Year": "2018",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{gongal2018apple,\n  title={Apple fruit size estimation using a 3D machine vision system},\n  author={Gongal, A and Karkee, M and Amatya, S},\n  journal={Information Processing in Agriculture},\n  volume={5},\n  number={4},\n  pages={498--503},\n  year={2018},\n  publisher={Elsevier}\n}\n",
          "Document Type": "",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "Estimation of fruit size in tree fruit crops is essential for selective robotic harvesting and crop-load estimation. Machine vision systems for fruit detection and localization have been studied widely for robotic harvesting and crop-load estimation. However, only a few studies have been carried out to estimate fruit size in orchards using machine vision systems. This study was carried out to develop a machine vision system consisting of a color CCD camera and a time-of-flight (TOF) light-based 3D camera for estimating apple size in tree canopies. As a measure of fruit size, the major axis (longest axis) was estimated based on (i) the 3D coordinates of pixels on corresponding apple surfaces, and (ii) the 2D size of individual pixels within apple surfaces. In the 3D coordinates-based method, the distance between pairs of pixels within apple regions were calculated using 3D coordinates, and the maximum distance between all pixel pairs within an apple region was estimated to be the major axis. The accuracy of estimating the major axis using 3D coordinates was 69.1%. In the pixel-size-based method, the physical sizes of pixels were estimated using a calibration model developed based on pixel coordinates and the distance to pixels from the camera. The major axis length was then estimated by summing the size of individual pixels along the major axis of the fruit. The accuracy of size estimation increased to 84.8% when the pixel size-based method was used. The results showed the potential for estimating fruit size in outdoor environments using a 3D machine vision system. ? 2018 China Agricultural University",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Fast Recognition Method for Tomatoes under Complex Environments Based on Improved YOLO; [»ùÓÚ¸Ä½øÐÍYOLOµÄ¸´ÔÓ»·¾³ÏÂ·¬ÇÑ¹ûÊµ¿ìËÙÊ¶±ð·½·¨]",
        "year": "2020",
        "algorithm": "",
        "performance": "",
        "citation": "@article{Áõ·¼2020»ùÓÚ¸Ä½øÐÍ,\n  title={»ùÓÚ¸Ä½øÐÍ YOLO µÄ¸´ÔÓ»·¾³ÏÂ·¬ÇÑ¹ûÊµ¿ìËÙÊ¶±ð·½·¨},\n  author={Áõ·¼ and ÁõÓñÀ¤ and ÁÖÉ­ and ¹ùÎÄÖÒ and Ðì·² and ÕÅ°×},\n  journal={Å©Òµ»úÐµÑ§±¨},\n  volume={51},\n  number={6},\n  pages={229--237},\n  year={2020}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "",
          "Article Title": "Fast Recognition Method for Tomatoes under Complex Environments Based on Improved YOLO; [»ùÓÚ¸Ä½øÐÍYOLOµÄ¸´ÔÓ»·¾³ÏÂ·¬ÇÑ¹ûÊµ¿ìËÙÊ¶±ð·½·¨]",
          "Times Cited, All Databases": "76",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{Áõ·¼2020»ùÓÚ¸Ä½øÐÍ,\n  title={»ùÓÚ¸Ä½øÐÍ YOLO µÄ¸´ÔÓ»·¾³ÏÂ·¬ÇÑ¹ûÊµ¿ìËÙÊ¶±ð·½·¨},\n  author={Áõ·¼ and ÁõÓñÀ¤ and ÁÖÉ­ and ¹ùÎÄÖÒ and Ðì·² and ÕÅ°×},\n  journal={Å©Òµ»úÐµÑ§±¨},\n  volume={51},\n  number={6},\n  pages={229--237},\n  year={2020}\n}\n",
          "Document Type": "",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "In order to implement the fast and accurate recognition of tomatoes for agricultural harvesting robots under greenhouse environments, an improved multi-scale YOLO detection algorithm named IMS-YOLO was presented. A new backbone network structure, which was named darknet-20, with one residual block was designed based on a series of the previous YOLO algorithms, and a multi-scale detection structure was utilized simultaneously for the detection algorithm. Therefore, a new kind of neural network model was formed for the fast recognition of tomatoes under complex environments. Due to some features of the method such as the fewer layers required, the larger amount of information extracted, and by using the multi-scale structure to return both the detection categories and the bounding boxes, the detection speed and accuracy were improved. IMS-YOLO model was tested on our own tomato dataset, and the detection performance of the network before and after the improvement as well as the influence of the variation of the backbone network layers on the feature extraction capacity were analyzed respectively. The test results showed that the proposed method had ideal features with a precision of tomato image detection of 97.13%, an accuracy of 96.36%, a recall rate of 96.03%, an intersection over union (IOU) of 83.32% and a detection time of 7.719 ms. Furthermore, compared with YOLO v2, YOLO v3 and some other neural networks mentioned, IMS-YOLO can meet the requirements of both detection accuracy and speed. At last, the feasibility of the proposed algorithm applying to the robots was verified by the harvesting tests of the ripe tomatoes under the greenhouse environments. ? 2020, Chinese Society of Agricultural Machinery. All right reserved.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Peduncle Detection of Sweet Pepper for Autonomous Crop Harvesting-Combined Color and 3-D Information",
        "year": "2017",
        "algorithm": "",
        "performance": "",
        "citation": "@article{sa2017peduncle,\n  title={Peduncle detection of sweet pepper for autonomous crop harvesting¡ªcombined color and 3-D information},\n  author={Sa, Inkyu and Lehnert, Chris and English, Andrew and McCool, Chris and Dayoub, Feras and Upcroft, Ben and Perez, Tristan},\n  journal={IEEE Robotics and Automation Letters},\n  volume={2},\n  number={2},\n  pages={765--772},\n  year={2017},\n  publisher={IEEE}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{sa2017peduncle}",
          "Article Title": "Peduncle Detection of Sweet Pepper for Autonomous Crop Harvesting-Combined Color and 3-D Information",
          "Times Cited, All Databases": "82",
          "Publication Year": "2017",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{sa2017peduncle,\n  title={Peduncle detection of sweet pepper for autonomous crop harvesting¡ªcombined color and 3-D information},\n  author={Sa, Inkyu and Lehnert, Chris and English, Andrew and McCool, Chris and Dayoub, Feras and Upcroft, Ben and Perez, Tristan},\n  journal={IEEE Robotics and Automation Letters},\n  volume={2},\n  number={2},\n  pages={765--772},\n  year={2017},\n  publisher={IEEE}\n}\n",
          "Document Type": "",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "This letter presents a three-dimensional (3-D) visual detection method for the challenging task of detecting peduncles of sweet peppers (Capsicum annuum) in the field. Cutting the peduncle cleanly is one of the most difficult stages of the harvesting process, where the peduncle is the part of the crop that attaches it to the main stem of the plant. Accurate peduncle detection in 3-D space is, therefore, a vital step in reliable autonomous harvesting of sweet peppers, as this can lead to precise cutting while avoiding damage to the surrounding plant. This letter makes use of both color and geometry information acquired from an RGB-D sensor and utilizes a supervised-learning approach for the peduncle detection task. The performance of the proposed method is demonstrated and evaluated by using qualitative and quantitative results [the area-under-the-curve (AUC) of the detection precision-recall curve]. We are able to achieve an AUC of 0.71 for peduncle detection on field-grown sweet peppers. We release a set of manually annotated 3-D sweet pepper and peduncle images to assist the research community in performing further research on this topic. ? 2017 IEEE.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Design and simulation of an integrated end-effector for picking kiwifruit by robot",
        "year": "2020",
        "algorithm": "",
        "performance": "",
        "citation": "@article{mu2020design,\n  title={Design and simulation of an integrated end-effector for picking kiwifruit by robot},\n  author={Mu, Longtao and Cui, Gongpei and Liu, Yadong and Cui, Yongjie and Fu, Longsheng and Gejima, Yoshinori},\n  journal={Information Processing in Agriculture},\n  volume={7},\n  number={1},\n  pages={58--71},\n  year={2020},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{mu2020design}",
          "Article Title": "Design and simulation of an integrated end-effector for picking kiwifruit by robot",
          "Times Cited, All Databases": "78",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{mu2020design,\n  title={Design and simulation of an integrated end-effector for picking kiwifruit by robot},\n  author={Mu, Longtao and Cui, Gongpei and Liu, Yadong and Cui, Yongjie and Fu, Longsheng and Gejima, Yoshinori},\n  journal={Information Processing in Agriculture},\n  volume={7},\n  number={1},\n  pages={58--71},\n  year={2020},\n  publisher={Elsevier}\n}\n",
          "Document Type": "",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "The harvesting of fresh kiwifruit is a labor-intensive operation that accounts for more than 25% of annual production costs. Mechanized harvesting technologies are thus being developed to reduce labor requirements for harvesting kiwifruit. To improve the efficiency of a harvesting robot for picking kiwifruit, we designed an end-effector, which we report herein along with the results of tests to verify its operation. By using the established method of automated picking discussed in the literature and which is based on the characteristics of kiwifruit, we propose an automated method to pick kiwifruit that consists of separating the fruit from its stem on the tree. This method is experimentally verified by using it to pick clustered kiwifruit in a scaffolding canopy cultivation. In the experiment, the end-effector approaches a fruit from below and then envelops and grabs it with two bionic fingers. The fingers are then bent to separate the fruit from its stem. The grabbing, picking, and unloading processes are integrated, with automated picking and unloading performed using a connecting rod linkage following a trajectory model. The trajectory was analyzed and validated by using a simulation implemented in the software Automatic Dynamic Analysis of Mechanical Systems (ADAMS). In addition, a prototype of an end-effector was constructed, and its bionic fingers were equipped with fiber sensors to detect the best position for grabbing the kiwifruit and pressure sensors to ensure that the damage threshold was respected while picking. Tolerances for size and shape were incorporated by following a trajectory groove from grabbing and picking to unloading. The end-effector separates clustered kiwifruit and automatically grabs individual fruits. It takes on average 4¨C5 s to pick a single fruit, with a successful picking rate of 94.2% in an orchard test featuring 240 samples. This study shows the grabbing¨Cpicking¨Cunloading robotic end-effector has significant potential to facilitate the harvesting of kiwifruit. ? 2019 China Agricultural University",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Image recognition method of multi-cluster kiwifruit in field based on convolutional neural networks",
        "year": "2018",
        "algorithm": "",
        "performance": "",
        "citation": "@article{fu2018image,\n  title={Image recognition method of multi-cluster kiwifruit in field based on convolutional neural networks},\n  author={Fu, Longsheng and Feng, Yali and Elkamil, Tola and Liu, Zhihao and Li, Rui and Cui, Yongjie},\n  journal={Transactions of the Chinese Society of Agricultural Engineering},\n  volume={34},\n  number={2},\n  pages={205--211},\n  year={2018},\n  publisher={Editorial Office of Transactions of the Chinese Society of Agricultural~¡­}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "",
          "Article Title": "Image recognition method of multi-cluster kiwifruit in field based on convolutional neural networks",
          "Times Cited, All Databases": "109",
          "Publication Year": "2018",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{fu2018image,\n  title={Image recognition method of multi-cluster kiwifruit in field based on convolutional neural networks},\n  author={Fu, Longsheng and Feng, Yali and Elkamil, Tola and Liu, Zhihao and Li, Rui and Cui, Yongjie},\n  journal={Transactions of the Chinese Society of Agricultural Engineering},\n  volume={34},\n  number={2},\n  pages={205--211},\n  year={2018},\n  publisher={Editorial Office of Transactions of the Chinese Society of Agricultural~¡­}\n}\n",
          "Document Type": "",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "China is the largest country for cultivating kiwifruit, and Shaanxi Province provides the largest production, which accounts for approximately 70% of the production in China and 33% of the global production. Harvesting kiwifruit in this region relies mainly on manual picking which is labor-intensive. Therefore, the introduction of robotic harvesting is highly desirable and suitable. The fast and effective recognition of kiwifruit in the field under natural scenes is one of the key technologies for robotic harvesting. Recently, the study on kiwifruit recognition has been limited to a single cluster and multi clusters in the field have seldom been considered. In this paper, according to growth characteristics of kiwifruit grown on sturdy support structures, an RGB (red, green, blue) camera was placed around 100 cm underneath the canopy so that kiwifruit clusters could be included in the images. We proposed a kiwifruit image recognition system based on the convolutional neural network (CNN), which has a good robustness avoiding the subjectivity and limitation of the features selection by artificial means. The CNN could be trained end to end, from raw pixels to ultimate categories, and we optimized the critical structure parameters and the training strategy. Ultimately, the network was made up of 1 input layer, 3 convolutional layers, 2 sub-sampling layers, 1 full convolutional layer, and 1 output layer. The CNN architecture was optimized by using batch normalization (BN) method, which normalized the data distribution of the middle layer and the output data, accelerating the training convergence and reducing the training time. Therefore, the BN layers were added after the 1, 3 and 5th convolutional layer (Conv1, Conv3, and Conv5 layer) of the original LeNet network. The size of all convolutional kernels was 5¡Á5, and that of all the sub-sampling layers was 2¡Á2. The feature map numbers of Conv1, Conv3, and Conv5 were 6, 16 and 120, respectively. After manual selection and normalizing, the RGB image of kiwifruit was transferred into a matrix with the size of 32¡Á32 as the input of the network, stochastic gradient descent was used to train our models with mini-batch size of 100 examples, and momentum was set as 0.9. In addition, the CNN took advantages of the part connections, the weight sharing and Max pooling techniques to lower complexity and improve the training performance of the model simultaneously. The network used rectified linear units (ReLU) as activation function, which could greatly accelerate network convergence. The proposed model for training kiwifruit was represented as 32¡Á32-6C-2S-16C-2S-120C-2. Finally, 100 images of kiwifruit in the field (including 5918 fruits) were used to test the model, and the results showed that the recognition ratios of occluded fruit, overlapped fruit, adjacent fruit and separated fruit were 78.97%, 83.11%, 91.01% and 94.78%, respectively. The overall recognition rate of the model reached 89.29%, and it only took 0.27 s in average to recognize a fruit. There was no overlap between the testing samples and the training samples, which indicated that the network had a high generalization performance, and the testing images were captured from 9 a. m. to 5 p. m., which indicated the network had a good robustness to lightness variations. However, some fruits were wrongly detected and undetected, which included the fruits occluded by branches or leaves, overlapped to each other and the ones under extremely strong sunlight. Particularly, 2 or more fruits overlapped were recognized as one fruit, which was the main reason to the success rate not very high. This phenomenon demands a further research. By comparing with the conventional methods, it suggested that the method proposed obtained a higher recognition rate and better speed, and especially it could simultaneously identify multi-cluster kiwifruit in the field, which provided significant support for multi-arm operation of harvesting robotic. It proves that the CNN has a great potential for recognition of fruits in the field. ? 2018, Editorial Department of the Transactions of the Chinese Society of Agricultural Engineering. All right reserved.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "General improved SSD model for picking object recognition of multiple fruits in natural environment; [×ÔÈ»»·¾³ÏÂ¶àÀàË®¹û²ÉÕªÄ¿±êÊ¶±ðµÄÍ¨ÓÃ¸Ä½øSSDÄ£ÐÍ]",
        "year": "2018",
        "algorithm": "",
        "performance": "",
        "citation": "@article{peng2018general,\n  title={General improved SSD model for picking object recognition of multiple fruits in natural environment.},\n  author={Peng, HongXing and Huang, Bo and Shao, YuanYuan and Li, ZeSen and Zhang, ChaoWu and Chen, Yan and Xiong, JunTao and others},\n  journal={Transactions of the Chinese Society of Agricultural Engineering},\n  volume={34},\n  number={16},\n  pages={155--162},\n  year={2018},\n  publisher={Chinese Society of Agricultural Engineering}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{peng2018general}",
          "Article Title": "General improved SSD model for picking object recognition of multiple fruits in natural environment; [×ÔÈ»»·¾³ÏÂ¶àÀàË®¹û²ÉÕªÄ¿±êÊ¶±ðµÄÍ¨ÓÃ¸Ä½øSSDÄ£ÐÍ]",
          "Times Cited, All Databases": "87",
          "Publication Year": "2018",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{peng2018general,\n  title={General improved SSD model for picking object recognition of multiple fruits in natural environment.},\n  author={Peng, HongXing and Huang, Bo and Shao, YuanYuan and Li, ZeSen and Zhang, ChaoWu and Chen, Yan and Xiong, JunTao and others},\n  journal={Transactions of the Chinese Society of Agricultural Engineering},\n  volume={34},\n  number={16},\n  pages={155--162},\n  year={2018},\n  publisher={Chinese Society of Agricultural Engineering}\n}\n",
          "Document Type": "",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "China is the leading country in the world for the production of fruits, and the variety of fruits is very wide. But fruits harvesting heavily depends on manual hand picking, and it's time-consuming, low efficient and labor-intensive. Fruit picking robot can realize the automation of fruit picking operation and solve the problems of shortage of labor force and high cost. Fruit identification with machine vision is the primary task. But in the field environment, fruit images are easily affected by many external environmental factors such as light changes, fruit size difference, complicated background noise, which can reduce the identification accuracy of fruit with traditional fruit recognition algorithm. And without general feature extraction model, traditional fruit recognition algorithm can only focus on one specific fruit. Deep learning algorithm has the advantages of strong non-linear feature expression ability, and good generalization performance, and can avoid the subjectivity and limitation of human selection on feature selection. In order to solve the problems of low recognition rate and weak generalization for fruit recognition in the field environment, with the apple, litchi, navel orange, Huangdi gan as the research object, an improved single shot detector (SSD) deep learning model for fruit detection is put forward in this study. That is to use ResNet-101 model to replace the VGG16 network in the classic SSD detection framework. After the replacement, the framework still uses 6 feature extraction layers to predict the type and location of fruit objects at each layer; then the weight model under the large data sets is transplanted to multi-class fruit detection tasks with the method of transfer learning. The SSD deep learning model is optimized by using SGD (stochastic gradient descent) algorithm. The weight model of the pre-training on the ImageNet data set is used as the initial weight model of the SSD detection framework, and the training time and resources are further reduced by transferring the characteristics of the learning. At the same time, data enhancement method is used to improve the robustness of the algorithm without reducing the detection accuracy. Based on the Caffe deep learning framework, fruit detection results are compared for the multi-class fruit pictures collected in the field environment with different network models, different data set sizes and different occlusion ratios. Experimental results show that after a day of training, the residual error reference model takes about 0.14 s when detecting the image with a resolution of 500¡Á500 pixels, only about 0.09 s slower than the VGG16 network model. And in various environments, the average detection accuracy of the 4 kinds of fruit based on the improved SSD deep learning fruit detection model can reach 88.4%, which is higher than that of the classic SSD deep learning model that is 86.38%. After data are enhanced, the average detection accuracy can be improved by 1.13 percentage points and reach 89.53%, and the F1-score can reach 96.12% when the occlusion area is lower than 50%. Therefore, compared with the traditional recognition algorithm, this method based on improved SSD model can realize multi-class fruit image detection simultaneously without artificial feature selection for different fruit images, and has better generalization and robustness. It can achieve accurate detection of multiple kinds of fruits in the field environment, and provides a new solution for the problem of fruit detection and recognition in agricultural automation. ? 2018, Editorial Department of the Transactions of the Chinese Society of Agricultural Engineering. All right reserved.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Vermicomposting as manure management strategy for urban small-holder animal farms - Kampala case study",
        "year": "2015",
        "algorithm": "",
        "performance": "",
        "citation": "@article{lalander2015vermicomposting,\n  title={Vermicomposting as manure management strategy for urban small-holder animal farms--Kampala case study},\n  author={Lalander, Cecilia Helena and Komakech, Allan John and Vinner{\\aa}s, Bj{\\\"o}rn},\n  journal={Waste management},\n  volume={39},\n  pages={96--103},\n  year={2015},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{lalander2015vermicomposting}",
          "Article Title": "Vermicomposting as manure management strategy for urban small-holder animal farms - Kampala case study",
          "Times Cited, All Databases": "75",
          "Publication Year": "2015",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{lalander2015vermicomposting,\n  title={Vermicomposting as manure management strategy for urban small-holder animal farms--Kampala case study},\n  author={Lalander, Cecilia Helena and Komakech, Allan John and Vinner{\\aa}s, Bj{\\\"o}rn},\n  journal={Waste management},\n  volume={39},\n  pages={96--103},\n  year={2015},\n  publisher={Elsevier}\n}\n",
          "Document Type": "",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "Inadequate organic waste management can contribute to the spread of diseases and have negative impacts on the environment. Vermicomposting organic waste could have dual beneficial effects by generating an economically viable animal feed protein in the form of worm biomass, while alleviating the negative effects of poor organic waste management. In this study, a low-maintenance vermicomposting system was evaluated as manure and food waste management system for small-holder farmers. A vermicomposting system using the earthworm species Eudrilus eugeniae and treating cow manure and food waste was set up in Kampala, Uganda, and monitored for 172. days. The material degradation and protein production rates were evaluated after 63. days and at the end of the experiment. The material reduction was 45.9% and the waste-to-biomass conversion rate was 3.5% in the vermicomposting process on a total solids basis. A possible increase in the conversion rate could be achieved by increasing the frequency of worm harvesting. Vermicomposting was found to be a viable manure management method in small-scale urban animal agriculture; the return of investment was calculated to be 280% for treating the manure of a 450. kg cow. The vermicompost was not sanitised, although hygiene quality could be improved by introducing a post-stabilisation step in which no fresh material is added. The value of the animal feed protein generated in the process can act as an incentive to improve current manure management strategies. ? 2015 The Authors.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "A survey of ranging and imaging techniques for precision agriculture phenotyping",
        "year": "2017",
        "algorithm": "",
        "performance": "",
        "citation": "@article{narvaez2017survey,\n  title={A survey of ranging and imaging techniques for precision agriculture phenotyping},\n  author={Narvaez, Francisco Yandun and Reina, Giulio and Torres-Torriti, Miguel and Kantor, George and Cheein, Fernando Auat},\n  journal={IEEE/ASME Transactions on Mechatronics},\n  volume={22},\n  number={6},\n  pages={2428--2439},\n  year={2017},\n  publisher={IEEE}\n}",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{narvaez2017survey}",
          "Article Title": "A survey of ranging and imaging techniques for precision agriculture phenotyping",
          "Times Cited, All Databases": "98",
          "Publication Year": "2017",
          "Highly Cited Status": "",
          "Publisher": "IEEE/ASME Transactions on Mechatronics",
          "Citation": "@article{narvaez2017survey,\n  title={A survey of ranging and imaging techniques for precision agriculture phenotyping},\n  author={Narvaez, Francisco Yandun and Reina, Giulio and Torres-Torriti, Miguel and Kantor, George and Cheein, Fernando Auat},\n  journal={IEEE/ASME Transactions on Mechatronics},\n  volume={22},\n  number={6},\n  pages={2428--2439},\n  year={2017},\n  publisher={IEEE}\n}",
          "Document Type": "Review",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "Agricultural production must double by 2050 in order to meet the expected food demand due to population growth. Precision agriculture is the key to improve productivity and efficiency in the use of resources, thus helping to achieve this goal under the diverse challenges currently faced by agriculture mainly due to climate changes, land degradation, availability of farmable land, labor force shortage, and increasing costs. To face these challenges, precision agriculture uses and develops sensing methodologies that provide information about crop growth and health indicators. This paper presents a survey of the state-of-the-art in optical visible and near-visible spectrum sensors and techniques to estimate phenotyping variables from intensity, spectral, and volumetric measurements. The sensing methodologies are classified into three areas according to the purpose of the measurements: 1) plant structural characterization; 2) plant/fruit detection; and 3) plant physiology assessment. This paper also discusses the progress in data processing methods and the current open challenges in agricultural tasks in which the development of innovative sensing methodologies is required, such as pruning, fertilizer and pesticide management, crop monitoring, and automated harvesting. ? 1996-2012 IEEE.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "L*a*b*Fruits: A rapid and robust outdoor fruit detection system combining bio-inspired features with one-stage deep learning networks",
        "year": "2020",
        "algorithm": "",
        "performance": "",
        "citation": "@article{kirk2020b,\n  title={L* a* b* fruits: A rapid and robust outdoor fruit detection system combining bio-inspired features with one-stage deep learning networks},\n  author={Kirk, Raymond and Cielniak, Grzegorz and Mangan, Michael},\n  journal={Sensors},\n  volume={20},\n  number={1},\n  pages={275},\n  year={2020},\n  publisher={MDPI}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{kirk2020b}",
          "Article Title": "L*a*b*Fruits: A rapid and robust outdoor fruit detection system combining bio-inspired features with one-stage deep learning networks",
          "Times Cited, All Databases": "53",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{kirk2020b,\n  title={L* a* b* fruits: A rapid and robust outdoor fruit detection system combining bio-inspired features with one-stage deep learning networks},\n  author={Kirk, Raymond and Cielniak, Grzegorz and Mangan, Michael},\n  journal={Sensors},\n  volume={20},\n  number={1},\n  pages={275},\n  year={2020},\n  publisher={MDPI}\n}\n",
          "Document Type": "",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "Automation of agricultural processes requires systems that can accurately detect and classify produce in real industrial environments that include variation in fruit appearance due to illumination, occlusion, seasons, weather conditions, etc. In this paper we combine a visual processing approach inspired by colour-opponent theory in humans with recent advancements in one-stage deep learning networks to accurately, rapidly and robustly detect ripe soft fruits (strawberries) in real industrial settings and using standard (RGB) camera input. The resultant system was tested on an existent data-set captured in controlled conditions as well our new real-world data-set captured on a real strawberry farm over two months. We utilise F1 score, the harmonic mean of precision and recall, to show our system matches the state-of-the-art detection accuracy (F1: 0.793 vs. 0.799) in controlled conditions; has greater generalisation and robustness to variation of spatial parameters (camera viewpoint) in the real-world data-set (F1: 0.744); and at a fraction of the computational cost allowing classification at almost 30fps. We propose that the L*a*b*Fruits system addresses some of the most pressing limitations of current fruit detection systems and is well-suited to application in areas such as yield forecasting and harvesting. Beyond the target application in agriculture this work also provides a proof-of-principle whereby increased performance is achieved through analysis of the domain data, capturing features at the input level rather than simply increasing model complexity. ? 2020 by the authors. Licensee MDPI, Basel, Switzerland.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Ensuring agricultural sustainability through remote sensing in the era of agriculture 5.0",
        "year": "2021",
        "algorithm": "",
        "performance": "",
        "citation": "@article{martos2021ensuring,\n  title={Ensuring agricultural sustainability through remote sensing in the era of agriculture 5.0},\n  author={Martos, Vanesa and Ahmad, Ali and Cartujo, Pedro and Ordo{\\~n}ez, Javier},\n  journal={Applied Sciences},\n  volume={11},\n  number={13},\n  pages={5911},\n  year={2021},\n  publisher={MDPI}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{martos2021ensuring}",
          "Article Title": "Ensuring agricultural sustainability through remote sensing in the era of agriculture 5.0",
          "Times Cited, All Databases": "69",
          "Publication Year": "2021",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{martos2021ensuring,\n  title={Ensuring agricultural sustainability through remote sensing in the era of agriculture 5.0},\n  author={Martos, Vanesa and Ahmad, Ali and Cartujo, Pedro and Ordo{\\~n}ez, Javier},\n  journal={Applied Sciences},\n  volume={11},\n  number={13},\n  pages={5911},\n  year={2021},\n  publisher={MDPI}\n}\n",
          "Document Type": "",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "Timely and reliable information about crop management, production, and yield is consid-ered of great utility by stakeholders (e.g., national and international authorities, farmers, commercial units, etc.) to ensure food safety and security. By 2050, according to Food and Agriculture Organization (FAO) estimates, around 70% more production of agricultural products will be needed to fulfil the demands of the world population. Likewise, to meet the Sustainable Development Goals (SDGs), especially the second goal of ¡°zero hunger¡±, potential technologies like remote sensing (RS) need to be efficiently integrated into agriculture. The application of RS is indispensable today for a highly productive and sustainable agriculture. Therefore, the present study draws a general overview of RS technology with a special focus on the principal platforms of this technology, i.e., satellites and remotely piloted aircrafts (RPAs), and the sensors used, in relation to the 5th industrial revolution. Nevertheless, since 1957, RS technology has found applications, through the use of satellite imagery, in agriculture, which was later enriched by the incorporation of remotely piloted aircrafts (RPAs), which is further pushing the boundaries of proficiency through the upgrading of sensors capable of higher spectral, spatial, and temporal resolutions. More prominently, wireless sensor technologies (WST) have streamlined real time information acquisition and programming for respective measures. Improved algorithms and sensors can, not only add significant value to crop data acquisition, but can also devise simulations on yield, harvesting and irrigation periods, metrological data, etc., by making use of cloud computing. The RS technology generates huge sets of data that necessitate the incorporation of artificial intelligence (AI) and big data to extract useful products, thereby augmenting the adeptness and efficiency of agriculture to ensure its sustainability. These technologies have made the orientation of current research towards the estimation of plant physiological traits rather than the structural parameters possible. Futuristic approaches for benefiting from these cutting-edge technologies are discussed in this study. This study can be helpful for researchers, academics, and young students aspiring to play a role in the achievement of sustainable agriculture. ? 2021 by the authors. Licensee MDPI, Basel, Switzerland.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Phytoextraction of copper from a contaminated soil using arable and vegetable crops",
        "year": "2019",
        "algorithm": "",
        "performance": "",
        "citation": "@article{napoli2019phytoextraction,\n  title={Phytoextraction of copper from a contaminated soil using arable and vegetable crops},\n  author={Napoli, Marco and Cecchi, Stefano and Grassi, Chiara and Baldi, Ada and Zanchi, Camillo A and Orlandini, Simone},\n  journal={Chemosphere},\n  volume={219},\n  pages={122--129},\n  year={2019},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{napoli2019phytoextraction}",
          "Article Title": "Phytoextraction of copper from a contaminated soil using arable and vegetable crops",
          "Times Cited, All Databases": "72",
          "Publication Year": "2019",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{napoli2019phytoextraction,\n  title={Phytoextraction of copper from a contaminated soil using arable and vegetable crops},\n  author={Napoli, Marco and Cecchi, Stefano and Grassi, Chiara and Baldi, Ada and Zanchi, Camillo A and Orlandini, Simone},\n  journal={Chemosphere},\n  volume={219},\n  pages={122--129},\n  year={2019},\n  publisher={Elsevier}\n}\n",
          "Document Type": "",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "Copper (Cu) is among the main contaminant of agricultural soil. The reclamation of Cu polluted soils can be achieved with phytoextraction even if, in general, plants are Cu-excluders and uncommon are Cu-accumulators. The research objectives were to establish the Cu removal capacity by arable and vegetable crops and to investigate the distribution of Cu in their roots, stems and leaves, and fruits. Pot trials were conducted for two subsequent years in Tuscany (Italy). Cu was added into soil in four levels (0, 200, 400, 600 mg kg?1 of Cu). At harvesting, the crops roots, stems and leaves, and fruits or seeds were separately collected, oven dried, weighted, milled and separately analyzed. The results show that the GDUs value to reach the physiological maturity for barley, common bean, Indian mustard, and ricinus was significantly positively correlated with Cu concentration in soil in contrast with observed in sorghum, spinach, and tomato. Leaves and stems of spinach and ricinus have a good storage capacity in contrast with common bean, tomato, Indian mustard sorghum and barley. Tomato storage Cu mainly in fruits and roots which show a remarkable concentration of Cu that increases progressively with the increase of Cu concentration in the soil. In addition, the roots of common bean and ricinus showed a very high concentration of Cu. All species can be considered Cu-excluders because of their low capacity to uptake high quantity of Cu. Indian mustard can be considered a plant able to translocate the metal from root to epigeal tissue. ? 2018 The Authors",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Research advance on vision system of apple picking robot",
        "year": "2017",
        "algorithm": "",
        "performance": "",
        "citation": "@article{wang2017research,\n  title={Research advance on vision system of apple picking robot},\n  author={Wang, Dandan and Song, Huaibo and He, Dongjian},\n  journal={Transactions of the Chinese Society of Agricultural Engineering},\n  volume={33},\n  number={10},\n  pages={59--69},\n  year={2017},\n  publisher={Editorial Office of Transactions of the Chinese Society of Agricultural~¡­}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "",
          "Article Title": "Research advance on vision system of apple picking robot",
          "Times Cited, All Databases": "68",
          "Publication Year": "2017",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{wang2017research,\n  title={Research advance on vision system of apple picking robot},\n  author={Wang, Dandan and Song, Huaibo and He, Dongjian},\n  journal={Transactions of the Chinese Society of Agricultural Engineering},\n  volume={33},\n  number={10},\n  pages={59--69},\n  year={2017},\n  publisher={Editorial Office of Transactions of the Chinese Society of Agricultural~¡­}\n}\n",
          "Document Type": "",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "Vision system is one of the most important parts of apple picking robot, which, to some extent, determines the quality and the speed of picking task implemented by apple picking robot. In this review, we enumerated the existing apple picking robots. Some information, such as type of visual sensors, hardware of vision system, success rate of harvesting and run time, was illustrated in details. Meanwhile, on the basis of discussing the vision system of apple picking robot, we focused on summarizing hardware structure of existing vision systems and apple image segmentation methods as well as apple recognition and localization methods applied in vision systems. The vision system of apple picking robot mainly includes machine vision system, laser vision system, three-dimensional vision system and vision system formed by machine vision and other vision system. And the machine vision system can be classified into 3 types according to the number of image sensors used, that is, monocular vision system, binocular vision system and multi vision system. The recognition and localization of apple target is the first step of the implementation of the picking task for picking robot. The currently used apple segmentation and recognition methods include threshold segmentation algorithm, chromatic aberration based algorithm, K-mean clustering algorithm, region growing algorithm, segmentation method combining 2 or more algorithms, and so on. There are 4 methods that are commonly used in the localization of apple target. They are the methods based on centroid, fitting circle, symmetry axes, and three-dimensional coordinates, respectively. In natural scene, the recognition and localization of apple target may be affected by many factors. Hence, recognition and localization of apple target under different conditions, such as color nonuniformity, different illumination, shadow on the surface, oscillation, overlapping and occlusion, was reviewed and analyzed. Among all these conditions, occlusion can be regarded as the most serious factor. The condition of occlusion can be roughly divided into 4 kinds, i.e. apple target blocked by other apple, by branches, by leaves and by branches, leaves and other apple simultaneously. As for apple targets blocked by branches, one apple may be separated by branches, thus causing that an apple may be recognized as several apples. For the apple targets blocked by leaves, the symmetry of apple can be utilized to localize apple targets. The apple targets blocked by other apple can be considered as overlapping. There are overlapped apples with series connection, parallel connection, and blend connection. Because of the complexity of overlapping, the recognition and localization of apple targets blocked by other apple target is a little more difficult. In addition, the detection of obstacles like tree trunk and branches in apple orchard is important for apple picking robots to avoid obstacles, and thus obstacles detection methods were summarized in this review. In the process of target recognition and localization, binocular vision technology was commonly used in vision system. The key point of binocular vision technology is stereo matching. Therefore, stereo matching was then reviewed, and the image matching methods in existence can generally be divided into 2 categories, i.e. region-based image matching method and feature-based image matching method. What's more, the problems exist in recognition and localization methods used in the vision system of apple picking robot, including accuracy, effectiveness, character of real-time and universal applicability, were analyzed. Further study will concentrate on optimizing the structure of vision system, optimizing the intelligent algorithms used in the vision system, improving the real-time capability, recognizing and locating apple targets when the apple targets and vision systems are influenced by oscillation, and improving cost performance. The paper has summarized and analyzed vision system of apple picking robot comprehensively, which can provide reference for future research. ? 2017, Editorial Department of the Transactions of the Chinese Society of Agricultural Engineering. All right reserved.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Fruit detection in an apple orchard using a mobile terrestrial laser scanner",
        "year": "2019",
        "algorithm": "",
        "performance": "",
        "citation": "@article{gene2019fruit,\n  title={Fruit detection in an apple orchard using a mobile terrestrial laser scanner},\n  author={Gen{\\'e}-Mola, Jordi and Gregorio, Eduard and Guevara, Javier and Auat, Fernando and Sanz-Cortiella, Ricardo and Escol{\\`a}, Alexandre and Llorens, Jordi and Morros, Josep-Ramon and Ruiz-Hidalgo, Javier and Vilaplana, Ver{\\'o}nica and others},\n  journal={Biosystems engineering},\n  volume={187},\n  pages={171--184},\n  year={2019},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{gene2019fruit}",
          "Article Title": "Fruit detection in an apple orchard using a mobile terrestrial laser scanner",
          "Times Cited, All Databases": "71",
          "Publication Year": "2019",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{gene2019fruit,\n  title={Fruit detection in an apple orchard using a mobile terrestrial laser scanner},\n  author={Gen{\\'e}-Mola, Jordi and Gregorio, Eduard and Guevara, Javier and Auat, Fernando and Sanz-Cortiella, Ricardo and Escol{\\`a}, Alexandre and Llorens, Jordi and Morros, Josep-Ramon and Ruiz-Hidalgo, Javier and Vilaplana, Ver{\\'o}nica and others},\n  journal={Biosystems engineering},\n  volume={187},\n  pages={171--184},\n  year={2019},\n  publisher={Elsevier}\n}\n",
          "Document Type": "",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "The development of reliable fruit detection and localization systems provides an opportunity to improve the crop value and management by limiting fruit spoilage and optimised harvesting practices. Most proposed systems for fruit detection are based on RGB cameras and thus are affected by intrinsic constraints, such as variable lighting conditions. This work presents a new technique that uses a mobile terrestrial laser scanner (MTLS) to detect and localise Fuji apples. An experimental test focused on Fuji apple trees (Malus domestica Borkh. cv. Fuji) was carried out. A 3D point cloud of the scene was generated using an MTLS composed of a Velodyne VLP-16 LiDAR sensor synchronised with an RTK-GNSS satellite navigation receiver. A reflectance analysis of tree elements was performed, obtaining mean apparent reflectance values of 28.9%, 29.1%, and 44.3% for leaves, branches and trunks, and apples, respectively. These results suggest that the apparent reflectance parameter (at 905 nm wavelength) can be useful to detect apples. For that purpose, a four-step fruit detection algorithm was developed. By applying this algorithm, a localization success of 87.5%, an identification success of 82.4%, and an F1-score of 0.858 were obtained in relation to the total amount of fruits. These detection rates are similar to those obtained by RGB-based systems, but with the additional advantages of providing direct 3D fruit location information, which is not affected by sunlight variations. From the experimental results, it can be concluded that LiDAR-based technology and, particularly, its reflectance information, has potential for remote apple detection and 3D location. ? 2019 IAgrE",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Comparative classification analysis of post-harvest growth detection from terrestrial LiDAR point clouds in precision agriculture",
        "year": "2015",
        "algorithm": "",
        "performance": "",
        "citation": "@article{koenig2015comparative,\n  title={Comparative classification analysis of post-harvest growth detection from terrestrial LiDAR point clouds in precision agriculture},\n  author={Koenig, Kristina and H{\\\"o}fle, Bernhard and H{\\\"a}mmerle, Martin and Jarmer, Thomas and Siegmann, Bastian and Lilienthal, Holger},\n  journal={ISPRS journal of photogrammetry and remote sensing},\n  volume={104},\n  pages={112--125},\n  year={2015},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{koenig2015comparative}",
          "Article Title": "Comparative classification analysis of post-harvest growth detection from terrestrial LiDAR point clouds in precision agriculture",
          "Times Cited, All Databases": "61",
          "Publication Year": "2015",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{koenig2015comparative,\n  title={Comparative classification analysis of post-harvest growth detection from terrestrial LiDAR point clouds in precision agriculture},\n  author={Koenig, Kristina and H{\\\"o}fle, Bernhard and H{\\\"a}mmerle, Martin and Jarmer, Thomas and Siegmann, Bastian and Lilienthal, Holger},\n  journal={ISPRS journal of photogrammetry and remote sensing},\n  volume={104},\n  pages={112--125},\n  year={2015},\n  publisher={Elsevier}\n}\n",
          "Document Type": "",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "In precision agriculture, detailed geoinformation on plant and soil properties plays an important role, e.g., in crop protection or the application of fertilizers. This paper presents a comparative classification analysis for post-harvest growth detection using geometric and radiometric point cloud features of terrestrial laser scanning (TLS) data, considering the local neighborhood of each point. Radiometric correction of the TLS data was performed via an empirical range-correction function derived from a field experiment. Thereafter, the corrected amplitude and local elevation features were explored regarding their importance for classification. For the comparison, tree induction, Nayi{cyrillic}ve Bayes, and k-Means-derived classifiers were tested for different point densities to distinguish between ground and post-harvest growth. The classification performance was validated against highly detailed RGB reference images and the red edge normalized difference vegetation index (NDVI705), derived from a hyperspectral sensor. Using both geometric and radiometric features, we achieved a precision of 99% with the tree induction. Compared to the reference image classification, the calculated post-harvest growth coverage map reached an accuracy of 80%. RGB and LiDAR-derived coverage showed a polynomial correlation to NDVI705 of degree two with R2 of 0.8 and 0.7, respectively. Larger post-harvest growth patches (>10¡Á10cm) could already be detected by a point density of 2pts./0.01m2. The results indicate a high potential of radiometric and geometric LiDAR point cloud features for the identification of post-harvest growth using tree induction classification. The proposed technique can potentially be applied over larger areas using vehicle-mounted scanners. ? 2015 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS).",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Intact detection of highly occluded immature tomatoes on plants using deep learning techniques",
        "year": "2020",
        "algorithm": "",
        "performance": "",
        "citation": "@article{mu2020intact,\n  title={Intact detection of highly occluded immature tomatoes on plants using deep learning techniques},\n  author={Mu, Yue and Chen, Tai-Shen and Ninomiya, Seishi and Guo, Wei},\n  journal={Sensors},\n  volume={20},\n  number={10},\n  pages={2984},\n  year={2020},\n  publisher={MDPI}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{mu2020intact}",
          "Article Title": "Intact detection of highly occluded immature tomatoes on plants using deep learning techniques",
          "Times Cited, All Databases": "67",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{mu2020intact,\n  title={Intact detection of highly occluded immature tomatoes on plants using deep learning techniques},\n  author={Mu, Yue and Chen, Tai-Shen and Ninomiya, Seishi and Guo, Wei},\n  journal={Sensors},\n  volume={20},\n  number={10},\n  pages={2984},\n  year={2020},\n  publisher={MDPI}\n}\n",
          "Document Type": "",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "Automatic detection of intact tomatoes on plants is highly expected for low-cost and optimal management in tomato farming. Mature tomato detection has been wildly studied, while immature tomato detection, especially when occluded with leaves, is difficult to perform using traditional image analysis, which is more important for long-term yield prediction. Therefore, tomato detection that can generalize well in real tomato cultivation scenes and is robust to issues such as fruit occlusion and variable lighting conditions is highly desired. In this study, we build a tomato detection model to automatically detect intact green tomatoes regardless of occlusions or fruit growth stage using deep learning approaches. The tomato detection model used faster region-based convolutional neural network (R-CNN) with Resnet-101 and transfer learned from the Common Objects in Context (COCO) dataset. The detection on test dataset achieved high average precision of 87.83% (intersection over union ¡Ý 0.5) and showed a high accuracy of tomato counting (R2 = 0.87). In addition, all the detected boxes were merged into one image to compile the tomato location map and estimate their size along one row in the greenhouse. By tomato detection, counting, location and size estimation, this method shows great potential for ripeness and yield prediction. ? 2020 by the authors. Licensee MDPI, Basel, Switzerland.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Integration of RGB-based vegetation index, crop surface model and object-based image analysis approach for sugarcane yield estimation using unmanned aerial vehicle",
        "year": "2021",
        "algorithm": "",
        "performance": "",
        "citation": "@article{sumesh2021integration,\n  title={Integration of RGB-based vegetation index, crop surface model and object-based image analysis approach for sugarcane yield estimation using unmanned aerial vehicle},\n  author={Sumesh, KC and Ninsawat, Sarawut and Som-Ard, Jaturong},\n  journal={Computers and Electronics in Agriculture},\n  volume={180},\n  pages={105903},\n  year={2021},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{sumesh2021integration}",
          "Article Title": "Integration of RGB-based vegetation index, crop surface model and object-based image analysis approach for sugarcane yield estimation using unmanned aerial vehicle",
          "Times Cited, All Databases": "56",
          "Publication Year": "2021",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{sumesh2021integration,\n  title={Integration of RGB-based vegetation index, crop surface model and object-based image analysis approach for sugarcane yield estimation using unmanned aerial vehicle},\n  author={Sumesh, KC and Ninsawat, Sarawut and Som-Ard, Jaturong},\n  journal={Computers and Electronics in Agriculture},\n  volume={180},\n  pages={105903},\n  year={2021},\n  publisher={Elsevier}\n}\n",
          "Document Type": "",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "Estimation of yield is a major challenge in the production of many agricultural crops, including sugarcane. Mapping the spatial variability of plant height (PH) and the stalk density is important for accurate sugarcane yield estimation, and this estimation can aid in the planning of upcoming labor- and cost-intensive actions like harvesting, milling, and forward selling decisions. The objective of this research is to assess the potential of a consumer-grade red-green-blue (RGB) camera mounted on an unmanned aerial vehicle (UAV) for sugarcane yield estimation with minimal field dataset. The study mapped the spatial variability of PH and stalk density at the grid level (4 m ¡Á 4 m) on a farm. The average PH was estimated at the grid level by masking the sugarcane area. An object-based image analysis (OBIA) approach was used to extract the sugarcane area by integrating the plant height model (PHM), extracted by subtracting the digital elevation model (DEM) from the crop surface model (CSM). Both CSM and DEM were generated from UAV images, where CSM was produced approximately one month before the harvest and the DEM after the sugarcane was harvested. The PHM improved the overall accuracy of classification from 61.98% to 87.45%. The UAV estimated PH showed a high correlation (r = 0.95) with ground observed PH, with an average overestimation of 0.10 m. An ordinary least square (OLS) linear regression model was developed to estimate millable stalk height (MSH) from PH, weight from estimated MSH, and stalk density from vegetation indices (VIs) at the grid-level. Excess green (ExG) derived from RGB showed R2 of 0.754 with the stalk density. Likewise, R2 of 0.798 and 0.775 were obtained between MSH and PH, and weight and MSH. Eventually, the yield was estimated by integrating the variability of PH and stalk density and weight information. The estimated yield from ExG (200.66 tons) was close to the actual harvest yield (192.1 tons). The very high-resolution RGB-based images from the UAV and OBIA approach demonstrate significant potential for mapping the spatial variability of PH and stalk density and for estimating sugarcane yield. This can aid growers and millers in decision making. ? 2020 Elsevier B.V.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "A mature-tomato detection algorithm using machine learning and color analysis",
        "year": "2019",
        "algorithm": "",
        "performance": "",
        "citation": "@article{liu2019mature,\n  title={A mature-tomato detection algorithm using machine learning and color analysis},\n  author={Liu, Guoxu and Mao, Shuyi and Kim, Jae Ho},\n  journal={Sensors},\n  volume={19},\n  number={9},\n  pages={2023},\n  year={2019},\n  publisher={MDPI}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{liu2019mature}",
          "Article Title": "A mature-tomato detection algorithm using machine learning and color analysis",
          "Times Cited, All Databases": "65",
          "Publication Year": "2019",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{liu2019mature,\n  title={A mature-tomato detection algorithm using machine learning and color analysis},\n  author={Liu, Guoxu and Mao, Shuyi and Kim, Jae Ho},\n  journal={Sensors},\n  volume={19},\n  number={9},\n  pages={2023},\n  year={2019},\n  publisher={MDPI}\n}\n",
          "Document Type": "",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "An algorithm was proposed for automatic tomato detection in regular color images to reduce the influence of illumination and occlusion. In this method, the Histograms of Oriented Gradients (HOG) descriptor was used to train a Support VectorMachine (SVM) classifier. A coarse-to-fine scanning method was developed to detect tomatoes, followed by a proposed False Color Removal (FCR) method to remove the false-positive detections. Non-Maximum Suppression (NMS) was used to merge the overlapped results. Compared with other methods, the proposed algorithm showed substantial improvement in tomato detection. The results of tomato detection in the test images showed that the recall, precision, and F1 score of the proposed method were 90.00%, 94.41 and 92.15%, respectively. ? 2019 by the authors. Licensee MDPI, Basel, Switzerland.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Citrus Detection Method in Night Environment Based on Improved YOLO v3 Network; [»ùÓÚ¸Ä½øYOLO v3ÍøÂçµÄÒ¹¼ä»·¾³¸ÌéÙÊ¶±ð·½·¨]",
        "year": "2020",
        "algorithm": "",
        "performance": "",
        "citation": "@article{ÐÜ¿¡ÌÎ2020»ùÓÚ¸Ä½ø,\n  title={»ùÓÚ¸Ä½ø YOLO v3 ÍøÂçµÄÒ¹¼ä»·¾³¸ÌéÙÊ¶±ð·½·¨},\n  author={ÐÜ¿¡ÌÎ and Ö£Õò»Ô and Áº¼Î¶÷ and ÖÓ×Æ and Áõ°ØÁÖ and Ëï±¦Ï¼},\n  journal={Å©Òµ»úÐµÑ§±¨},\n  volume={51},\n  number={4},\n  pages={199--206},\n  year={2020}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "",
          "Article Title": "Citrus Detection Method in Night Environment Based on Improved YOLO v3 Network; [»ùÓÚ¸Ä½øYOLO v3ÍøÂçµÄÒ¹¼ä»·¾³¸ÌéÙÊ¶±ð·½·¨]",
          "Times Cited, All Databases": "55",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{ÐÜ¿¡ÌÎ2020»ùÓÚ¸Ä½ø,\n  title={»ùÓÚ¸Ä½ø YOLO v3 ÍøÂçµÄÒ¹¼ä»·¾³¸ÌéÙÊ¶±ð·½·¨},\n  author={ÐÜ¿¡ÌÎ and Ö£Õò»Ô and Áº¼Î¶÷ and ÖÓ×Æ and Áõ°ØÁÖ and Ëï±¦Ï¼},\n  journal={Å©Òµ»úÐµÑ§±¨},\n  volume={51},\n  number={4},\n  pages={199--206},\n  year={2020}\n}\n",
          "Document Type": "",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "In China, citrus production occupies an important position in agriculture and has great economic benefit. For a long time, most of citrus harvesting relies on manual work, which has low efficiency and high labor cost. The labor cost accounts for almost one-half of total labor cost in citrus production process. In addition, citrus picking is usually carried out during the day, while makes less use of night time. Therefore, it is of great significance to develop a fruit picking robot working at nighttime. Focusing on citrus picking process, a multi-scale convolution neural network named Des-YOLO v3 was proposed and used to detect citrus at nighttime under natural environment. By using ResNet and DenseNet for reference, the Des-YOLO v3 network was designed to realize the reuse and fusion of multi-layer features of the network, which strengthened the robustness of small target and overlapping occlusion fruit recognition, and significantly improved the precision of fruit detection. The experimental results showed that the precision, recall rate and F1 value of the Des-YOLO v3 network were 97.67%, 97.46% and 0.976, respectively, while those of YOLO v3 network were 91.41%, 91.10% and 0.913, respectively. At the same time, the mean average precision of the trained model under the test set was 90.75%, and the detection speed was 53 f/s, which was 2.27 percentage points and 11 f/s higher than those of YOLO v3_DarkNet53, respectively. The final results showed that the Des-YOLO v3 recognition network had stronger robustness and higher detection precision for the recognition of mature citrus in the complex field environment at night, which provided technical support for the visual recognition of citrus picking robot. ? 2020, Chinese Society of Agricultural Machinery. All right reserved.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Development and experiment of end-effector for kiwifruit harvesting robot",
        "year": "2015",
        "algorithm": "",
        "performance": "",
        "citation": "@article{longsheng2015development,\n  title={Development and experiment of end-effector for kiwifruit harvesting robot},\n  author={Longsheng, Fu and Fanian, Zhang and Yoshinori, Gejima and Zhen, Li and Bin, Wang and Yongjie, Cui},\n  journal={Nongye Jixie Xuebao/transactions of the Chinese Society of Agricultural Machinery},\n  volume={46},\n  number={3},\n  year={2015}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{longsheng2015development}",
          "Article Title": "Development and experiment of end-effector for kiwifruit harvesting robot",
          "Times Cited, All Databases": "54",
          "Publication Year": "2015",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{longsheng2015development,\n  title={Development and experiment of end-effector for kiwifruit harvesting robot},\n  author={Longsheng, Fu and Fanian, Zhang and Yoshinori, Gejima and Zhen, Li and Bin, Wang and Yongjie, Cui},\n  journal={Nongye Jixie Xuebao/transactions of the Chinese Society of Agricultural Machinery},\n  volume={46},\n  number={3},\n  year={2015}\n}\n",
          "Document Type": "",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "Fruit nondestructive picking is one of the key technologies of developing harvesting robot. A nondestructive picking end-effector of kiwifruit was studied. Firstly, based on the artificial way of kiwifruit picking and the biology characteristics of kiwifruit stem, a fruit picking method for robot was proposed, which need to separate the fruit from stem and hold the fruit to prevent it dropping. Then, the picking method was verified by a special designed separation test of fruit and its stem. After that, an end-effector was designed and manufactured based on the fruit picking method, which approached a fruit from the bottom, and enveloped and grabbed the fruit from two sides, and then rotated up to separate the fruit from stem. In the end, the performance of end-effector prototype was tested on the most common cultivar 'Hayward' at the Meixian Kiwifruit Experimental Station of Northwest A&F University. The results showed that the proposed picking method could separate the fruit successfully with the least force of 1.3 N when the angle between fruit and stem was set as 60¡ã, which is not significantly different from the manual picking of which the angle between fruit and stem is approximately to 90¡ã in normal. The end-effector was tested on 68 samples (28 in the morning, 25 in the noon, and 15 in the night). All of them were successfully picked and held by the end-effector. Among them, two samples were picked with stem which might be caused by the reason of that the fruit is not ripe enough to be harvested. All the picked fruit were free of damage until ripen for eating. In all, the end-effector could effectively solve the problems of separating the adjacent fruits, grab a single fruit with an accuracy of 100%, and pick and hold it nondestructively. The success rate of picking was 96.0% and the average picking time was 22 s. ?, 2015, Chinese Society of Agricultural Machinery. All right reserved.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Development of sensors-based agri-food traceability system remotely managed by a software platform for optimized farm management",
        "year": "2020",
        "algorithm": "",
        "performance": "",
        "citation": "@article{visconti2020development,\n  title={Development of sensors-based agri-food traceability system remotely managed by a software platform for optimized farm management},\n  author={Visconti, Paolo and de Fazio, Roberto and Vel{\\'a}zquez, Ramiro and Del-Valle-Soto, Carolina and Giannoccaro, Nicola Ivan},\n  journal={Sensors},\n  volume={20},\n  number={13},\n  pages={3632},\n  year={2020},\n  publisher={MDPI}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{visconti2020development}",
          "Article Title": "Development of sensors-based agri-food traceability system remotely managed by a software platform for optimized farm management",
          "Times Cited, All Databases": "56",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{visconti2020development,\n  title={Development of sensors-based agri-food traceability system remotely managed by a software platform for optimized farm management},\n  author={Visconti, Paolo and de Fazio, Roberto and Vel{\\'a}zquez, Ramiro and Del-Valle-Soto, Carolina and Giannoccaro, Nicola Ivan},\n  journal={Sensors},\n  volume={20},\n  number={13},\n  pages={3632},\n  year={2020},\n  publisher={MDPI}\n}\n",
          "Document Type": "",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "The huge spreading of Internet of things (IoT)-oriented modern technologies is revolutionizing all fields of human activities, leading several benefits and allowing to strongly optimize classic productive processes. The agriculture field is also affected by these technological advances, resulting in better water and fertilizers¡¯ usage and so huge improvements of both quality and yield of the crops. In this manuscript, the development of an IoT-based smart traceability and farm management system is described, which calibrates the irrigations and fertigation operations as a function of crop typology, growth phase, soil and environment parameters and weather information; a suitable software architecture was developed to support the system decision-making process, also based on data collected on-field by a properly designed solar-powered wireless sensor network (WSN). The WSN nodes were realized by using the ESP8266 NodeMCU module exploiting its microcontroller functionalities and Wi-Fi connectivity. Thanks to a properly sized solar power supply system and an optimized scheduling scheme, a long node autonomy was guaranteed, as experimentally verified by its power consumption measures, thus reducing WSN maintenance. In addition, a literature analysis on the most used wireless technologies for agri-food products¡¯ traceability is reported, together with the design and testing of a Bluetooth low energy (BLE) low-cost sensor tag to be applied into the containers of agri-food products, just collected from the fields or already processed, to monitor the main parameters indicative of any failure or spoiling over time along the supply chain. A mobile application was developed for monitoring the tracking information and storing conditions of the agri-food products. Test results in real-operative scenarios demonstrate the proper operation of the BLE smart tag prototype and tracking system. ? 2020 by the authors. Licensee MDPI, Basel, Switzerland.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Robotics and Automation in Agriculture: Present and Future Applications",
        "year": "2020",
        "algorithm": "",
        "performance": "",
        "citation": "@article{mahmud2020robotics,\n  title={Robotics and automation in agriculture: present and future applications},\n  author={Mahmud, Mohd Saiful Azimi and Abidin, Mohamad Shukri Zainal and Emmanuel, Abioye Abiodun and Hasan, Hameedah Sahib},\n  journal={Applications of Modelling and Simulation},\n  volume={4},\n  pages={130--140},\n  year={2020}\n}",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{mahmud2020robotics}",
          "Article Title": "Robotics and Automation in Agriculture: Present and Future Applications",
          "Times Cited, All Databases": "50",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "Applications of Modelling and Simulation",
          "Citation": "@article{mahmud2020robotics,\n  title={Robotics and automation in agriculture: present and future applications},\n  author={Mahmud, Mohd Saiful Azimi and Abidin, Mohamad Shukri Zainal and Emmanuel, Abioye Abiodun and Hasan, Hameedah Sahib},\n  journal={Applications of Modelling and Simulation},\n  volume={4},\n  pages={130--140},\n  year={2020}\n}",
          "Document Type": "Review",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "Agriculture is the backbone of society as it mainly functions to provide food, feed and fiber on which all human depends to live. Precision agriculture is implemented with a goal to apply sufficient treatments at the right place in the right time with the purpose to provide low-input, high efficiency and sustainable agricultural production. In precision agriculture, automation and robotics have become one of the main frameworks which focusing on minimizing environmental impact and simultaneously maximizing agricultural produce. The application of automation and robotics in precision agriculture is essentially implemented for precise farm management by using modern technologies. In the past decades, a significant amount of research has focused on the applications of mobile robot for agricultural operations such as planting, inspection, spraying and harvesting. This paper reviews the recent applications of automation and robotics in agriculture in the past five years. In this paper, the recent implementations are divided into four categories which indicates different operations executed for planting management starting from a seed until the product is ready to be harvested. Towards the end of this paper, several challenges and suggestions are described to indicate the opportunities and improvements that can be made in designing an efficient autonomous and robotics system for agricultural applications. Based on the conducted review, different operations have different challenges thus require diverse solutions to solve the specific operational problem. Therefore, the development process of an efficient autonomous agricultural robotic system must consider all possibilities and challenges in different types of agricultural operation to minimize system errors during future implementation. In addition, the development cost needs to be fully considered to ensure that the farmers will be able to invest their capital as a consumer. Therefore, it will become highly possible for the autonomous agricultural robotic system to be widely implemented throughout the world in the future. ? 2020 The Authors.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Ethics of Using AI and Big Data in Agriculture: The Case of a Large Agriculture Multinational",
        "year": "2019",
        "algorithm": "",
        "performance": "",
        "citation": "@article{mark2019ethics,\n  title={Ethics of using AI and big data in agriculture: The case of a large agriculture multinational},\n  author={Mark, Ryan},\n  journal={The ORBIT Journal},\n  volume={2},\n  number={2},\n  pages={1--27},\n  year={2019},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{mark2019ethics}",
          "Article Title": "Ethics of Using AI and Big Data in Agriculture: The Case of a Large Agriculture Multinational",
          "Times Cited, All Databases": "66",
          "Publication Year": "2019",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{mark2019ethics,\n  title={Ethics of using AI and big data in agriculture: The case of a large agriculture multinational},\n  author={Mark, Ryan},\n  journal={The ORBIT Journal},\n  volume={2},\n  number={2},\n  pages={1--27},\n  year={2019},\n  publisher={Elsevier}\n}\n",
          "Document Type": "article",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Ryan Mark",
          "Abstract": "Smart information systems (Big Data and artificial intelligence) are used in the agricultural industry to help the planting, seeding, and harvesting of crops, as well as farm management, plant and livestock illness and disease detection. I looked at how a Digital Division at a large agricultural multinational is using smart information systems (SIS), through their SISproject, to provide farmers with local weather predictions, farm efficiency and sustainability metrics, and early detection systems for weed, pests and disease. SIS being used in agriculture, types of data retrieved from the farm, how this data is analysed, and agribusinesses involved in this burgeoning field. Agricultural SIS has the potential to automate activities that are typically done by agronomists, allowing for cost reductions, quick and effective crop forecasting, and improved decision-making and efficiency for the farmer. Agricultural SIS also offers agribusinesses an additional revenue, better customer-relations, and reduced costs from hiring additional agronomists and advisors. The world¡¯s population will exceed 9 billion by 2050, forcing the agricultural sector to increase its production levels by up to 70%. SIS are being hailed as one possible solution to help plant, seed, harvest, and manage farms better and more effectively. However, the use of agricultural SIS may create a number of ethical concerns. For example, the accuracy of data and recommendations provided by SIS may lead to lost harvests, ill livestock, and loss of earnings. There is also a tension between ensuring an agribusinesses¡¯ intellectual property and the protection of the farmer¡¯s data ownership. The use of SIS is relatively expensive, which may create a digital divide. Agricultural Big Data is also vulnerable to privacy and security threats because it could be used nefariously by corrupt governments, competitors, or even market traders. Sensors, robots and devices may cause harm, distress, and damage to animal welfare and the environment.To assess if these ethical issues mirror those experienced in the field, I interviewed three members of this company working on their SIS project. This project combines data retrieved from the farmer with the company¡¯s agronomic knowledge to manage their farm more effectively. The project was designed to provide farmers with local weather predictions, plant disease in situ detection, and recommendation tools to minimise risk, crop and yield previews, farm efficiency and sustainability metrics, and early detection systems for weed, pests and disease. One of the primary motivations for using SIS technology for the company is the ability to make the farmer¡¯s life easier, more productive, and to save costs. The aim is to improve farm management, not by increasing fertilizer use, but by more intelligent farming decisions and practices. The ethical issues faced in the project strongly correlated with those in the literature, with the addition of employment. The general public is concerned that SIS will replace human jobs, such as the agronomist, but the team stated that their SIS is intended to complement the human expert, rather than replace them. Accuracy and availability of data proved to be an issue because not all farmers had available data and data retrieved from third-parties may not be accurate. The team ensure that their customers¡¯ privacyis protected by having strong security measures to avoid misuse and hacking. Data ownership belongs to the farmer and they can move to a different farm management system supplier, with that data, if they choose to. The tool is free to use to avoid the issue of a digital divide. The company incorporate a strong sustainability agenda into their SIS, developing it from the European PEF (Product Environmental Footprint) and a Life-cycle assessment (LCA) framework. Overall, my report was able to evaluate how ethical issues found within the SIS literature correlate with those identified, and tackled, in practice.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Fuzzy classification of pre-harvest tomatoes for ripeness estimation ¨C An approach based on automatic rule learning using decision tree",
        "year": "2015",
        "algorithm": "",
        "performance": "",
        "citation": "@article{goel2015fuzzy,\n  title={Fuzzy classification of pre-harvest tomatoes for ripeness estimation--An approach based on automatic rule learning using decision tree},\n  author={Goel, Nidhi and Sehgal, Priti},\n  journal={Applied Soft Computing},\n  volume={36},\n  pages={45--56},\n  year={2015},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{goel2015fuzzy}",
          "Article Title": "Fuzzy classification of pre-harvest tomatoes for ripeness estimation ¨C An approach based on automatic rule learning using decision tree",
          "Times Cited, All Databases": "123",
          "Publication Year": "2015",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{goel2015fuzzy,\n  title={Fuzzy classification of pre-harvest tomatoes for ripeness estimation--An approach based on automatic rule learning using decision tree},\n  author={Goel, Nidhi and Sehgal, Priti},\n  journal={Applied Soft Computing},\n  volume={36},\n  pages={45--56},\n  year={2015},\n  publisher={Elsevier}\n}\n",
          "Document Type": "article",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Nidhi Goel and Priti Sehgal",
          "Abstract": "Tomato (Solanum lycopersicum) ripeness estimation is an important process that affects its quality evaluation and marketing. However, the slow speed, subjectivity, time consumption associated with manual assessment has been forcing the agriculture industry to apply automation through robots. The vision system of harvesting robot is responsible for two-tasks. The first task is the recognition of object (tomato) and second is the classification of recognized objects (tomatoes). In this paper, Fuzzy Rule-Based Classification approach (FRBCS) has been proposed to estimate the ripeness of tomatoes based on color. The two color depictions: red-green color difference and red-green color ratio are derived from extracted RGB color information. These are then compared as a criterion for classification. Fuzzy partitioning of the feature space into linguistic variables is done by means of a learning algorithm. A rule set is automatically generated from the derived feature set using Decision Trees. Mamdani fuzzy inference system is adopted for building the fuzzy rule based classification system that classifies the tomatoes into six maturity stages. Dataset used for experiments has been created using the real images that were collected from a farm. 70% of the total images were used for training and 30% images of the total were used for testing the dataset respectively. Training dataset is divided into six classes representing the six different stages of tomato ripeness. Experimental results showed the system achieved the ripeness classification accuracy of 94.29% using proposed FRBCS.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Human¨Crobot interaction in agriculture: A survey and current challenges",
        "year": "2019",
        "algorithm": "",
        "performance": "",
        "citation": "@article{vasconez2019human,\n  title={Human--robot interaction in agriculture: A survey and current challenges},\n  author={Vasconez, Juan P and Kantor, George A and Cheein, Fernando A Auat},\n  journal={Biosystems engineering},\n  volume={179},\n  pages={35--48},\n  year={2019},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{vasconez2019human}",
          "Article Title": "Human¨Crobot interaction in agriculture: A survey and current challenges",
          "Times Cited, All Databases": "271",
          "Publication Year": "2019",
          "Highly Cited Status": "",
          "Publisher": "Biosystems engineering",
          "Citation": "@article{vasconez2019human,\n  title={Human--robot interaction in agriculture: A survey and current challenges},\n  author={Vasconez, Juan P and Kantor, George A and Cheein, Fernando A Auat},\n  journal={Biosystems engineering},\n  volume={179},\n  pages={35--48},\n  year={2019},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Juan P. Vasconez and George A. Kantor and Fernando A. {Auat Cheein}",
          "Abstract": "Human¨Crobot interaction (HRI) is an extensive and diverse research topic that has been gaining importance in last years. Different fields of study have used HRI approaches for solving complicated problems, where humans and robots interact in some way to obtain advantages from their collaboration. Many industrial areas benefit by applying HRI strategies in their applications, and agriculture is one of the most challenging of them. Currently, field crops can reach highly autonomous levels whereas speciality crops do not. In particular, crops such as fruits and vegetables are still harvested manually, and also some tasks such as pruning and thinning have long been considered to be too complex to automate completely. In addition, several countries face the problem of farm labour shortages. As a consequence, the production process is affected. In this context, we survey HRI approaches and ap-plications focused on improving the working conditions, agility, efficiency, safety, productivity and profitability of agricultural processes, in cases where manual labour cannot be replaced by but can be complemented with robots.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Automatic non-destructive video estimation of maturation levels in Fuji apple (Malus Malus pumila) fruit in orchard based on colour (Vis) and spectral (NIR) data",
        "year": "2020",
        "algorithm": "",
        "performance": "",
        "citation": "@article{pourdarbani2020automatic,\n  title={Automatic non-destructive video estimation of maturation levels in Fuji apple (Malus Malus pumila) fruit in orchard based on colour (Vis) and spectral (NIR) data},\n  author={Pourdarbani, Razieh and Sabzi, Sajad and Kalantari, Davood and Karimzadeh, Rouhollah and Ilbeygi, Elham and Arribas, Juan I},\n  journal={Biosystems Engineering},\n  volume={195},\n  pages={136--151},\n  year={2020},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{pourdarbani2020automatic}",
          "Article Title": "Automatic non-destructive video estimation of maturation levels in Fuji apple (Malus Malus pumila) fruit in orchard based on colour (Vis) and spectral (NIR) data",
          "Times Cited, All Databases": "51",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{pourdarbani2020automatic,\n  title={Automatic non-destructive video estimation of maturation levels in Fuji apple (Malus Malus pumila) fruit in orchard based on colour (Vis) and spectral (NIR) data},\n  author={Pourdarbani, Razieh and Sabzi, Sajad and Kalantari, Davood and Karimzadeh, Rouhollah and Ilbeygi, Elham and Arribas, Juan I},\n  journal={Biosystems Engineering},\n  volume={195},\n  pages={136--151},\n  year={2020},\n  publisher={Elsevier}\n}\n",
          "Document Type": "article",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Razieh Pourdarbani and Sajad Sabzi and Davood Kalantari and Rouhollah Karimzadeh and Elham Ilbeygi and Juan I. Arribas",
          "Abstract": "Non-destructive estimates information on the desired properties of fruit without damaging them. The objective of this work is to present an algorithm for the automatic and non-destructive estimation of four maturity stages (unripe, half-ripe, ripe, or overripe) of Fuji apples (Malus Malus pumila) using both colour and spectral data from fruit. In order to extract spectral and colour data to train a proposed system, 170 samples of Fuji apples were collected. Colour and spectral features were extracted using a CR-400 Chroma Meter colorimeter and a custom set up. The second component a? of La?b? colour space and near infrared (NIR) spectrum data in wavelength ranges of 535¨C560?nm, 835¨C855?nm, and 950¨C975?nm, were used to train the proposed algorithm. A hybrid artificial neural network-simulated annealing algorithm (ANN-SA) was used for classification purposes. A total of 1000 iterations were conducted to evaluate the reliability of the classification process. Results demonstrated that after training the correction classification rate (CCR, accuracy) was, at the best state, 100% (test set) using both colour and spectral data. The CCR of the four different classifiers were 93.27%, 99.62%, 98.55%, and 99.59%, for colour features, spectral data wavelength ranges of 535¨C560?nm, 835¨C855?nm, and 950¨C975?nm, respectively, over the test set. These results suggest that the proposed method is capable of the non-destructive estimation of different maturity stages of Fuji apple with a remarkable accuracy, in particular within the 535¨C560?nm wavelength range.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "A comprehensive review of fruit and vegetable classification?techniques",
        "year": "2018",
        "algorithm": "",
        "performance": "",
        "citation": "@article{hameed2018comprehensive,\n  title={A comprehensive review of fruit and vegetable classification techniques},\n  author={Hameed, Khurram and Chai, Douglas and Rassau, Alexander},\n  journal={Image and Vision Computing},\n  volume={80},\n  pages={24--44},\n  year={2018},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{hameed2018comprehensive}",
          "Article Title": "A comprehensive review of fruit and vegetable classification?techniques",
          "Times Cited, All Databases": "170",
          "Publication Year": "2018",
          "Highly Cited Status": "",
          "Publisher": "Image and Vision Computing",
          "Citation": "@article{hameed2018comprehensive,\n  title={A comprehensive review of fruit and vegetable classification techniques},\n  author={Hameed, Khurram and Chai, Douglas and Rassau, Alexander},\n  journal={Image and Vision Computing},\n  volume={80},\n  pages={24--44},\n  year={2018},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Khurram Hameed and Douglas Chai and Alexander Rassau",
          "Abstract": "Recent advancements in computer vision have enabled wide-ranging applications in every field of life. One such application area is fresh produce classification, but the classification of fruit and vegetable has proven to be a complex problem and needs to be further developed. Fruit and vegetable classification presents significant challenges due to interclass similarities and irregular intraclass characteristics. Selection of appropriate data acquisition sensors and feature representation approach is also crucial due to the huge diversity of the field. Fruit and vegetable classification methods have been developed for quality assessment and robotic harvesting but the current state-of-the-art has been developed for limited classes and small datasets. The problem is of a multi-dimensional nature and offers significantly hyperdimensional features, which is one of the major challenges with current machine learning approaches. Substantial research has been conducted for the design and analysis of classifiers for hyperdimensional features which require significant computational power to optimise with such features. In recent years numerous machine learning techniques for example, Support Vector Machine (SVM), K-Nearest Neighbour (KNN), Decision Trees, Artificial Neural Networks (ANN) and Convolutional Neural Networks (CNN) have been exploited with many different feature description methods for fruit and vegetable classification in many real-life applications. This paper presents a critical comparison of different state-of-the-art computer vision methods proposed by researchers for classifying fruit and vegetable.",
          "Keywords Plus": ""
        }
      }
    ],
    "Mask R-CNN": [
      {
        "title": "Fruit detection for strawberry harvesting robot in non-structural environment based on Mask-RCNN",
        "year": "2019",
        "algorithm": "Mask-RCNN,Resnet50 backbone",
        "performance": "An average detection precision rate of 95.78%, the recall rate was 95.41%.",
        "citation": "@article{yu2019fruit,\n  title={Fruit detection for strawberry harvesting robot in non-structural environment based on Mask-RCNN},\n  author={Yu, Yang and Zhang, Kailiang and Yang, Li and Zhang, Dongxing},\n  journal={Computers and Electronics in Agriculture},\n  volume={163},\n  pages={104846},\n  year={2019},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "strawberry",
        "paper": {
          "relevant": "y",
          "": "\\cite{yu2019fruit}",
          "Article Title": "Fruit detection for strawberry harvesting robot in non-structural environment based on Mask-RCNN",
          "Times Cited, All Databases": "373",
          "Publication Year": "2019",
          "Highly Cited Status": "Y",
          "Publisher": "Computers and Electronics in Agriculture",
          "Citation": "@article{yu2019fruit,\n  title={Fruit detection for strawberry harvesting robot in non-structural environment based on Mask-RCNN},\n  author={Yu, Yang and Zhang, Kailiang and Yang, Li and Zhang, Dongxing},\n  journal={Computers and Electronics in Agriculture},\n  volume={163},\n  pages={104846},\n  year={2019},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "vision:fruit detection",
          "fruit/veg": "strawberry",
          "Data Modality": "",
          "Learning Algorithm": "Mask-RCNN,Resnet50 backbone",
          "Locomotion": "",
          "Performance": "An average detection precision rate of 95.78%, the recall rate was 95.41%.",
          "challenges": "overlapping and hidden fruits, and those under varying illumination",
          "Authors": "Yu, Y; Zhang, KL; Yang, L; Zhang, DX",
          "Abstract": "Deep learning has demonstrated excellent capabilities for learning image features and is widely used in image object detection. In order to improve the performance of machine vision in fruit detection for a strawberry harvesting robot, Mask Region Convolutional Neural Network (Mask-RCNN) was introduced. Resnet50 was adopted as backbone network, combined with the Feature Pyramid Network (FPN) architecture for feature extraction. The Region Proposal Network (RPN) was trained end-to-end to create region proposals for each feature map. After generating mask images of ripe fruits from Mask R-CNN, a visual localization method for strawberry picking points was performed. Fruit detection results of 100 test images showed that the average detection precision rate was 95.78%, the recall rate was 95.41% and the mean intersection over union (MIoU) rate for instance segmentation was 89.85%. The prediction results of 573 ripe fruit picking points showed that the average error was +/- 1.2 mm. Compared with four traditional methods, the method proposed demonstrates improved universality and robustness in a non-structural environment, particularly for overlapping and hidden fruits, and those under varying illumination.",
          "Keywords Plus": "APPLE DETECTION; RECOGNITION"
        }
      },
      {
        "title": "Detection and segmentation of overlapped fruits based on optimized mask R-CNN application in apple harvesting robot",
        "year": "2020",
        "algorithm": "Mask-RCNN",
        "performance": "The Precision Rate has reached 97.31%, and the Recall Rate has reached 95.70%.",
        "citation": "@article{jia2020detection,\n  title={Detection and segmentation of overlapped fruits based on optimized mask R-CNN application in apple harvesting robot},\n  author={Jia, Weikuan and Tian, Yuyu and Luo, Rong and Zhang, Zhonghua and Lian, Jian and Zheng, Yuanjie},\n  journal={Computers and Electronics in Agriculture},\n  volume={172},\n  pages={105380},\n  year={2020},\n  publisher={Elsevier}\n}",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{jia2020detection}",
          "Article Title": "Detection and segmentation of overlapped fruits based on optimized mask R-CNN application in apple harvesting robot",
          "Times Cited, All Databases": "198",
          "Publication Year": "2020",
          "Highly Cited Status": "Y",
          "Publisher": "N",
          "Citation": "@article{jia2020detection,\n  title={Detection and segmentation of overlapped fruits based on optimized mask R-CNN application in apple harvesting robot},\n  author={Jia, Weikuan and Tian, Yuyu and Luo, Rong and Zhang, Zhonghua and Lian, Jian and Zheng, Yuanjie},\n  journal={Computers and Electronics in Agriculture},\n  volume={172},\n  pages={105380},\n  year={2020},\n  publisher={Elsevier}\n}",
          "Document Type": "Article",
          "Main Contribution": "detection and segmentation",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "Mask-RCNN",
          "Locomotion": "",
          "Performance": "The Precision Rate has reached 97.31%, and the Recall Rate has reached 95.70%.",
          "challenges": "",
          "Authors": "Jia, WK; Tian, YY; Luo, R; Zhang, ZH; Lian, J; Zheng, YJ",
          "Abstract": "In order to better apply the good performance of feature extraction and target detection used in deep learning to fruit detection in orchards, a model of harvesting robot vision detector based on Mask Region Convolutional Neural Network (Mask R-CNN) is proposed. The model was improved to make it more suitable for the recognition and segmentation of overlapped apples. Residual Network (ResNet) combined with Densely Connected Convolutional Networks (DenseNet) can greatly reduce input parameters and is used as a backbone network for feature extraction. Feature maps are input to the Region Proposal Network (RPN) for end-to-end training to generate the region of interest (Rot), and finally the mask is generated by the full convolution network (FCN) to get the region where the apple is located. The method is tested by a random test set with 120 images, and the Precision Rate has reached 97.31%, and the Recall Rate has reached 95.70%. And the recognition speed is faster, which can meet the requirements of the apple harvesting robot's vision system.",
          "Keywords Plus": ""
        }
      },
      {
        "title": "Deep learning-based apple detection using a suppression mask R-CNN",
        "year": "2021",
        "algorithm": "Mask-RCNN",
        "performance": "",
        "citation": "@article{chu2021deep,\n  title={Deep learning-based apple detection using a suppression mask R-CNN},\n  author={Chu, Pengyu and Li, Zhaojian and Lammers, Kyle and Lu, Renfu and Liu, Xiaoming},\n  journal={Pattern Recognition Letters},\n  volume={147},\n  pages={206--211},\n  year={2021},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{chu2021deep}",
          "Article Title": "Deep learning-based apple detection using a suppression mask R-CNN",
          "Times Cited, All Databases": "72",
          "Publication Year": "2021",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{chu2021deep,\n  title={Deep learning-based apple detection using a suppression mask R-CNN},\n  author={Chu, Pengyu and Li, Zhaojian and Lammers, Kyle and Lu, Renfu and Liu, Xiaoming},\n  journal={Pattern Recognition Letters},\n  volume={147},\n  pages={206--211},\n  year={2021},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "fruit dection",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "Mask-RCNN",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Chu, PY; Li, ZJ; Lammers, K; Lu, RF; Liu, XM",
          "Abstract": "Robotic apple harvesting has received much research attention in the past few years due to growing shortage and rising cost in labor. One key enabling technology towards automated harvesting is accurate and robust apple detection, which poses great challenges as a result of the complex orchard environment that involves varying lighting conditions and foliage/branch occlusions. This letter reports on the development of a novel deep learning-based apple detection framework named Suppression Mask R-CNN. Specifically, we first collect a comprehensive apple orchard dataset for Gala and Blondee apples, using a color camera, under different lighting conditions (overcast and front lighting vs. back lighting). We then develop a novel suppression Mask R-CNN for apple detection, in which a suppression branch is added to the standard Mask R-CNN to suppress non-apple features generated by the original network. Comprehensive evaluations are performed, which show that the developed suppression Mask R-CNN network outperforms state-of-the-art models with a higher F1-score of 0.905 and a detection time of 0.25 second per frame on a standard desktop computer. (C) 2021 Elsevier B.V. All rights reserved.",
          "Keywords Plus": "COMPUTER VISION; FRUIT"
        }
      }
    ],
    "YOLOv3": [
      {
        "title": "YOLO-Tomato: A Robust Algorithm for Tomato Detection Based on YOLOv3",
        "year": "2020",
        "algorithm": "YOLOv3",
        "performance": "",
        "citation": "@article{liu2020yolo,\n  title={YOLO-tomato: A robust algorithm for tomato detection based on YOLOv3},\n  author={Liu, Guoxu and Nouaze, Joseph Christian and Touko Mbouembe, Philippe Lyonel and Kim, Jae Ho},\n  journal={Sensors},\n  volume={20},\n  number={7},\n  pages={2145},\n  year={2020},\n  publisher={MDPI}\n}\n",
        "fruit_veg": "tomato",
        "paper": {
          "relevant": "y",
          "": "\\cite{liu2020yolo}",
          "Article Title": "YOLO-Tomato: A Robust Algorithm for Tomato Detection Based on YOLOv3",
          "Times Cited, All Databases": "223",
          "Publication Year": "2020",
          "Highly Cited Status": "Y",
          "Publisher": "sensors",
          "Citation": "@article{liu2020yolo,\n  title={YOLO-tomato: A robust algorithm for tomato detection based on YOLOv3},\n  author={Liu, Guoxu and Nouaze, Joseph Christian and Touko Mbouembe, Philippe Lyonel and Kim, Jae Ho},\n  journal={Sensors},\n  volume={20},\n  number={7},\n  pages={2145},\n  year={2020},\n  publisher={MDPI}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "fruit dection",
          "fruit/veg": "tomato",
          "Data Modality": "",
          "Learning Algorithm": "YOLOv3",
          "Locomotion": "",
          "Performance": "",
          "challenges": "illumination variation, branch, and leaf occlusion as well as tomato overlap",
          "Authors": "Liu, GX; Nouaze, JC; Mbouembe, PLT; Kim, JH",
          "Abstract": "Automatic fruit detection is a very important benefit of harvesting robots. However, complicated environment conditions, such as illumination variation, branch, and leaf occlusion as well as tomato overlap, have made fruit detection very challenging. In this study, an improved tomato detection model called YOLO-Tomato is proposed for dealing with these problems, based on YOLOv3. A dense architecture is incorporated into YOLOv3 to facilitate the reuse of features and help to learn a more compact and accurate model. Moreover, the model replaces the traditional rectangular bounding box (R-Bbox) with a circular bounding box (C-Bbox) for tomato localization. The new bounding boxes can then match the tomatoes more precisely, and thus improve the Intersection-over-Union (IoU) calculation for the Non-Maximum Suppression (NMS). They also reduce prediction coordinates. An ablation study demonstrated the efficacy of these modifications. The YOLO-Tomatowas compared to several state-of-the-art detection methods and it had the best detection performance.",
          "Keywords Plus": "IMAGE-ANALYSIS; VISION; FRUIT; LOCALIZATION; MACHINE; APPLES; SCENES"
        }
      },
      {
        "title": "Tomato detection based on modified YOLOv3 framework",
        "year": "2021",
        "algorithm": "YOLOv3",
        "performance": "picking time of 44ms, a success rate of over 98\\%.",
        "citation": "@article{lawal2021tomato,\n  title={Tomato detection based on modified YOLOv3 framework},\n  author={Lawal, Mubashiru Olarewaju},\n  journal={Scientific Reports},\n  volume={11},\n  number={1},\n  pages={1--11},\n  year={2021},\n  publisher={Nature Publishing Group}\n}\n",
        "fruit_veg": "tomato",
        "paper": {
          "relevant": "y",
          "": "\\cite{lawal2021tomato}",
          "Article Title": "Tomato detection based on modified YOLOv3 framework",
          "Times Cited, All Databases": "154",
          "Publication Year": "2021",
          "Highly Cited Status": "Y",
          "Publisher": "N",
          "Citation": "@article{lawal2021tomato,\n  title={Tomato detection based on modified YOLOv3 framework},\n  author={Lawal, Mubashiru Olarewaju},\n  journal={Scientific Reports},\n  volume={11},\n  number={1},\n  pages={1--11},\n  year={2021},\n  publisher={Nature Publishing Group}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "detection",
          "fruit/veg": "tomato",
          "Data Modality": "",
          "Learning Algorithm": "YOLOv3",
          "Locomotion": "",
          "Performance": "picking time of 44ms, a success rate of over 98\\%.",
          "challenges": " uneven environment conditions, such as branch and leaf occlusion, illumination variation, clusters of tomatoes, shading, and so on, ",
          "Authors": "Lawal, MO",
          "Abstract": "Fruit detection forms a vital part of the robotic harvesting platform. However, uneven environment conditions, such as branch and leaf occlusion, illumination variation, clusters of tomatoes, shading, and so on, have made fruit detection very challenging. In order to solve these problems, a modified YOLOv3 model called YOLO-Tomato models were adopted to detect tomatoes in complex environmental conditions. With the application of label what you see approach, densely architecture incorporation, spatial pyramid pooling and Mish function activation to the modified YOLOv3 model, the YOLO-Tomato models: YOLO-Tomato-A at AP 98.3% with detection time 48 ms, YOLO-Tomato-B at AP 99.3% with detection time 44 ms, and YOLO-Tomato-C at AP 99.5% with detection time 52 ms, performed better than other state-of-the-art methods.",
          "Keywords Plus": "FRUIT DETECTION; VISION; AGRICULTURE; SYSTEM"
        }
      },
      {
        "title": "Using YOLOv3 Algorithm with Pre- and Post-Processing for Apple Detection in Fruit-Harvesting Robot",
        "year": "2020",
        "algorithm": "YOLOv3",
        "performance": "a detecion time of 19ms and error rate of 7.8\\% in mistake and 9.2\\% in unrecognization.",
        "citation": "@article{kuznetsova2020using,\n  title={Using YOLOv3 algorithm with pre-and post-processing for apple detection in fruit-harvesting robot},\n  author={Kuznetsova, Anna and Maleva, Tatiana and Soloviev, Vladimir},\n  journal={Agronomy},\n  volume={10},\n  number={7},\n  pages={1016},\n  year={2020},\n  publisher={MDPI}\n}\n",
        "fruit_veg": "apples",
        "paper": {
          "relevant": "y",
          "": "\\cite{kuznetsova2020using}",
          "Article Title": "Using YOLOv3 Algorithm with Pre- and Post-Processing for Apple Detection in Fruit-Harvesting Robot",
          "Times Cited, All Databases": "113",
          "Publication Year": "2020",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{kuznetsova2020using,\n  title={Using YOLOv3 algorithm with pre-and post-processing for apple detection in fruit-harvesting robot},\n  author={Kuznetsova, Anna and Maleva, Tatiana and Soloviev, Vladimir},\n  journal={Agronomy},\n  volume={10},\n  number={7},\n  pages={1016},\n  year={2020},\n  publisher={MDPI}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "detection",
          "fruit/veg": "apples",
          "Data Modality": "",
          "Learning Algorithm": "YOLOv3",
          "Locomotion": "",
          "Performance": "a detecion time of 19ms and error rate of 7.8\\% in mistake and 9.2\\% in unrecognization.",
          "challenges": "",
          "Authors": "Kuznetsova, A; Maleva, T; Soloviev, V",
          "Abstract": "A machine vision system for detecting apples in orchards was developed. The system was designed to be used in harvesting robots and is based on a YOLOv3 algorithm with special pre- and post-processing. The proposed pre- and post-processing techniques made it possible to adapt the YOLOv3 algorithm to be used in an apple-harvesting robot machine vision system, providing an average apple detection time of 19 ms with a share of objects being mistaken for apples at 7.8% and a share of unrecognized apples at 9.2%. Both the average detection time and error rates are less than in all known similar systems. The system can operate not only in apple-harvesting robots but also in orange-harvesting robots.",
          "Keywords Plus": "VISION SYSTEM; RECOGNITION; COLOR; ARCHITECTURE; EXTRACTION; FEATURES; FUSION; RGB"
        }
      },
      {
        "title": "Automatic Bunch Detection in White Grape Varieties Using YOLOv3, YOLOv4, and YOLOv5 Deep Learning Algorithms",
        "year": "2022",
        "algorithm": "YOLOv3,YOLOv4,YOLOv5",
        "performance": "",
        "citation": "@article{sozzi2022automatic,\n  title={Automatic bunch detection in white grape varieties using YOLOv3, YOLOv4, and YOLOv5 deep learning algorithms},\n  author={Sozzi, Marco and Cantalamessa, Silvia and Cogato, Alessia and Kayad, Ahmed and Marinello, Francesco},\n  journal={Agronomy},\n  volume={12},\n  number={2},\n  pages={319},\n  year={2022},\n  publisher={MDPI}\n}\n",
        "fruit_veg": "grape",
        "paper": {
          "relevant": "y",
          "": "\\cite{sozzi2022automatic}",
          "Article Title": "Automatic Bunch Detection in White Grape Varieties Using YOLOv3, YOLOv4, and YOLOv5 Deep Learning Algorithms",
          "Times Cited, All Databases": "127",
          "Publication Year": "2022",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{sozzi2022automatic,\n  title={Automatic bunch detection in white grape varieties using YOLOv3, YOLOv4, and YOLOv5 deep learning algorithms},\n  author={Sozzi, Marco and Cantalamessa, Silvia and Cogato, Alessia and Kayad, Ahmed and Marinello, Francesco},\n  journal={Agronomy},\n  volume={12},\n  number={2},\n  pages={319},\n  year={2022},\n  publisher={MDPI}\n}\n",
          "Document Type": "",
          "Main Contribution": "vision: pick point",
          "fruit/veg": "grape",
          "Data Modality": "",
          "Learning Algorithm": "YOLOv3,YOLOv4,YOLOv5",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "Over the last few years, several Convolutional Neural Networks for object detection have been proposed, characterised by different accuracy and speed. In viticulture, yield estimation and prediction is used for efficient crop management, taking advantage of precision viticulture techniques. Convolutional Neural Networks for object detection represent an alternative methodology for grape yield estimation, which usually relies on manual harvesting of sample plants. In this paper, six versions of the You Only Look Once (YOLO) object detection algorithm (YOLOv3, YOLOv3-tiny, YOLOv4, YOLOv4-tiny, YOLOv5x, and YOLOv5s) were evaluated for real-time bunch detection and counting in grapes. White grape varieties were chosen for this study, as the identification of white berries on a leaf background is trickier than red berries. YOLO models were trained using a heterogeneous dataset populated by images retrieved from open datasets and acquired on the field in several illumination conditions, background, and growth stages. Results have shown that YOLOv5x and YOLOv4 achieved an F1-score of 0.76 and 0.77, respectively, with a detection speed of 31 and 32 FPS. Differently, YOLO5s and YOLOv4-tiny achieved an F1-score of 0.76 and 0.69, respectively, with a detection speed of 61 and 196 FPS. The final YOLOv5x model for bunch number, obtained considering bunch occlusion, was able to estimate the number of bunches per plant with an average error of 13.3% per vine. The best combination of accuracy and speed was achieved by YOLOv4-tiny, which should be considered for real-time grape yield estimation, while YOLOv3 was affected by a False Positive¨CFalse Negative compensation, which decreased the RMSE. ? 2022 by the authors. Licensee MDPI, Basel, Switzerland.",
          "Keywords Plus": ""
        }
      }
    ],
    "YOLOv4": [
      {
        "title": "A detection algorithm for cherry fruits based on the improved YOLO-v4 model",
        "year": "2023",
        "algorithm": "improved YOLOv4",
        "performance": "an accuracy of 0.15 higher than that of YOLOv4",
        "citation": "@article{gai2023detection,\n  title={A detection algorithm for cherry fruits based on the improved YOLO-v4 model},\n  author={Gai, Rongli and Chen, Na and Yuan, Hai},\n  journal={Neural Computing and Applications},\n  volume={35},\n  number={19},\n  pages={13895--13906},\n  year={2023},\n  publisher={Springer}\n}",
        "fruit_veg": "cherry",
        "paper": {
          "relevant": "y",
          "": "\\cite{gai2023detection}",
          "Article Title": "A detection algorithm for cherry fruits based on the improved YOLO-v4 model",
          "Times Cited, All Databases": "112",
          "Publication Year": "2023",
          "Highly Cited Status": "Y",
          "Publisher": "N",
          "Citation": "@article{gai2023detection,\n  title={A detection algorithm for cherry fruits based on the improved YOLO-v4 model},\n  author={Gai, Rongli and Chen, Na and Yuan, Hai},\n  journal={Neural Computing and Applications},\n  volume={35},\n  number={19},\n  pages={13895--13906},\n  year={2023},\n  publisher={Springer}\n}",
          "Document Type": "Article",
          "Main Contribution": "different ripeness",
          "fruit/veg": "cherry",
          "Data Modality": "",
          "Learning Algorithm": "improved YOLOv4",
          "Locomotion": "",
          "Performance": "an accuracy of 0.15 higher than that of YOLOv4",
          "challenges": " How to increase product output?environmental problems such as shading",
          "Authors": "Gai, RL; Chen, N; Yuan, H",
          "Abstract": "Digital agriculture is rapidly affecting the value of agricultural output. Robotic picking of the ripe agricultural product enables accurate and rapid picking, making agricultural harvesting intelligent. How to increase product output has also become a challenge for digital agriculture. During the cherry growth process, realizing the rapid and accurate detection of cherry fruits is the key to the development of cherry fruits in digital agriculture. Due to the inaccurate detection of cherry fruits, environmental problems such as shading have become the biggest challenge for cherry fruit detection. This paper proposes an improved YOLO-V4 deep learning algorithm to detect cherry fruits. This model is suitable for cherry fruits with a small volume. It is proposed to increase the network based on the YOLO-V4 backbone network CSPDarknet53 network, combined with DenseNet The density between layers, the a priori box in the YOLO-V4 model, is changed to a circular marker box that fits the shape of the cherry fruit. Based on the improved YOLO-V4 model, the feature extraction is enhanced, the network structure is deepened, and the detection speed is improved. To verify the effectiveness of this method, different deep learning algorithms of YOLO-V3, YOLO-V3-dense and YOLO-V4 are compared. The results show that the mAP (average accuracy) value obtained by using the improved YOLO-V4 model (YOLO-V4-dense) network in this paper is 0.15 higher than that of yolov4. In actual orchard applications, cherries with different ripeness of cherries in the same area can be detected, and the fruits with larger ripeness differences can be artificially intervened, and finally, the yield of cherry fruits can be increased.",
          "Keywords Plus": "APPLE DETECTION; SEGMENTATION; TREES; RGB; CNN"
        }
      },
      {
        "title": "A real-time table grape detection method based on improved YOLOv4-tiny network in complex background",
        "year": "2021",
        "algorithm": "YOLOv4-tiny",
        "performance": "",
        "citation": "@article{li2021real,\n  title={A real-time table grape detection method based on improved YOLOv4-tiny network in complex background},\n  author={Li, Huipeng and Li, Changyong and Li, Guibin and Chen, Lixin},\n  journal={Biosystems Engineering},\n  volume={212},\n  pages={347--359},\n  year={2021},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "grape",
        "paper": {
          "relevant": "y",
          "": "\\cite{li2021real}",
          "Article Title": "A real-time table grape detection method based on improved YOLOv4-tiny network in complex background",
          "Times Cited, All Databases": "51",
          "Publication Year": "2021",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{li2021real,\n  title={A real-time table grape detection method based on improved YOLOv4-tiny network in complex background},\n  author={Li, Huipeng and Li, Changyong and Li, Guibin and Chen, Lixin},\n  journal={Biosystems Engineering},\n  volume={212},\n  pages={347--359},\n  year={2021},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "",
          "fruit/veg": "grape",
          "Data Modality": "",
          "Learning Algorithm": "YOLOv4-tiny",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Li, HP; Li, CY; Li, GB; Chen, LX",
          "Abstract": "Accurate identification of table grapes is crucial to the harvesting process of the grape picking robot. This paper proposes an efficient grape detection model, YOLO-Grape, to solve the problem of unrecognition or decreased recognition accuracy caused by the complicated growth environment, shadows of branches and leaves, and overlapping grapes. To improve the network recognition accuracy, a down-sampling fusion structure is integrated into the network, and the Mish activation function is used. Meanwhile, an attention mechanism is added to the network, and the non-maximum suppression (NMS) function is replaced with the soft non-maximum suppression (Soft-NMS) function, thereby reducing the missing of predicted boxes due to overlapping grapes. Besides, the depthwise separable convolution is introduced to improve the detection speed of the network. In addition, transfer learning is used in the training process to improve the detection accuracy and generalization ability of the model. On the test data set of 700 grape images, the experimental results show that YOLO-Grape achieves an F1-score of 90.47%, a mAP of 91.08% and a detection speed of 81 frames per second. Compared with FasterRCNN(Resnet50), SSD300, YOLOv4, and YOLOv4-tiny, the mAP of the YOLO-Grape model is increased by 1.67%, 2.28%, 0.84%, and 6.69%, respectively. The average recognition speeds of the YOLO-Grape model were 31.15, 3.38 and 6.45 times of Faster-RCNN(Resnet50), SSD300, and YOLOv4 respectively. Through four sets of comparative experiments, it is found that the proposed YOLO-Grape model achieves high recognition accuracy for occluded grapes, meeting the requirements of grape picking robots for real-time detection of multiple varieties of table grapes in complex situations. (c) 2021 IAgrE. Published by Elsevier Ltd. All rights reserved.",
          "Keywords Plus": "RECOGNITION; CLUSTERS; RGB"
        }
      },
      {
        "title": "Fruit detection and positioning technology for a Camellia oleifera C. Abel orchard based on improved YOLOv4-tiny model and binocular stereo vision",
        "year": "2023",
        "algorithm": "YOLOv4-tiny",
        "performance": "",
        "citation": "@article{tang2023fruit,\n  title={Fruit detection and positioning technology for a Camellia oleifera C. Abel orchard based on improved YOLOv4-tiny model and binocular stereo vision},\n  author={Tang, Yunchao and Zhou, Hao and Wang, Hongjun and Zhang, Yunqi},\n  journal={Expert systems with applications},\n  volume={211},\n  pages={118573},\n  year={2023},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "Camellia oleifera",
        "paper": {
          "relevant": "y",
          "": "\\cite{tang2023fruit}",
          "Article Title": "Fruit detection and positioning technology for a Camellia oleifera C. Abel orchard based on improved YOLOv4-tiny model and binocular stereo vision",
          "Times Cited, All Databases": "121",
          "Publication Year": "2023",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{tang2023fruit,\n  title={Fruit detection and positioning technology for a Camellia oleifera C. Abel orchard based on improved YOLOv4-tiny model and binocular stereo vision},\n  author={Tang, Yunchao and Zhou, Hao and Wang, Hongjun and Zhang, Yunqi},\n  journal={Expert systems with applications},\n  volume={211},\n  pages={118573},\n  year={2023},\n  publisher={Elsevier}\n}\n",
          "Document Type": "",
          "Main Contribution": "",
          "fruit/veg": "Camellia oleifera",
          "Data Modality": "",
          "Learning Algorithm": "YOLOv4-tiny",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "",
          "Abstract": "In the complex environment of an orchard, changes in illumination, leaf occlusion, and fruit overlap make it challenging for mobile picking robots to detect and locate oil-seed camellia fruit. To address this problem, YOLO-Oleifera was developed as a fruit detection model method based on a YOLOv4-tiny model, To obtain clustering results appropriate to the size of the Camellia oleifera fruit, the k-means++ clustering algorithm was used instead of the k-means clustering algorithm used by the YOLOv4-tiny model to determine bounding box priors. Two convolutional kernels of 1 ¡Á 1 and 3 ¡Á 3 were respectively added after the second and third CSPBlock modules of the YOLOv4-tiny model. This model allows the learning of Camellia oleifera fruit feature information and reduces overall computational complexity. Compared with the classic stereo matching method based on binocular camera images, this method innovatively used the bounding box generated by the YOLO-Oleifera model to extract the region of interest of the fruit, and then adaptively performs stereo matching according to the generation mechanism of the bounding box. This allows the determination of disparity and facilitates the subsequent use of the triangulation principle to determine the picking position of the fruit. An ablation experiment demonstrated the effective improvement of the YOLOv4-tiny model. Camellia oleifera fruit images obtained under sunlight and shading conditions were used to test the YOLO-Oleifera model, and the model robustly detected the fruit under different illumination conditions. Occluded Camellia oleifera fruit decreased precision and recall due to the loss of semantic information. Comparison of this model with deep learning models YOLOv5-s,YOLOv3-tiny, and YOLOv4-tiny, the YOLO-Oleifera model achieved the highest AP of 0.9207 with the smallest data weight of 29 MB. The YOLO-Oleifera model took an average of 31 ms to detect each fruit image, fast enough to meet the demand for real-time detection. The algorithm exhibited high positioning stability and robust function despite changes in illumination. The results of this study can provide a technical reference for the robust detection and positioning of Camellia oleifera fruit by a mobile picking robot in a complex orchard environment. ? 2022",
          "Keywords Plus": ""
        }
      }
    ],
    "CNN": [
      {
        "title": "Single-Shot Convolution Neural Networks for Real-Time Fruit Detection Within the Tree",
        "year": "2019",
        "algorithm": "single-shot CNN",
        "performance": "Over 90\\% fruit detection",
        "citation": "@article{bresilla2019single,\n  title={Single-shot convolution neural networks for real-time fruit detection within the tree},\n  author={Bresilla, Kushtrim and Perulli, Giulio Demetrio and Boini, Alexandra and Morandi, Brunella and Corelli Grappadelli, Luca and Manfrini, Luigi},\n  journal={Frontiers in plant science},\n  volume={10},\n  pages={421226},\n  year={2019},\n  publisher={Frontiers}\n}",
        "fruit_veg": "apples,pears",
        "paper": {
          "relevant": "y",
          "": "\\cite{bresilla2019single}",
          "Article Title": "Single-Shot Convolution Neural Networks for Real-Time Fruit Detection Within the Tree",
          "Times Cited, All Databases": "97",
          "Publication Year": "2019",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{bresilla2019single,\n  title={Single-shot convolution neural networks for real-time fruit detection within the tree},\n  author={Bresilla, Kushtrim and Perulli, Giulio Demetrio and Boini, Alexandra and Morandi, Brunella and Corelli Grappadelli, Luca and Manfrini, Luigi},\n  journal={Frontiers in plant science},\n  volume={10},\n  pages={421226},\n  year={2019},\n  publisher={Frontiers}\n}",
          "Document Type": "Article",
          "Main Contribution": "Image/video processing",
          "fruit/veg": "apples,pears",
          "Data Modality": "",
          "Learning Algorithm": "single-shot CNN",
          "Locomotion": "",
          "Performance": "Over 90\\% fruit detection",
          "challenges": "",
          "Authors": "Bresilla, K; Perulli, GD; Boini, A; Morandi, B; Grappadelli, LC; Manfrini, L",
          "Abstract": "Image/video processing for fruit detection in the tree using hard-coded feature extraction algorithms has shown high accuracy on fruit detection during recent years. While accurate, these approaches even with high-end hardware are still computationally intensive and too slow for real-time systems. This paper details the use of deep convolution neural networks architecture based on single-stage detectors. Using deep-learning techniques eliminates the need for hard-code specific features for specific fruit shapes, color and/or other attributes. This architecture takes the input image and divides into AxA grid, where A is a configurable hyper-parameter that defines the fineness of the grid. To each grid cell an image detection and localization algorithm is applied. Each of those cells is responsible to predict bounding boxes and confidence score for fruit (apple and pear in the case of this study) detected in that cell. We want this confidence score to be high if a fruit exists in a cell, otherwise to be zero, if no fruit is in the cell. More than 100 images of apple and pear trees were taken. Each tree image with approximately 50 fruits, that at the end resulted on more than 5000 images of apple and pear fruits each. Labeling images for training consisted on manually specifying the bounding boxes for fruits, where (x, y) are the center coordinates of the box and (w, h) are width and height. This architecture showed an accuracy of more than 90% fruit detection. Based on correlation between number of visible fruits, detected fruits on one frame and the real number of fruits on one tree, a model was created to accommodate this error rate. Processing speed is higher than 20 FPS which is fast enough for any grasping/harvesting robotic arm or other real-time applications.",
          "Keywords Plus": "RECOGNITION; ADOPTION; COLOR"
        }
      },
      {
        "title": "A comprehensive review of fruit and vegetable classification techniques",
        "year": "2018",
        "algorithm": "SVM,KNN,CNN",
        "performance": "",
        "citation": "@article{hameed2018comprehensive,\n  title={A comprehensive review of fruit and vegetable classification techniques},\n  author={Hameed, Khurram and Chai, Douglas and Rassau, Alexander},\n  journal={Image and Vision Computing},\n  volume={80},\n  pages={24--44},\n  year={2018},\n  publisher={Elsevier}\n}\n",
        "fruit_veg": "",
        "paper": {
          "relevant": "y",
          "": "\\cite{hameed2018comprehensive}",
          "Article Title": "A comprehensive review of fruit and vegetable classification techniques",
          "Times Cited, All Databases": "92",
          "Publication Year": "2018",
          "Highly Cited Status": "",
          "Publisher": "Image and Vision Computing",
          "Citation": "@article{hameed2018comprehensive,\n  title={A comprehensive review of fruit and vegetable classification techniques},\n  author={Hameed, Khurram and Chai, Douglas and Rassau, Alexander},\n  journal={Image and Vision Computing},\n  volume={80},\n  pages={24--44},\n  year={2018},\n  publisher={Elsevier}\n}\n",
          "Document Type": "Review",
          "Main Contribution": "vision:classification",
          "fruit/veg": "",
          "Data Modality": "",
          "Learning Algorithm": "SVM,KNN,CNN",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Hameed, K; Chai, D; Rassau, A",
          "Abstract": "Recent advancements in computer vision have enabled wide-ranging applications in every field of life. One such application area is fresh produce classification, but the classification of fruit and vegetable has proven to be a complex problem and needs to be further developed. Fruit and vegetable classification presents significant challenges due to interclass similarities and irregular intraclass characteristics. Selection of appropriate data acquisition sensors and feature representation approach is also crucial due to the huge diversity of the field. Fruit and vegetable classification methods have been developed for quality assessment and robotic harvesting but the current state-of-the-art has been developed for limited classes and small datasets. The problem is of a multi-dimensional nature and offers significantly hyperdimensional features, which is one of the major challenges with current machine learning approaches. Substantial research has been conducted for the design and analysis of classifiers for hyperdimensional features which require significant computational power to optimise with such features. In recent years numerous machine learning techniques for example, Support Vector Machine (SVM), K-Nearest Neighbour (KNN), Decision Trees, Artificial Neural Networks (ANN) and Convolutional Neural Networks (CNN) have been exploited with many different feature description methods for fruit and vegetable classification in many real-life applications. This paper presents a critical comparison of different state-of-the-art computer vision methods proposed by researchers for classifying fruit and vegetable. (C) 2018 Elsevier B.V. All rights reserved.",
          "Keywords Plus": "OBJECT RECOGNITION; FIRMNESS MEASUREMENT; TEXTURE FEATURES; CHILLING INJURY; COLOR; SHAPE; VISION; SEGMENTATION; IMAGES; QUALITY"
        }
      }
    ],
    "YOLO": [
      {
        "title": "Evaluating the Single-Shot MultiBox Detector and YOLO Deep Learning Models for the Detection of Tomatoes in a Greenhouse",
        "year": "2021",
        "algorithm": "single-shot, YOLO",
        "performance": "",
        "citation": "@article{magalhaes2021evaluating,\n  title={Evaluating the single-shot multibox detector and YOLO deep learning models for the detection of tomatoes in a greenhouse},\n  author={Magalh{\\~a}es, Sandro Augusto and Castro, Lu{\\'\\i}s and Moreira, Germano and Dos Santos, Filipe Neves and Cunha, M{\\'a}rio and Dias, Jorge and Moreira, Ant{\\'o}nio Paulo},\n  journal={Sensors},\n  volume={21},\n  number={10},\n  pages={3569},\n  year={2021},\n  publisher={MDPI}\n}\n",
        "fruit_veg": "tomato",
        "paper": {
          "relevant": "y",
          "": "\\cite{magalhaes2021evaluating}",
          "Article Title": "Evaluating the Single-Shot MultiBox Detector and YOLO Deep Learning Models for the Detection of Tomatoes in a Greenhouse",
          "Times Cited, All Databases": "66",
          "Publication Year": "2021",
          "Highly Cited Status": "",
          "Publisher": "",
          "Citation": "@article{magalhaes2021evaluating,\n  title={Evaluating the single-shot multibox detector and YOLO deep learning models for the detection of tomatoes in a greenhouse},\n  author={Magalh{\\~a}es, Sandro Augusto and Castro, Lu{\\'\\i}s and Moreira, Germano and Dos Santos, Filipe Neves and Cunha, M{\\'a}rio and Dias, Jorge and Moreira, Ant{\\'o}nio Paulo},\n  journal={Sensors},\n  volume={21},\n  number={10},\n  pages={3569},\n  year={2021},\n  publisher={MDPI}\n}\n",
          "Document Type": "Article",
          "Main Contribution": "detection",
          "fruit/veg": "tomato",
          "Data Modality": "",
          "Learning Algorithm": "single-shot, YOLO",
          "Locomotion": "",
          "Performance": "",
          "challenges": "",
          "Authors": "Magalhaes, SA; Castro, L; Moreira, G; dos Santos, FN; Cunha, M; Dias, J; Moreira, AP",
          "Abstract": "The development of robotic solutions for agriculture requires advanced perception capabilities that can work reliably in any crop stage. For example, to automatise the tomato harvesting process in greenhouses, the visual perception system needs to detect the tomato in any life cycle stage (flower to the ripe tomato). The state-of-the-art for visual tomato detection focuses mainly on ripe tomato, which has a distinctive colour from the background. This paper contributes with an annotated visual dataset of green and reddish tomatoes. This kind of dataset is uncommon and not available for research purposes. This will enable further developments in edge artificial intelligence for in situ and in real-time visual tomato detection required for the development of harvesting robots. Considering this dataset, five deep learning models were selected, trained and benchmarked to detect green and reddish tomatoes grown in greenhouses. Considering our robotic platform specifications, only the Single-Shot MultiBox Detector (SSD) and YOLO architectures were considered. The results proved that the system can detect green and reddish tomatoes, even those occluded by leaves. SSD MobileNet v2 had the best performance when compared against SSD Inception v2, SSD ResNet 50, SSD ResNet 101 and YOLOv4 Tiny, reaching an F1-score of 66.15%, an mAP of 51.46% and an inference time of 16.44 ms with the NVIDIA Turing Architecture platform, an NVIDIA Tesla T4, with 12 GB. YOLOv4 Tiny also had impressive results, mainly concerning inferring times of about 5 ms.",
          "Keywords Plus": "DETECTION ALGORITHM; RECOGNITION; SCENE"
        }
      }
    ]
  },
  "performance_data": [
    {
      "title": "Harvesting Robots for High-value Crops: State-of-the-art Rev...",
      "year": "2014",
      "algorithm": "",
      "algorithm_family": "Other",
      "accuracy": [],
      "precision": [],
      "recall": [],
      "percentages": [
        "85",
        "75",
        "66",
        "5",
        "45"
      ],
      "performance_text": "localization success was 85%, detachment success was 75%, harvest success was 66%, fruit damage was 5%, peduncle damage was 45%, and cycle time was 33 s",
      "citation": "@article{bac2014harvesting,\n  title={Harvesting robots for high-value crops: State-of-the-art review and challenges ahead},\n  author={Bac, C Wouter and Van Henten, Eldert J and Hemming, Jochen and Edan, Yael},\n  journal={Journal of field robotics},\n  volume={31},\n  number={6},\n  pages={888--911},\n  year={2014},\n  publisher={Wiley Online Library}\n}\n",
      "fruit_veg": ""
    },
    {
      "title": "Fruit detection for strawberry harvesting robot in non-struc...",
      "year": "2019",
      "algorithm": "Mask-RCNN,Resnet50 backbone",
      "algorithm_family": "Mask R-CNN",
      "accuracy": [],
      "precision": [
        "95.78"
      ],
      "recall": [],
      "percentages": [
        "95.78",
        "95.41"
      ],
      "performance_text": "An average detection precision rate of 95.78%, the recall rate was 95.41%.",
      "citation": "@article{yu2019fruit,\n  title={Fruit detection for strawberry harvesting robot in non-structural environment based on Mask-RCNN},\n  author={Yu, Yang and Zhang, Kailiang and Yang, Li and Zhang, Dongxing},\n  journal={Computers and Electronics in Agriculture},\n  volume={163},\n  pages={104846},\n  year={2019},\n  publisher={Elsevier}\n}\n",
      "fruit_veg": "strawberry"
    },
    {
      "title": "Deep Count: Fruit Counting Based on Deep Simulated Learning",
      "year": "2017",
      "algorithm": "Inception-ResNet",
      "algorithm_family": "Other",
      "accuracy": [],
      "precision": [],
      "recall": [],
      "percentages": [
        "91",
        "93"
      ],
      "performance_text": "a 91% average test accuracy on real images and 93% on synthetic images.",
      "citation": "@article{rahnemoonfar2017deep,\n  title={Deep count: fruit counting based on deep simulated learning},\n  author={Rahnemoonfar, Maryam and Sheppard, Clay},\n  journal={Sensors},\n  volume={17},\n  number={4},\n  pages={905},\n  year={2017},\n  publisher={MDPI}\n}\n",
      "fruit_veg": ""
    },
    {
      "title": "Design, integration, and field evaluation of a robotic apple...",
      "year": "2017",
      "algorithm": "",
      "algorithm_family": "Other",
      "accuracy": [],
      "precision": [],
      "recall": [],
      "percentages": [
        "84"
      ],
      "performance_text": "Localization time of 1.5s, a picking time of 6s; a success pick rate of 84%.",
      "citation": "@article{silwal2017design,\n  title={Design, integration, and field evaluation of a robotic apple harvester},\n  author={Silwal, Abhisesh and Davidson, Joseph R and Karkee, Manoj and Mo, Changki and Zhang, Qin and Lewis, Karen},\n  journal={Journal of Field Robotics},\n  volume={34},\n  number={6},\n  pages={1140--1159},\n  year={2017},\n  publisher={Wiley Online Library}\n}\n",
      "fruit_veg": "apples"
    },
    {
      "title": "Detection and segmentation of overlapped fruits based on opt...",
      "year": "2020",
      "algorithm": "Mask-RCNN",
      "algorithm_family": "Mask R-CNN",
      "accuracy": [],
      "precision": [],
      "recall": [],
      "percentages": [
        "97.31",
        "95.70"
      ],
      "performance_text": "The Precision Rate has reached 97.31%, and the Recall Rate has reached 95.70%.",
      "citation": "@article{jia2020detection,\n  title={Detection and segmentation of overlapped fruits based on optimized mask R-CNN application in apple harvesting robot},\n  author={Jia, Weikuan and Tian, Yuyu and Luo, Rong and Zhang, Zhonghua and Lian, Jian and Zheng, Yuanjie},\n  journal={Computers and Electronics in Agriculture},\n  volume={172},\n  pages={105380},\n  year={2020},\n  publisher={Elsevier}\n}",
      "fruit_veg": ""
    },
    {
      "title": "Faster R-CNN-based apple detection in dense-foliage fruiting...",
      "year": "2020",
      "algorithm": "Faster R-CNN",
      "algorithm_family": "Faster R-CNN",
      "accuracy": [],
      "precision": [
        "0.893"
      ],
      "recall": [],
      "percentages": [],
      "performance_text": "a precision of 0.893",
      "citation": "@article{fu2020faster,\n  title={Faster R--CNN--based apple detection in dense-foliage fruiting-wall trees using RGB and depth features for robotic harvesting},\n  author={Fu, Longsheng and Majeed, Yaqoob and Zhang, Xin and Karkee, Manoj and Zhang, Qin},\n  journal={Biosystems Engineering},\n  volume={197},\n  pages={245--256},\n  year={2020},\n  publisher={Elsevier}\n}\n",
      "fruit_veg": "apples"
    },
    {
      "title": "A detection algorithm for cherry fruits based on the improve...",
      "year": "2023",
      "algorithm": "improved YOLOv4",
      "algorithm_family": "YOLOv4",
      "accuracy": [
        "0.15"
      ],
      "precision": [],
      "recall": [],
      "percentages": [],
      "performance_text": "an accuracy of 0.15 higher than that of YOLOv4",
      "citation": "@article{gai2023detection,\n  title={A detection algorithm for cherry fruits based on the improved YOLO-v4 model},\n  author={Gai, Rongli and Chen, Na and Yuan, Hai},\n  journal={Neural Computing and Applications},\n  volume={35},\n  number={19},\n  pages={13895--13906},\n  year={2023},\n  publisher={Springer}\n}",
      "fruit_veg": "cherry"
    },
    {
      "title": "Detection of red and bicoloured apples on tree with an RGB-D...",
      "year": "2016",
      "algorithm": "",
      "algorithm_family": "Other",
      "accuracy": [],
      "precision": [],
      "recall": [],
      "percentages": [
        "100",
        "82"
      ],
      "performance_text": "100% of the fully visible apples and 82% of the partially occluded apples",
      "citation": "@article{nguyen2016detection,\n  title={Detection of red and bicoloured apples on tree with an RGB-D camera},\n  author={Nguyen, Tien Thanh and Vandevoorde, Koenraad and Wouters, Niels and Kayacan, Erdal and De Baerdemaeker, Josse G and Saeys, Wouter},\n  journal={Biosystems Engineering},\n  volume={146},\n  pages={33--44},\n  year={2016},\n  publisher={Elsevier}\n}\n",
      "fruit_veg": "apples"
    },
    {
      "title": "Detection of Fruit-Bearing Branches and Localization of Litc...",
      "year": "2020",
      "algorithm": "",
      "algorithm_family": "Other",
      "accuracy": [
        "83.3"
      ],
      "precision": [],
      "recall": [],
      "percentages": [],
      "performance_text": "an accuracy of 83.3\\%, detection time of 0.464s",
      "citation": "@article{li2020detection,\n  title={Detection of fruit-bearing branches and localization of litchi clusters for vision-based harvesting robots},\n  author={Li, Jinhui and Tang, Yunchao and Zou, Xiangjun and Lin, Guichao and Wang, Hongjun},\n  journal={IEEE Access},\n  volume={8},\n  pages={117746--117758},\n  year={2020},\n  publisher={IEEE}\n}\n",
      "fruit_veg": "Litch"
    },
    {
      "title": "Detecting tomatoes in greenhouse scenes by combining AdaBoos...",
      "year": "2016",
      "algorithm": "adaboost classifier and color",
      "algorithm_family": "Other",
      "accuracy": [],
      "precision": [],
      "recall": [],
      "percentages": [
        "96"
      ],
      "performance_text": "over 96% of ripe tomatoes",
      "citation": "@article{zhao2016detecting,\n  title={Detecting tomatoes in greenhouse scenes by combining AdaBoost classifier and colour analysis},\n  author={Zhao, Yuanshen and Gong, Liang and Zhou, Bin and Huang, Yixiang and Liu, Chengliang},\n  journal={Biosystems Engineering},\n  volume={148},\n  pages={127--137},\n  year={2016},\n  publisher={Elsevier}\n}\n",
      "fruit_veg": "tomato"
    }
  ]
}