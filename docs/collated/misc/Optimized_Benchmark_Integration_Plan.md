#!/usr/bin/env python3
"""
çœŸå®æ•°æ®é€‚é…å™¨

å°†WiFi-CSI-Sensing-Benchmarkçš„æ•°æ®é›†é€‚é…åˆ°æˆ‘ä»¬ç°æœ‰çš„è®­ç»ƒæ¡†æ¶ä¸­ï¼Œ
ä¸“æ³¨äºSim2RealéªŒè¯ï¼Œè€Œéæ¨¡å‹æ¶æ„å¯¹æ¯”ã€‚

æ ¸å¿ƒåŠŸèƒ½:
1. å°†çœŸå®æ•°æ®é›†è½¬æ¢ä¸ºæˆ‘ä»¬çš„æ•°æ®æ ¼å¼
2. æ”¯æŒä¸åˆæˆæ•°æ®çš„æ··åˆè®­ç»ƒ
3. å®ç°å°‘æ ·æœ¬å­¦ä¹ å®éªŒ
4. æœ€å°åŒ–å¯¹ç°æœ‰ä»£ç çš„ä¿®æ”¹

ä½œè€…: AI Assistant  
æ—¥æœŸ: 2025-01-16
"""

import sys
import torch
import numpy as np
from pathlib import Path
from torch.utils.data import DataLoader, TensorDataset, Dataset
import pickle

# æ·»åŠ benchmarkè·¯å¾„
BENCHMARK_DIR = Path(__file__).parent.parent / "benchmarks" / "wifi_csi_benchmark"
sys.path.append(str(BENCHMARK_DIR))

# æ·»åŠ æˆ‘ä»¬çš„æºç è·¯å¾„
SRC_DIR = Path(__file__).parent.parent / "src"
sys.path.append(str(SRC_DIR))


class RealDataAdapter:
    """çœŸå®æ•°æ®é€‚é…å™¨ - å°†benchmarkæ•°æ®é›†é€‚é…åˆ°æˆ‘ä»¬çš„è®­ç»ƒæ¡†æ¶"""
    
    def __init__(self, cache_dir="cache/real_data"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # æ”¯æŒçš„æ•°æ®é›†é…ç½®
        self.dataset_configs = {
            "UT_HAR": {
                "name": "UT_HAR_data",
                "num_classes": 7,
                "description": "è¡Œä¸ºè¯†åˆ«æ•°æ®é›†"
            },
            "NTU_Fi_HAR": {
                "name": "NTU-Fi_HAR", 
                "num_classes": 6,
                "description": "è¡Œä¸ºè¯†åˆ«æ•°æ®é›†"
            }
        }
    
    def load_real_dataset(self, dataset_name: str, use_cache=True):
        """
        åŠ è½½çœŸå®æ•°æ®é›†å¹¶è½¬æ¢ä¸ºæˆ‘ä»¬çš„æ ¼å¼
        
        Args:
            dataset_name: æ•°æ®é›†åç§° ("UT_HAR" æˆ– "NTU_Fi_HAR")
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            train_loader, test_loader: æˆ‘ä»¬æ ¼å¼çš„æ•°æ®åŠ è½½å™¨
        """
        cache_file = self.cache_dir / f"{dataset_name}_adapted.pkl"
        
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        if use_cache and cache_file.exists():
            print(f"ğŸ“ ä»ç¼“å­˜åŠ è½½ {dataset_name}")
            with open(cache_file, 'rb') as f:
                data = pickle.load(f)
            return self._create_loaders(data)
        
        # ä»benchmarkåŠ è½½åŸå§‹æ•°æ®
        print(f"ğŸ“Š ä»benchmarkåŠ è½½ {dataset_name}")
        config = self.dataset_configs[dataset_name]
        
        try:
            from util import load_data_n_model
            train_loader, test_loader, _, _ = load_data_n_model(
                config["name"], 
                "MLP",  # ä½¿ç”¨æœ€ç®€å•çš„æ¨¡å‹ï¼Œåªæ˜¯ä¸ºäº†è·å–æ•°æ®
                str(BENCHMARK_DIR / "Data")
            )
            
            # æå–æ•°æ®å¼ é‡
            train_data = self._extract_data_from_loader(train_loader)
            test_data = self._extract_data_from_loader(test_loader)
            
            # é€‚é…åˆ°æˆ‘ä»¬çš„æ ¼å¼
            adapted_data = self._adapt_to_our_format(train_data, test_data, config)
            
            # ä¿å­˜åˆ°ç¼“å­˜
            if use_cache:
                with open(cache_file, 'wb') as f:
                    pickle.dump(adapted_data, f)
                print(f"ğŸ’¾ æ•°æ®å·²ç¼“å­˜åˆ° {cache_file}")
            
            return self._create_loaders(adapted_data)
            
        except Exception as e:
            print(f"âŒ åŠ è½½ {dataset_name} å¤±è´¥: {e}")
            print("è¯·ç¡®ä¿:")
            print("1. benchmarkæ•°æ®é›†å·²ä¸‹è½½åˆ°æ­£ç¡®ä½ç½®")
            print("2. æ•°æ®é›†æ ¼å¼æ­£ç¡®")
            raise
    
    def _extract_data_from_loader(self, loader):
        """ä»DataLoaderä¸­æå–æ‰€æœ‰æ•°æ®"""
        X_list, y_list = [], []
        
        for batch in loader:
            X, y = batch
            X_list.append(X)
            y_list.append(y)
        
        X = torch.cat(X_list, dim=0)
        y = torch.cat(y_list, dim=0)
        
        return X, y
    
    def _adapt_to_our_format(self, train_data, test_data, config):
        """å°†æ•°æ®é€‚é…åˆ°æˆ‘ä»¬çš„æ ¼å¼"""
        X_train, y_train = train_data
        X_test, y_test = test_data
        
        print(f"ğŸ“ åŸå§‹æ•°æ®å½¢çŠ¶:")
        print(f"  è®­ç»ƒ: X={X_train.shape}, y={y_train.shape}")
        print(f"  æµ‹è¯•: X={X_test.shape}, y={y_test.shape}")
        
        # ç¡®ä¿æ•°æ®æ ¼å¼ä¸æˆ‘ä»¬çš„åˆæˆæ•°æ®ä¸€è‡´
        # æˆ‘ä»¬çš„æ ¼å¼: [B, T, F] (batch, time, features)
        
        if len(X_train.shape) == 4:  # [B, C, H, W] -> [B, T, F]
            B, C, H, W = X_train.shape
            X_train = X_train.reshape(B, H, C*W)  # [B, T, F]
            X_test = X_test.reshape(X_test.shape[0], H, C*W)
        elif len(X_train.shape) == 3:  # [B, T, F] - å·²ç»æ˜¯æ­£ç¡®æ ¼å¼
            pass
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®å½¢çŠ¶: {X_train.shape}")
        
        # è·å–ç»´åº¦ä¿¡æ¯
        B_train, T, F = X_train.shape
        B_test = X_test.shape[0]
        num_classes = config["num_classes"]
        
        print(f"ğŸ“ é€‚é…åæ•°æ®å½¢çŠ¶:")
        print(f"  è®­ç»ƒ: X={X_train.shape}, y={y_train.shape}")
        print(f"  æµ‹è¯•: X={X_test.shape}, y={y_test.shape}")
        print(f"  å‚æ•°: T={T}, F={F}, num_classes={num_classes}")
        
        return {
            "X_train": X_train.float(),
            "y_train": y_train.long(),
            "X_test": X_test.float(), 
            "y_test": y_test.long(),
            "T": T,
            "F": F,
            "num_classes": num_classes,
            "dataset_info": config
        }
    
    def _create_loaders(self, data):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        train_dataset = TensorDataset(data["X_train"], data["y_train"])
        test_dataset = TensorDataset(data["X_test"], data["y_test"])
        
        train_loader = DataLoader(
            train_dataset, 
            batch_size=64,  # ä¸æˆ‘ä»¬çš„è®¾ç½®ä¿æŒä¸€è‡´
            shuffle=True,
            pin_memory=True
        )
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=256,
            shuffle=False, 
            pin_memory=True
        )
        
        return train_loader, test_loader, data
    
    def create_few_shot_loader(self, data, ratio=0.1):
        """åˆ›å»ºå°‘æ ·æœ¬å­¦ä¹ çš„æ•°æ®åŠ è½½å™¨"""
        X_train, y_train = data["X_train"], data["y_train"]
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°
        unique_classes = torch.unique(y_train)
        few_shot_indices = []
        
        for cls in unique_classes:
            cls_indices = torch.where(y_train == cls)[0]
            n_samples = max(1, int(len(cls_indices) * ratio))
            selected = torch.randperm(len(cls_indices))[:n_samples]
            few_shot_indices.append(cls_indices[selected])
        
        few_shot_indices = torch.cat(few_shot_indices)
        
        X_few_shot = X_train[few_shot_indices]
        y_few_shot = y_train[few_shot_indices]
        
        print(f"ğŸ“Š å°‘æ ·æœ¬æ•°æ®: {len(X_few_shot)} æ ·æœ¬ ({ratio*100:.1f}% of {len(X_train)})")
        
        few_shot_dataset = TensorDataset(X_few_shot, y_few_shot)
        few_shot_loader = DataLoader(
            few_shot_dataset,
            batch_size=32,
            shuffle=True,
            pin_memory=True
        )
        
        return few_shot_loader


def create_sim2real_experiment(dataset_name: str = "UT_HAR"):
    """
    åˆ›å»ºSim2Realå®éªŒçš„æ•°æ®
    
    Args:
        dataset_name: çœŸå®æ•°æ®é›†åç§°
        
    Returns:
        synthetic_loaders: åˆæˆæ•°æ®åŠ è½½å™¨
        real_loaders: çœŸå®æ•°æ®åŠ è½½å™¨  
        metadata: æ•°æ®å…ƒä¿¡æ¯
    """
    print(f"ğŸ”„ åˆ›å»º {dataset_name} Sim2Realå®éªŒæ•°æ®...")
    
    # 1. åŠ è½½çœŸå®æ•°æ®
    adapter = RealDataAdapter()
    real_train_loader, real_test_loader, real_data = adapter.load_real_dataset(dataset_name)
    
    # 2. åˆ›å»ºåŒ¹é…çš„åˆæˆæ•°æ®
    from data_synth import get_synth_loaders
    
    # ä½¿ç”¨çœŸå®æ•°æ®çš„ç»´åº¦åˆ›å»ºåˆæˆæ•°æ®
    T = real_data["T"]
    F = real_data["F"] 
    num_classes = real_data["num_classes"]
    
    print(f"ğŸ“Š åˆ›å»ºåŒ¹é…çš„åˆæˆæ•°æ®: T={T}, F={F}, classes={num_classes}")
    
    synth_train_loader, synth_test_loader = get_synth_loaders(
        n=10000,  # åˆæˆæ•°æ®é‡
        T=T,
        F=F,
        difficulty="hard",
        seed=0,
        num_classes=num_classes
    )
    
    metadata = {
        "dataset": dataset_name,
        "T": T,
        "F": F, 
        "num_classes": num_classes,
        "real_train_size": len(real_data["X_train"]),
        "real_test_size": len(real_data["X_test"]),
        "synth_train_size": 7000,  # é»˜è®¤è®­ç»ƒé›†å¤§å°
        "synth_test_size": 3000    # é»˜è®¤æµ‹è¯•é›†å¤§å°
    }
    
    return {
        "synthetic": (synth_train_loader, synth_test_loader),
        "real": (real_train_loader, real_test_loader),
        "metadata": metadata,
        "adapter": adapter,
        "real_data": real_data
    }


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("ğŸš€ çœŸå®æ•°æ®é€‚é…å™¨æµ‹è¯•")
    
    # æµ‹è¯•æ•°æ®é€‚é…
    try:
        experiment_data = create_sim2real_experiment("UT_HAR")
        print("âœ… Sim2Realå®éªŒæ•°æ®åˆ›å»ºæˆåŠŸ!")
        
        metadata = experiment_data["metadata"] 
        print(f"ğŸ“Š å®éªŒé…ç½®: {metadata}")
        
        # æµ‹è¯•å°‘æ ·æœ¬å­¦ä¹ æ•°æ®
        adapter = experiment_data["adapter"]
        real_data = experiment_data["real_data"]
        
        few_shot_loader = adapter.create_few_shot_loader(real_data, ratio=0.1)
        print("âœ… å°‘æ ·æœ¬å­¦ä¹ æ•°æ®åˆ›å»ºæˆåŠŸ!")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        print("è¿™æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºå¯èƒ½æ²¡æœ‰ä¸‹è½½çœŸå®æ•°æ®é›†")
        print("å®é™…ä½¿ç”¨æ—¶éœ€è¦å…ˆä¸‹è½½æ•°æ®é›†åˆ°benchmark/Data/ç›®å½•")