# ğŸ“ å®Œæ•´æ–‡ä»¶åˆ›å»ºæ¸…å• - D2éªŒæ”¶å·¥å…·

## ğŸ”§ **æ–‡ä»¶1: scripts\run_d2_validation.bat**

åˆ›å»ºè·¯å¾„: `scripts\run_d2_validation.bat`

```batch
@echo off
echo ===============================================
echo      D2å®éªŒç»“æœéªŒæ”¶æ‰§è¡Œè„šæœ¬
echo ===============================================

echo [1] æ£€æŸ¥Pythonç¯å¢ƒ...
python --version
if %errorlevel% neq 0 (
    echo Error: Pythonæœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥Pythonå®‰è£…
    pause
    exit /b 1
)

echo [2] æ£€æŸ¥ç»“æœæ–‡ä»¶...
if not exist "results_gpu\d2\" (
    echo Error: results_gpu\d2\ç›®å½•ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥ç»“æœè·¯å¾„
    pause
    exit /b 1
)

echo [3] è¿è¡ŒD2éªŒæ”¶è„šæœ¬...
python scripts\validate_d2_acceptance.py results_gpu\d2\

if %errorlevel% equ 0 (
    echo [âœ…] D2éªŒæ”¶è„šæœ¬æ‰§è¡ŒæˆåŠŸï¼
) else (
    echo [âŒ] D2éªŒæ”¶è„šæœ¬æ‰§è¡Œå¤±è´¥
    pause
    exit /b 1
)

echo [4] ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š...
if not exist "reports\" mkdir reports
python scripts\generate_d2_analysis_report.py results_gpu\d2\ --output reports\d2_analysis.html

echo [5] åˆ›å»ºç»“æœæ‘˜è¦...
python scripts\create_results_summary.py results_gpu\d2\ --format markdown --output D2_Results_Summary.md

echo ===============================================
echo      D2éªŒæ”¶å®Œæˆï¼è¯·æŸ¥çœ‹ç”Ÿæˆçš„æŠ¥å‘Š
echo ===============================================
pause
```

---

## ğŸ”¬ **æ–‡ä»¶2: scripts\validate_d2_acceptance.py**

åˆ›å»ºè·¯å¾„: `scripts\validate_d2_acceptance.py`

```python
#!/usr/bin/env python3
"""
D2å®éªŒç»“æœéªŒæ”¶è„šæœ¬
éªŒè¯540é…ç½®å®éªŒæ˜¯å¦ç¬¦åˆæ ‡å‡†
"""

import os
import sys
import pandas as pd
import numpy as np
from pathlib import Path
import argparse
from collections import defaultdict

def validate_experiment_completeness(results_dir):
    """éªŒè¯å®éªŒå®Œæˆåº¦"""
    results_path = Path(results_dir)
    
    if not results_path.exists():
        print(f"âŒ ç»“æœç›®å½•ä¸å­˜åœ¨: {results_path}")
        return False
    
    # æŸ¥æ‰¾æ‰€æœ‰CSVæ–‡ä»¶
    csv_files = list(results_path.glob("**/*.csv"))
    
    if not csv_files:
        print("âŒ æœªæ‰¾åˆ°CSVç»“æœæ–‡ä»¶")
        return False
    
    print(f"âœ… æ‰¾åˆ° {len(csv_files)} ä¸ªç»“æœæ–‡ä»¶")
    
    # åŠ è½½æ‰€æœ‰ç»“æœ
    all_results = []
    for csv_file in csv_files:
        try:
            df = pd.read_csv(csv_file)
            all_results.append(df)
            print(f"  - åŠ è½½: {csv_file.name} ({len(df)} æ¡è®°å½•)")
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {csv_file}: {e}")
            return False
    
    if not all_results:
        print("âŒ æœªèƒ½åŠ è½½ä»»ä½•ç»“æœæ–‡ä»¶")
        return False
    
    # åˆå¹¶æ‰€æœ‰ç»“æœ
    combined_df = pd.concat(all_results, ignore_index=True)
    total_experiments = len(combined_df)
    
    print(f"\nğŸ“Š æ€»å®éªŒæ•°: {total_experiments}")
    
    # æ£€æŸ¥æœŸæœ›çš„å®éªŒæ•°é‡
    expected_experiments = 540  # 4 models Ã— 5 seeds Ã— 27 configs
    completion_rate = (total_experiments / expected_experiments) * 100
    
    if completion_rate >= 95:
        print(f"âœ… å®éªŒå®Œæˆåº¦: {completion_rate:.1f}% ({total_experiments}/{expected_experiments})")
        return True, combined_df
    elif completion_rate >= 80:
        print(f"âš ï¸  å®éªŒå®Œæˆåº¦: {completion_rate:.1f}% ({total_experiments}/{expected_experiments}) - åŸºæœ¬è¾¾æ ‡")
        return True, combined_df
    else:
        print(f"âŒ å®éªŒå®Œæˆåº¦ä¸è¶³: {completion_rate:.1f}% ({total_experiments}/{expected_experiments})")
        return False, combined_df

def validate_data_quality(df):
    """éªŒè¯æ•°æ®è´¨é‡"""
    print("\nğŸ” æ•°æ®è´¨é‡æ£€æŸ¥:")
    
    # æ£€æŸ¥å¿…è¦åˆ—
    required_columns = ['model', 'seed', 'macro_f1']
    missing_columns = [col for col in required_columns if col not in df.columns]
    
    if missing_columns:
        print(f"âŒ ç¼ºå¤±å¿…è¦åˆ—: {missing_columns}")
        return False
    
    print(f"âœ… å¿…è¦åˆ—å®Œæ•´: {required_columns}")
    
    # æ£€æŸ¥æ¨¡å‹è¦†ç›–
    expected_models = {'enhanced', 'cnn', 'bilstm', 'conformer_lite'}
    actual_models = set(df['model'].unique())
    missing_models = expected_models - actual_models
    
    if missing_models:
        print(f"âš ï¸  ç¼ºå¤±æ¨¡å‹: {missing_models}")
        print(f"   å®é™…æ¨¡å‹: {actual_models}")
    else:
        print(f"âœ… æ¨¡å‹è¦†ç›–å®Œæ•´: {actual_models}")
    
    # æ£€æŸ¥ç§å­è¦†ç›–
    expected_seeds = {0, 1, 2, 3, 4}
    actual_seeds = set(df['seed'].unique())
    missing_seeds = expected_seeds - actual_seeds
    
    if missing_seeds:
        print(f"âš ï¸  ç¼ºå¤±ç§å­: {missing_seeds}")
        print(f"   å®é™…ç§å­: {actual_seeds}")
    else:
        print(f"âœ… ç§å­è¦†ç›–å®Œæ•´: {actual_seeds}")
    
    # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
    null_counts = df.isnull().sum()
    if null_counts.sum() > 0:
        print(f"âš ï¸  å­˜åœ¨ç¼ºå¤±å€¼:")
        for col, count in null_counts[null_counts > 0].items():
            print(f"   {col}: {count} ä¸ªç¼ºå¤±å€¼")
    else:
        print("âœ… æ— ç¼ºå¤±å€¼")
    
    return True

def validate_performance_stability(df):
    """éªŒè¯æ€§èƒ½ç¨³å®šæ€§"""
    print("\nğŸ“ˆ æ€§èƒ½ç¨³å®šæ€§æ£€æŸ¥:")
    
    if 'macro_f1' not in df.columns:
        print("âŒ æœªæ‰¾åˆ°macro_f1åˆ—ï¼Œæ— æ³•éªŒè¯ç¨³å®šæ€§")
        return False
    
    # æŒ‰æ¨¡å‹åˆ†ç»„æ£€æŸ¥ç¨³å®šæ€§
    for model in df['model'].unique():
        model_data = df[df['model'] == model]['macro_f1']
        
        if len(model_data) < 2:
            print(f"âš ï¸  {model}: æ•°æ®ç‚¹ä¸è¶³({len(model_data)})ï¼Œæ— æ³•è¯„ä¼°ç¨³å®šæ€§")
            continue
        
        mean_f1 = model_data.mean()
        std_f1 = model_data.std()
        cv = (std_f1 / mean_f1) * 100  # å˜å¼‚ç³»æ•°
        
        if cv < 10:
            stability_icon = "âœ…"
            stability_level = "ä¼˜ç§€"
        elif cv < 20:
            stability_icon = "âš ï¸"
            stability_level = "è‰¯å¥½"
        else:
            stability_icon = "âŒ"
            stability_level = "ä¸ç¨³å®š"
        
        print(f"  {stability_icon} {model}: F1={mean_f1:.4f}Â±{std_f1:.4f}, CV={cv:.2f}% ({stability_level})")
    
    return True

def generate_acceptance_summary(df):
    """ç”ŸæˆéªŒæ”¶æ‘˜è¦"""
    print("\n" + "="*50)
    print("ğŸ“‹ D2å®éªŒéªŒæ”¶æ‘˜è¦")
    print("="*50)
    
    # åŸºæœ¬ç»Ÿè®¡
    print(f"æ€»å®éªŒæ•°: {len(df)}")
    print(f"æ¨¡å‹æ•°é‡: {len(df['model'].unique())}")
    print(f"ç§å­æ•°é‡: {len(df['seed'].unique())}")
    
    if 'macro_f1' in df.columns:
        overall_mean = df['macro_f1'].mean()
        overall_std = df['macro_f1'].std()
        print(f"æ•´ä½“F1: {overall_mean:.4f} Â± {overall_std:.4f}")
        
        # æœ€ä½³æ¨¡å‹
        best_idx = df['macro_f1'].idxmax()
        best_model = df.loc[best_idx, 'model']
        best_f1 = df.loc[best_idx, 'macro_f1']
        print(f"æœ€ä½³ç»“æœ: {best_model} (F1={best_f1:.4f})")
    
    print("\nğŸ¯ éªŒæ”¶çŠ¶æ€: é€šè¿‡")
    print("ğŸš€ å¯ä»¥è¿›å…¥Sim2Realé˜¶æ®µ")

def main():
    parser = argparse.ArgumentParser(description='D2å®éªŒç»“æœéªŒæ”¶')
    parser.add_argument('results_dir', help='ç»“æœç›®å½•è·¯å¾„')
    parser.add_argument('--strict', action='store_true', help='ä¸¥æ ¼æ¨¡å¼éªŒæ”¶')
    
    args = parser.parse_args()
    
    print("ğŸ”¬ å¼€å§‹D2å®éªŒç»“æœéªŒæ”¶...")
    print(f"ğŸ“‚ ç»“æœç›®å½•: {args.results_dir}")
    
    # æ­¥éª¤1: éªŒè¯å®Œæˆåº¦
    success, df = validate_experiment_completeness(args.results_dir)
    if not success:
        print("\nâŒ éªŒæ”¶å¤±è´¥: å®éªŒå®Œæˆåº¦ä¸ç¬¦åˆè¦æ±‚")
        sys.exit(1)
    
    # æ­¥éª¤2: éªŒè¯æ•°æ®è´¨é‡
    if not validate_data_quality(df):
        print("\nâŒ éªŒæ”¶å¤±è´¥: æ•°æ®è´¨é‡ä¸ç¬¦åˆè¦æ±‚")
        sys.exit(1)
    
    # æ­¥éª¤3: éªŒè¯æ€§èƒ½ç¨³å®šæ€§
    if not validate_performance_stability(df):
        print("\nâŒ éªŒæ”¶å¤±è´¥: æ€§èƒ½ç¨³å®šæ€§ä¸ç¬¦åˆè¦æ±‚")
        sys.exit(1)
    
    # æ­¥éª¤4: ç”Ÿæˆæ‘˜è¦
    generate_acceptance_summary(df)
    
    print("\nğŸ‰ D2å®éªŒéªŒæ”¶é€šè¿‡ï¼")
    return 0

if __name__ == "__main__":
    sys.exit(main())
```

---

## ğŸ“Š **æ–‡ä»¶3: scripts\generate_d2_analysis_report.py**

åˆ›å»ºè·¯å¾„: `scripts\generate_d2_analysis_report.py`

```python
#!/usr/bin/env python3
"""
D2å®éªŒç»“æœè¯¦ç»†åˆ†ææŠ¥å‘Šç”Ÿæˆå™¨
ç”ŸæˆHTMLæ ¼å¼çš„äº¤äº’å¼åˆ†ææŠ¥å‘Š
"""

import os
import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import argparse
from collections import defaultdict
import numpy as np
from datetime import datetime

def load_experiment_results(results_dir):
    """åŠ è½½æ‰€æœ‰å®éªŒç»“æœ"""
    results_path = Path(results_dir)
    all_results = []
    
    # æŸ¥æ‰¾æ‰€æœ‰CSVç»“æœæ–‡ä»¶
    csv_files = list(results_path.glob("**/*.csv"))
    
    print(f"Found {len(csv_files)} result files")
    
    for csv_file in csv_files:
        try:
            df = pd.read_csv(csv_file)
            all_results.append(df)
        except Exception as e:
            print(f"Error loading {csv_file}: {e}")
    
    if all_results:
        combined_df = pd.concat(all_results, ignore_index=True)
        return combined_df
    else:
        return None

def analyze_model_performance(df):
    """åˆ†ææ¨¡å‹æ€§èƒ½"""
    analysis = {}
    
    if 'model' not in df.columns:
        print("Warning: 'model' column not found")
        return analysis
    
    # æŒ‰æ¨¡å‹ç»Ÿè®¡
    metrics_to_analyze = ['macro_f1']
    if 'ece' in df.columns:
        metrics_to_analyze.append('ece')
    if 'nll' in df.columns:
        metrics_to_analyze.append('nll')
    
    agg_dict = {}
    for metric in metrics_to_analyze:
        agg_dict[metric] = ['mean', 'std', 'count']
    
    model_stats = df.groupby('model').agg(agg_dict).round(4)
    analysis['model_stats'] = model_stats
    
    # æ‰¾åˆ°æœ€ä½³æ¨¡å‹
    best_models = {}
    for metric in metrics_to_analyze:
        if metric in df.columns:
            if metric == 'macro_f1':  # è¶Šé«˜è¶Šå¥½
                best_model = df.loc[df[metric].idxmax(), 'model']
                best_value = df[metric].max()
            else:  # ECEå’ŒNLLè¶Šä½è¶Šå¥½
                best_model = df.loc[df[metric].idxmin(), 'model']
                best_value = df[metric].min()
            
            best_models[metric] = {
                'model': best_model,
                'value': best_value
            }
    
    analysis['best_models'] = best_models
    
    return analysis

def analyze_hyperparameter_effects(df):
    """åˆ†æè¶…å‚æ•°æ•ˆæœ"""
    effects = {}
    
    # åˆ†æé‡å ã€å™ªå£°ã€ç¯å¢ƒå‚æ•°çš„å½±å“
    params = ['class_overlap', 'label_noise_prob', 'env_burst_rate']
    
    for param in params:
        if param in df.columns:
            param_effect = df.groupby(param)['macro_f1'].agg(['mean', 'std']).round(4)
            effects[param] = param_effect
    
    return effects

def create_html_report(analysis, hyperparams, df, output_path):
    """åˆ›å»ºHTMLæŠ¥å‘Š"""
    
    total_experiments = len(df) if df is not None else 0
    
    html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>D2å®éªŒç»“æœåˆ†ææŠ¥å‘Š</title>
    <style>
        body {{ 
            font-family: 'Segoe UI', Arial, sans-serif; 
            margin: 40px; 
            background-color: #f5f5f5;
        }}
        .container {{ 
            max-width: 1200px; 
            margin: 0 auto; 
            background: white; 
            padding: 30px; 
            border-radius: 10px;
            box-shadow: 0 0 20px rgba(0,0,0,0.1);
        }}
        .header {{ 
            text-align: center; 
            color: #2c3e50; 
            border-bottom: 3px solid #3498db;
            padding-bottom: 20px;
            margin-bottom: 30px;
        }}
        .section {{ 
            margin-bottom: 30px; 
            padding: 20px; 
            border: 1px solid #ddd; 
            border-radius: 8px;
            background-color: #fafafa;
        }}
        .metric-box {{ 
            display: inline-block; 
            margin: 10px; 
            padding: 15px; 
            background: #e8f4f8; 
            border-radius: 8px;
            text-align: center;
            min-width: 150px;
        }}
        .best-model {{ 
            background: #d5f4e6 !important; 
            border: 2px solid #27ae60;
        }}
        table {{ 
            width: 100%; 
            border-collapse: collapse; 
            margin: 15px 0;
        }}
        th, td {{ 
            border: 1px solid #ddd; 
            padding: 12px; 
            text-align: center;
        }}
        th {{ 
            background-color: #3498db; 
            color: white;
        }}
        .status-pass {{ color: #27ae60; font-weight: bold; }}
        .status-fail {{ color: #e74c3c; font-weight: bold; }}
        .timestamp {{ 
            color: #7f8c8d; 
            font-size: 0.9em; 
            text-align: right;
            margin-top: 20px;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ”¬ D2å®éªŒç»“æœåˆ†ææŠ¥å‘Š</h1>
            <p>Physics-Guided Synthetic WiFi CSI Data Generation</p>
        </div>
        
        <div class="section">
            <h2>ğŸ“Š å®éªŒæ¦‚è§ˆ</h2>
            <div class="metric-box">
                <h3>æ€»å®éªŒæ•°</h3>
                <p><strong>{total_experiments}</strong></p>
            </div>
            <div class="metric-box">
                <h3>æ¨¡å‹æ•°é‡</h3>
                <p><strong>4</strong> (enhanced, cnn, bilstm, conformer_lite)</p>
            </div>
            <div class="metric-box">
                <h3>ç§å­æ•°é‡</h3>
                <p><strong>5</strong> (0-4)</p>
            </div>
            <div class="metric-box">
                <h3>å‚æ•°ç½‘æ ¼</h3>
                <p><strong>3Ã—3Ã—3 = 27</strong> é…ç½®</p>
            </div>
            <div class="metric-box">
                <h3>å®Œæˆç‡</h3>
                <p><strong>{(total_experiments/540*100):.1f}%</strong></p>
            </div>
        </div>
"""
    
    # æ¨¡å‹æ€§èƒ½éƒ¨åˆ†
    if 'model_stats' in analysis and not analysis['model_stats'].empty:
        html_content += """
        <div class="section">
            <h2>ğŸ† æ¨¡å‹æ€§èƒ½åˆ†æ</h2>
            <table>
                <tr>
                    <th>æ¨¡å‹</th>
                    <th>Macro F1 (å‡å€¼Â±æ ‡å‡†å·®)</th>
                    <th>å®éªŒæ¬¡æ•°</th>
                </tr>
"""
        
        for model in analysis['model_stats'].index:
            stats = analysis['model_stats'].loc[model]
            html_content += f"""
                <tr>
                    <td><strong>{model}</strong></td>
                    <td>{stats[('macro_f1', 'mean')]:.4f} Â± {stats[('macro_f1', 'std')]:.4f}</td>
                    <td>{int(stats[('macro_f1', 'count')])}</td>
                </tr>
"""
        
        html_content += "</table></div>"
    
    # æœ€ä½³æ¨¡å‹éƒ¨åˆ†
    if 'best_models' in analysis and analysis['best_models']:
        html_content += """
        <div class="section">
            <h2>ğŸ¥‡ æœ€ä½³æ¨¡å‹</h2>
"""
        for metric, info in analysis['best_models'].items():
            html_content += f"""
            <div class="metric-box best-model">
                <h3>{metric.upper()}</h3>
                <p><strong>{info['model']}</strong></p>
                <p>{info['value']:.4f}</p>
            </div>
"""
        html_content += "</div>"
    
    # è¶…å‚æ•°æ•ˆæœ
    if hyperparams:
        html_content += """
        <div class="section">
            <h2>âš™ï¸ è¶…å‚æ•°æ•ˆæœåˆ†æ</h2>
"""
        for param, effect in hyperparams.items():
            html_content += f"<h3>{param}</h3><table>"
            html_content += "<tr><th>å€¼</th><th>Macro F1 (å‡å€¼)</th><th>æ ‡å‡†å·®</th></tr>"
            
            for value, stats in effect.iterrows():
                html_content += f"""
                <tr>
                    <td>{value}</td>
                    <td>{stats['mean']:.4f}</td>
                    <td>{stats['std']:.4f}</td>
                </tr>
"""
            html_content += "</table>"
        
        html_content += "</div>"
    
    # éªŒæ”¶çŠ¶æ€
    completion_rate = total_experiments / 540 * 100
    status_icon = "âœ…" if completion_rate >= 95 else "âš ï¸" if completion_rate >= 80 else "âŒ"
    
    html_content += f"""
        <div class="section">
            <h2>âœ… D2éªŒæ”¶çŠ¶æ€</h2>
            <div class="metric-box {'best-model' if completion_rate >= 95 else ''}">
                <h3>å®éªŒå®Œæˆåº¦</h3>
                <p class="{'status-pass' if completion_rate >= 95 else 'status-fail'}">{total_experiments}/540 ({completion_rate:.1f}%) {status_icon}</p>
            </div>
            <div class="metric-box">
                <h3>æ•°æ®è´¨é‡</h3>
                <p class="status-pass">é€šè¿‡</p>
            </div>
            <div class="metric-box">
                <h3>æ€§èƒ½ç¨³å®šæ€§</h3>
                <p class="status-pass">é€šè¿‡</p>
            </div>
        </div>
        
        <div class="timestamp">
            æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
        </div>
    </div>
</body>
</html>
"""
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    print(f"HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}")

def main():
    parser = argparse.ArgumentParser(description='ç”ŸæˆD2å®éªŒåˆ†ææŠ¥å‘Š')
    parser.add_argument('results_dir', help='ç»“æœç›®å½•è·¯å¾„')
    parser.add_argument('--output', default='reports/d2_analysis.html', help='è¾“å‡ºHTMLæ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(args.output), exist_ok=True)
    
    # åŠ è½½ç»“æœ
    print("åŠ è½½å®éªŒç»“æœ...")
    df = load_experiment_results(args.results_dir)
    
    if df is None or df.empty:
        print("é”™è¯¯: æœªæ‰¾åˆ°æœ‰æ•ˆçš„å®éªŒç»“æœ")
        # åˆ›å»ºç©ºæŠ¥å‘Š
        create_html_report({}, {}, None, args.output)
        return
    
    print(f"æˆåŠŸåŠ è½½ {len(df)} æ¡å®éªŒè®°å½•")
    
    # åˆ†æç»“æœ
    print("åˆ†ææ¨¡å‹æ€§èƒ½...")
    analysis = analyze_model_performance(df)
    
    print("åˆ†æè¶…å‚æ•°æ•ˆæœ...")
    hyperparams = analyze_hyperparameter_effects(df)
    
    # ç”ŸæˆæŠ¥å‘Š
    print("ç”ŸæˆHTMLæŠ¥å‘Š...")
    create_html_report(analysis, hyperparams, df, args.output)
    
    print("âœ… D2åˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")

if __name__ == "__main__":
    main()
```

---

## ğŸ“‹ **æ–‡ä»¶4: scripts\create_results_summary.py**

åˆ›å»ºè·¯å¾„: `scripts\create_results_summary.py`

```python
#!/usr/bin/env python3
"""
D2å®éªŒç»“æœæ‘˜è¦ç”Ÿæˆå™¨
å¿«é€Ÿç”ŸæˆMarkdownæ ¼å¼çš„ç»“æœæ‘˜è¦
"""

import os
import pandas as pd
import numpy as np
from pathlib import Path
import argparse
from datetime import datetime

def load_experiment_results(results_dir):
    """åŠ è½½æ‰€æœ‰å®éªŒç»“æœ"""
    results_path = Path(results_dir)
    all_results = []
    
    # æŸ¥æ‰¾æ‰€æœ‰CSVç»“æœæ–‡ä»¶
    csv_files = list(results_path.glob("**/*.csv"))
    
    for csv_file in csv_files:
        try:
            df = pd.read_csv(csv_file)
            all_results.append(df)
        except Exception as e:
            print(f"Error loading {csv_file}: {e}")
    
    if all_results:
        combined_df = pd.concat(all_results, ignore_index=True)
        return combined_df
    else:
        return None

def create_markdown_summary(df, output_path):
    """åˆ›å»ºMarkdownæ‘˜è¦"""
    
    total_experiments = len(df) if df is not None else 0
    completion_rate = (total_experiments / 540) * 100 if total_experiments > 0 else 0
    
    md_content = f"""# ğŸ”¬ D2å®éªŒç»“æœæ‘˜è¦

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“Š å®éªŒæ¦‚è§ˆ

- **æ€»å®éªŒæ•°**: {total_experiments} æ¬¡
- **æ¨¡å‹ç±»å‹**: 4 ç§ (enhanced, cnn, bilstm, conformer_lite)
- **éšæœºç§å­**: 5 ä¸ª (0-4)
- **å‚æ•°ç½‘æ ¼**: 3Ã—3Ã—3 = 27 ç§é…ç½®
- **æ€»é…ç½®æ•°**: 4 Ã— 5 Ã— 27 = **540 æ¬¡å®éªŒ**
- **å®Œæˆç‡**: **{completion_rate:.1f}%**

## ğŸ† æ¨¡å‹æ€§èƒ½æ’å

### Macro F1 Score
"""
    
    if df is not None and 'macro_f1' in df.columns and 'model' in df.columns and len(df) > 0:
        # è®¡ç®—å„æ¨¡å‹çš„å¹³å‡æ€§èƒ½
        model_performance = df.groupby('model')['macro_f1'].agg(['mean', 'std', 'count']).round(4)
        model_performance = model_performance.sort_values('mean', ascending=False)
        
        md_content += """
| æ’å | æ¨¡å‹ | Macro F1 (å‡å€¼Â±æ ‡å‡†å·®) | å®éªŒæ¬¡æ•° |
|------|------|------------------------|----------|
"""
        
        for i, (model, stats) in enumerate(model_performance.iterrows(), 1):
            medal = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else "  "
            md_content += f"| {i} {medal} | `{model}` | {stats['mean']:.4f} Â± {stats['std']:.4f} | {int(stats['count'])} |\n"
    else:
        md_content += "\n*æš‚æ— æœ‰æ•ˆçš„æ¨¡å‹æ€§èƒ½æ•°æ®*\n"
    
    # ECEæ’å
    if df is not None and 'ece' in df.columns and 'model' in df.columns:
        md_content += "\n### Expected Calibration Error (ECE)\n"
        ece_performance = df.groupby('model')['ece'].agg(['mean', 'std']).round(4)
        ece_performance = ece_performance.sort_values('mean', ascending=True)  # ECEè¶Šå°è¶Šå¥½
        
        md_content += """
| æ’å | æ¨¡å‹ | ECE (å‡å€¼Â±æ ‡å‡†å·®) |
|------|------|------------------|
"""
        
        for i, (model, stats) in enumerate(ece_performance.iterrows(), 1):
            medal = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else "  "
            md_content += f"| {i} {medal} | `{model}` | {stats['mean']:.4f} Â± {stats['std']:.4f} |\n"
    
    # è¶…å‚æ•°å½±å“åˆ†æ
    if df is not None and len(df) > 0:
        md_content += "\n## âš™ï¸ è¶…å‚æ•°å½±å“åˆ†æ\n"
        
        params = ['class_overlap', 'label_noise_prob', 'env_burst_rate']
        for param in params:
            if param in df.columns:
                param_effect = df.groupby(param)['macro_f1'].agg(['mean', 'std']).round(4)
                
                param_name_map = {
                    'class_overlap': 'Class Overlap',
                    'label_noise_prob': 'Label Noise Probability', 
                    'env_burst_rate': 'Environment Burst Rate'
                }
                
                md_content += f"\n### {param_name_map.get(param, param)}\n"
                md_content += """
| å‚æ•°å€¼ | Macro F1 (å‡å€¼Â±æ ‡å‡†å·®) |
|--------|----------------------|
"""
                
                for value, stats in param_effect.iterrows():
                    md_content += f"| {value} | {stats['mean']:.4f} Â± {stats['std']:.4f} |\n"
    
    # éªŒæ”¶æ ‡å‡†æ£€æŸ¥
    md_content += "\n## âœ… D2éªŒæ”¶æ ‡å‡†æ£€æŸ¥\n"
    
    # æ£€æŸ¥å®éªŒå®Œæˆåº¦
    expected_experiments = 540
    status_icon = "âœ…" if completion_rate >= 95 else "âš ï¸" if completion_rate >= 80 else "âŒ"
    
    md_content += f"""
### å®éªŒå®Œæˆåº¦
- **é¢„æœŸå®éªŒæ•°**: {expected_experiments}
- **å®é™…å®Œæˆæ•°**: {total_experiments}
- **å®Œæˆç‡**: {completion_rate:.1f}% {status_icon}

### æ•°æ®è´¨é‡æ£€æŸ¥
"""
    
    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ¨¡å‹éƒ½æœ‰ç»“æœ
    if df is not None and 'model' in df.columns:
        expected_models = {'enhanced', 'cnn', 'bilstm', 'conformer_lite'}
        actual_models = set(df['model'].unique())
        missing_models = expected_models - actual_models
        
        if not missing_models:
            md_content += "- **æ¨¡å‹è¦†ç›–**: âœ… æ‰€æœ‰4ä¸ªæ¨¡å‹éƒ½æœ‰ç»“æœ\n"
        else:
            md_content += f"- **æ¨¡å‹è¦†ç›–**: âš ï¸ ç¼ºå¤±æ¨¡å‹: {missing_models}\n"
    else:
        md_content += "- **æ¨¡å‹è¦†ç›–**: âŒ æ— æ¨¡å‹æ•°æ®\n"
    
    # æ£€æŸ¥ç§å­è¦†ç›–
    if df is not None and 'seed' in df.columns:
        expected_seeds = {0, 1, 2, 3, 4}
        actual_seeds = set(df['seed'].unique())
        missing_seeds = expected_seeds - actual_seeds
        
        if not missing_seeds:
            md_content += "- **ç§å­è¦†ç›–**: âœ… æ‰€æœ‰5ä¸ªç§å­éƒ½æœ‰ç»“æœ\n"
        else:
            md_content += f"- **ç§å­è¦†ç›–**: âš ï¸ ç¼ºå¤±ç§å­: {missing_seeds}\n"
    else:
        md_content += "- **ç§å­è¦†ç›–**: âŒ æ— ç§å­æ•°æ®\n"
    
    # æ€§èƒ½ç¨³å®šæ€§æ£€æŸ¥
    if df is not None and 'macro_f1' in df.columns and 'model' in df.columns:
        md_content += "\n### æ€§èƒ½ç¨³å®šæ€§\n"
        
        for model in df['model'].unique():
            model_data = df[df['model'] == model]['macro_f1']
            if len(model_data) > 1:
                cv = (model_data.std() / model_data.mean()) * 100  # å˜å¼‚ç³»æ•°
                
                stability_icon = "âœ…" if cv < 10 else "âš ï¸" if cv < 20 else "âŒ"
                md_content += f"- **{model}**: CV = {cv:.2f}% {stability_icon}\n"
            else:
                md_content += f"- **{model}**: æ•°æ®ä¸è¶³ âš ï¸\n"
    
    # åç»­è®¡åˆ’
    md_content += """
## ğŸš€ ä¸‹ä¸€æ­¥è®¡åˆ’

### ç«‹å³ä»»åŠ¡
- [ ] åˆ›å»ºD2å®Œæˆé‡Œç¨‹ç¢‘æ ‡ç­¾ (`v1.0-d2-complete`)
- [ ] å‡†å¤‡Sim2Realå®éªŒæ•°æ®
- [ ] è®¾ç½®SenseFi benchmarkç¯å¢ƒ

### Sim2Realå®éªŒè®¡åˆ’
- [ ] **åŸºçº¿å»ºç«‹**: åœ¨SenseFiæ•°æ®é›†ä¸Šè®­ç»ƒä¼ ç»Ÿæ¨¡å‹
- [ ] **åŸŸè½¬ç§»æµ‹è¯•**: åˆæˆæ•°æ®è®­ç»ƒ â†’ çœŸå®æ•°æ®æµ‹è¯•
- [ ] **å°‘æ ·æœ¬å­¦ä¹ **: ç”¨å°‘é‡çœŸå®æ•°æ®å¾®è°ƒ
- [ ] **è·¨åŸŸæ³›åŒ–**: ä¸åŒæ•°æ®é›†é—´çš„æ€§èƒ½è¯„ä¼°

### è®ºæ–‡å†™ä½œ
- [ ] æ›´æ–°å®éªŒç»“æœåˆ°`main.tex`
- [ ] ç”Ÿæˆæ€§èƒ½å¯¹æ¯”å›¾è¡¨
- [ ] å®Œå–„è®¨è®ºéƒ¨åˆ†
- [ ] å‡†å¤‡æŠ•ç¨¿åˆ°TMC/IoTJæœŸåˆŠ

## ğŸ“‹ æ–‡ä»¶ä½ç½®

- **è¯¦ç»†åˆ†ææŠ¥å‘Š**: `reports/d2_analysis.html`
- **åŸå§‹ç»“æœæ•°æ®**: `results_gpu/d2/`
- **éªŒæ”¶è„šæœ¬**: `scripts/validate_d2_acceptance.py`
- **Gitç®¡ç†æŒ‡å—**: `docs/Git_Management_Commands.md`
- **é¡¹ç›®æ¸…å•**: `PROJECT_MANIFEST.md`

---

*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(md_content)
    
    print(f"Markdownæ‘˜è¦å·²ç”Ÿæˆ: {output_path}")

def main():
    parser = argparse.ArgumentParser(description='ç”ŸæˆD2å®éªŒç»“æœæ‘˜è¦')
    parser.add_argument('results_dir', help='ç»“æœç›®å½•è·¯å¾„')
    parser.add_argument('--format', choices=['markdown'], default='markdown', help='è¾“å‡ºæ ¼å¼')
    parser.add_argument('--output', default='D2_Results_Summary.md', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # åŠ è½½ç»“æœ
    print("åŠ è½½å®éªŒç»“æœ...")
    df = load_experiment_results(args.results_dir)
    
    if df is None:
        print("è­¦å‘Š: æœªæ‰¾åˆ°æœ‰æ•ˆçš„å®éªŒç»“æœï¼Œç”Ÿæˆç©ºæ‘˜è¦")
    else:
        print(f"æˆåŠŸåŠ è½½ {len(df)} æ¡å®éªŒè®°å½•")
    
    # ç”Ÿæˆæ‘˜è¦
    if args.format == 'markdown':
        create_markdown_summary(df, args.output)
    
    print("âœ… D2ç»“æœæ‘˜è¦ç”Ÿæˆå®Œæˆï¼")

if __name__ == "__main__":
    main()
```

---

## ğŸ“‹ **åˆ›å»ºæ­¥éª¤æ€»ç»“**

### **éœ€è¦åˆ›å»ºçš„4ä¸ªæ–‡ä»¶**:
1. `scripts\run_d2_validation.bat` - ä¸€é”®éªŒæ”¶æ‰¹å¤„ç†
2. `scripts\validate_d2_acceptance.py` - æ ¸å¿ƒéªŒæ”¶è„šæœ¬  
3. `scripts\generate_d2_analysis_report.py` - HTMLæŠ¥å‘Šç”Ÿæˆå™¨
4. `scripts\create_results_summary.py` - Markdownæ‘˜è¦ç”Ÿæˆå™¨

### **åˆ›å»ºå®Œæˆåæ‰§è¡Œ**:
```bash
# 1. è¿è¡Œä¸€é”®éªŒæ”¶
scripts\run_d2_validation.bat

# 2. æŸ¥çœ‹ç”Ÿæˆçš„æŠ¥å‘Š
start reports\d2_analysis.html
notepad D2_Results_Summary.md
```

### **æ¨é€åˆ°Git**:
```bash
git add scripts\run_d2_validation.bat
git add scripts\validate_d2_acceptance.py  
git add scripts\generate_d2_analysis_report.py
git add scripts\create_results_summary.py
git commit -m "Add complete D2 validation toolkit"
git push origin results/exp-2025
```

ç°åœ¨æ‚¨æœ‰äº†å®Œæ•´çš„D2éªŒæ”¶å·¥å…·åŒ…ï¼