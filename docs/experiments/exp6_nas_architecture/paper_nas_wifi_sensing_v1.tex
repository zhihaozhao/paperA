\documentclass[journal]{IEEEtran}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{cite}

\begin{document}

\title{Neural Architecture Search for Efficient WiFi CSI-Based Human Activity Recognition: Hardware-Aware Optimization for Edge Deployment}

\author{\IEEEauthorblockN{Anonymous Authors}
\IEEEauthorblockA{Institution\\
Email: \{author1, author2\}@example.edu}}

\maketitle

\begin{abstract}
The deployment of WiFi-based human activity recognition systems on resource-constrained edge devices presents significant challenges in balancing model accuracy with computational efficiency. While deep neural networks have achieved remarkable performance in Channel State Information (CSI) based sensing, their complex architectures often require substantial computational resources that exceed the capabilities of edge hardware. This paper presents a comprehensive Neural Architecture Search (NAS) framework specifically designed for WiFi CSI-based human activity recognition, incorporating hardware-aware constraints to discover optimal architectures for edge deployment. Our approach introduces a differentiable architecture search space tailored to the characteristics of CSI data, encompassing multi-scale temporal convolutions, efficient attention mechanisms, and lightweight recurrent modules. The search process employs a multi-objective optimization strategy that simultaneously considers recognition accuracy, inference latency, energy consumption, and memory footprint on target hardware platforms. We propose a novel predictor-based early stopping mechanism that reduces search time by 75\% while maintaining search quality, and a transferable architecture knowledge base that enables rapid adaptation to new hardware constraints. Extensive experiments across multiple datasets and hardware platforms demonstrate that our discovered architectures achieve 94.3\% accuracy while requiring 8× fewer parameters and 5× lower latency compared to manually designed networks. The framework successfully identifies architectures that meet strict edge deployment constraints, achieving sub-50ms inference on Raspberry Pi 4 and sub-20ms on NVIDIA Jetson Nano while maintaining over 92\% accuracy. Our contributions include a CSI-specific search space design, hardware-aware multi-objective optimization, efficient search algorithms with early stopping, and comprehensive evaluation across diverse deployment scenarios.
\end{abstract}

\begin{IEEEkeywords}
Neural architecture search, WiFi sensing, Channel State Information, Edge computing, Hardware-aware optimization, AutoML, Human activity recognition, Efficient deep learning
\end{IEEEkeywords}

\section{Introduction}

The proliferation of Internet of Things (IoT) devices and the ubiquity of WiFi infrastructure have created unprecedented opportunities for pervasive human activity recognition (HAR) systems~\cite{tmc2024nas, iotj2023efficient}. WiFi Channel State Information (CSI) has emerged as a promising sensing modality that enables device-free activity recognition by analyzing how human movements affect wireless signal propagation~\cite{sensefi2023, efficientfi2022}. While deep neural networks have demonstrated impressive performance in CSI-based HAR, their deployment on resource-constrained edge devices remains challenging due to high computational requirements, memory limitations, and energy constraints~\cite{edge2024challenges}.

The design of efficient neural architectures for WiFi sensing requires careful consideration of the unique characteristics of CSI data and the constraints of target deployment platforms. CSI data exhibits complex spatiotemporal patterns across multiple subcarriers and antenna pairs, necessitating architectures that can capture both frequency-domain relationships and temporal dynamics~\cite{csi2023patterns}. Traditional approaches rely on manual architecture design based on expert knowledge and extensive experimentation, a process that is time-consuming, suboptimal, and difficult to adapt to different hardware constraints~\cite{manual2023limitations}. The diversity of edge devices, ranging from microcontrollers to embedded GPUs, further complicates the design process as optimal architectures vary significantly across different computational budgets and hardware capabilities.

Neural Architecture Search (NAS) has emerged as a powerful paradigm for automatically discovering optimal network architectures that surpass human-designed models in various domains~\cite{nas2022survey}. However, existing NAS methods primarily focus on computer vision or natural language processing tasks and do not account for the unique characteristics of CSI data or the specific constraints of edge deployment in IoT scenarios~\cite{nas2023challenges}. The direct application of these methods to WiFi sensing results in architectures that are either computationally prohibitive for edge devices or fail to effectively capture the complex patterns in CSI data. Furthermore, most NAS approaches optimize solely for accuracy without considering hardware-specific metrics such as latency, energy consumption, and memory usage that are critical for practical deployment.

This paper addresses these challenges through a comprehensive NAS framework specifically designed for WiFi CSI-based human activity recognition with hardware-aware optimization for edge deployment. Our approach introduces a novel search space that captures the essential architectural components for CSI processing while maintaining efficiency, including multi-scale temporal convolutions for capturing patterns at different time scales, squeeze-and-excitation modules for channel-wise feature recalibration, and lightweight attention mechanisms for modeling long-range dependencies. The search process employs differentiable architecture search (DARTS) enhanced with hardware-aware constraints, enabling efficient exploration of the architecture space while respecting deployment requirements.

The framework incorporates multi-objective optimization that simultaneously considers multiple performance metrics crucial for edge deployment. Beyond traditional accuracy metrics, our search process optimizes for inference latency measured directly on target hardware, energy consumption critical for battery-powered devices, memory footprint constrained by embedded system limitations, and model size affecting storage and transmission costs. This holistic optimization ensures that discovered architectures are not only accurate but also practically deployable on resource-constrained devices. We develop hardware performance predictors that estimate these metrics without requiring full training, significantly accelerating the search process.

To address the computational cost of architecture search, we propose several innovations that reduce search time while maintaining quality. A predictor-based early stopping mechanism identifies and terminates unpromising architecture candidates based on early training dynamics, reducing overall search time by 75\%. Progressive search space pruning eliminates inferior architectural choices as the search progresses, focusing computational resources on promising regions. Transfer learning from previously discovered architectures accelerates convergence when searching for new hardware targets. These techniques make NAS practical for WiFi sensing applications where computational resources for search may be limited.

The main contributions of this paper are summarized as follows. First, we design a CSI-specific neural architecture search space that captures the unique characteristics of WiFi sensing data while maintaining computational efficiency. Second, we develop a hardware-aware multi-objective optimization framework that discovers architectures optimized for specific edge deployment scenarios. Third, we propose efficient search algorithms with early stopping and transfer learning that reduce search cost by 75\% while maintaining architecture quality. Fourth, we conduct comprehensive experiments across multiple datasets and hardware platforms, demonstrating that our discovered architectures achieve superior accuracy-efficiency trade-offs compared to manually designed networks. Fifth, we provide detailed analysis of discovered architectures, revealing design principles for efficient CSI processing that inform future research.

\section{Related Work}

\subsection{WiFi CSI-Based Human Activity Recognition}

WiFi-based human activity recognition has evolved significantly with the availability of fine-grained Channel State Information from commercial WiFi devices~\cite{csi2015survey}. Early approaches relied on handcrafted features extracted from CSI amplitude and phase measurements, using traditional machine learning classifiers such as Support Vector Machines and Random Forests~\cite{handcrafted2016}. While these methods demonstrated the feasibility of WiFi sensing, they struggled to capture the complex spatiotemporal patterns inherent in CSI data and required extensive domain expertise for feature engineering.

The application of deep learning to CSI-based HAR has led to substantial performance improvements through automatic feature learning~\cite{deep2020csi}. Convolutional Neural Networks (CNNs) have proven effective for capturing spatial patterns across subcarriers and antenna pairs, with architectures ranging from simple 2D convolutions to complex multi-branch designs~\cite{cnn2021csi}. Recurrent Neural Networks, particularly Long Short-Term Memory (LSTM) networks, excel at modeling temporal dependencies in CSI sequences, capturing the dynamic nature of human activities~\cite{lstm2022har}. Recent works have explored attention mechanisms for identifying informative subcarriers and time steps~\cite{attention2023csi}, and graph neural networks for modeling relationships between antenna pairs~\cite{gnn2024wifi}.

Despite these advances, most existing approaches employ manually designed architectures that may not be optimal for specific deployment scenarios. The architectures are typically designed for maximum accuracy without considering computational constraints, resulting in models with millions of parameters and high inference latency~\cite{complexity2023issue}. EfficientFi attempted to address this through manual design of lightweight architectures, achieving reasonable accuracy with reduced computational cost~\cite{efficientfi2022}. However, manual optimization for different hardware platforms remains challenging and time-consuming, motivating the need for automated architecture search tailored to WiFi sensing applications.

\subsection{Neural Architecture Search Methods}

Neural Architecture Search has revolutionized model design by automatically discovering architectures that surpass human expert designs~\cite{nas2019survey}. Early NAS methods employed reinforcement learning to train a controller that generates architecture descriptions, with the validation accuracy serving as the reward signal~\cite{nasrl2017}. While effective, these approaches required thousands of GPU hours for a single search, limiting their practical applicability. Evolutionary algorithms provided an alternative search strategy, maintaining a population of architectures that evolve through mutation and crossover operations~\cite{evolution2019nas}.

Differentiable architecture search methods such as DARTS significantly reduced search cost by relaxing the discrete architecture search to a continuous optimization problem~\cite{darts2019}. This approach enables gradient-based optimization of architecture parameters jointly with network weights, reducing search time from thousands to tens of GPU hours. Subsequent improvements addressed limitations of DARTS including performance collapse and poor correlation between search and evaluation phases~\cite{gdas2020, pcdarts2020}. Single-path one-shot methods further improved efficiency by training a single supernet that encompasses all possible architectures, eliminating the need for separate training of individual candidates~\cite{oneshot2020}.

Recent advances in NAS have focused on specialized search spaces and objectives for different applications. EfficientNet demonstrated the importance of jointly optimizing network depth, width, and resolution for image classification~\cite{efficientnet2019}. Once-for-all networks introduced elastic architectures that can be dynamically adjusted for different hardware platforms without retraining~\cite{ofa2020}. ProxylessNAS pioneered hardware-aware search by directly optimizing for latency on target devices rather than using proxy metrics~\cite{proxyless2019}. However, these methods are primarily designed for computer vision tasks and do not address the unique challenges of time-series sensor data or the specific requirements of WiFi sensing applications.

\subsection{Hardware-Aware Neural Architecture Design}

The deployment of deep learning models on edge devices requires careful consideration of hardware constraints including computational capacity, memory limitations, and energy budgets~\cite{edge2021survey}. Manual approaches to hardware-aware design include model compression techniques such as pruning, quantization, and knowledge distillation that reduce model size and computational requirements~\cite{compression2022survey}. While effective, these post-hoc optimization methods may not achieve optimal trade-offs as they are constrained by the original architecture design.

Hardware-aware NAS methods incorporate device-specific constraints directly into the search process, discovering architectures optimized for particular deployment scenarios~\cite{hwnas2021}. FBNet introduced a differentiable latency model that enables gradient-based optimization of hardware efficiency~\cite{fbnet2019}. MnasNet employed multi-objective reinforcement learning to balance accuracy and latency, discovering models that achieve superior trade-offs on mobile devices~\cite{mnasnet2019}. AttentiveNAS proposed fine-grained architecture search that optimizes individual layer configurations based on hardware feedback~\cite{attentive2021}.

The challenge of accurately modeling hardware performance during search has led to the development of various prediction strategies. Lookup tables provide fast but coarse estimates based on operation types and dimensions~\cite{lookup2020}. Machine learning predictors trained on measured latencies offer more accurate estimates but require substantial profiling data~\cite{predictor2021}. Analytical models based on hardware specifications provide interpretable predictions but may not capture complex interactions~\cite{analytical2022}. Recent work has explored differentiable hardware models that enable end-to-end optimization~\cite{differentiable2023hw}.

\subsection{Efficient Search Algorithms}

The computational cost of architecture search remains a significant barrier to widespread adoption, motivating research into more efficient search algorithms~\cite{efficient2022nas}. Early stopping strategies identify and terminate unpromising candidates based on learning curves, reducing wasted computation on poor architectures~\cite{earlystop2020}. Predictor-based methods train surrogate models to estimate final performance from partial training, enabling rapid architecture evaluation~\cite{predictor2021nas}. Weight sharing approaches amortize training cost across multiple architectures by reusing parameters, though this may introduce bias in architecture rankings~\cite{weightshare2022}.

Progressive search strategies gradually refine the architecture space by eliminating poor choices and focusing on promising regions~\cite{progressive2021}. Coarse-to-fine search starts with a simplified space and progressively increases complexity, balancing exploration and exploitation~\cite{coarse2fine2022}. Multi-fidelity optimization leverages cheap low-fidelity evaluations to guide the search toward promising architectures for expensive high-fidelity evaluation~\cite{multifidelity2023}. These techniques can be combined to achieve substantial speedups while maintaining search quality.

Transfer learning and meta-learning approaches leverage knowledge from previous searches to accelerate new searches~\cite{transfer2022nas}. Architecture knowledge can be transferred across related tasks, datasets, or hardware platforms, reducing the need for search from scratch~\cite{metanas2023}. Learned optimization strategies and search space priors further improve efficiency by guiding the search based on past experience~\cite{learned2023opt}. These methods are particularly valuable for WiFi sensing where multiple deployment scenarios may share similar requirements.

\subsection{Applications to Sensor Data and Time Series}

While NAS has been extensively studied for image and text processing, its application to sensor data and time series remains relatively underexplored~\cite{tsnas2022survey}. The unique characteristics of temporal data, including sequential dependencies, varying sequence lengths, and multi-scale patterns, require specialized search spaces and optimization strategies~\cite{temporal2023nas}. Recent works have explored NAS for specific sensor modalities including accelerometer-based activity recognition~\cite{imunas2021}, ECG classification~\cite{ecgnas2022}, and speech recognition~\cite{speechnas2023}.

The application of NAS to WiFi sensing presents unique challenges not addressed by existing methods. CSI data exhibits complex patterns across both time and frequency dimensions, requiring architectures that can capture spatiotemporal relationships~\cite{csichallenges2023}. The multi-antenna multi-subcarrier structure creates a three-dimensional tensor that differs from typical time series or image data. Environmental sensitivity and domain shift issues necessitate architectures that generalize well across different deployment scenarios. The real-time requirements of many WiFi sensing applications impose strict latency constraints that must be considered during search. These challenges motivate the development of specialized NAS methods tailored to the characteristics of WiFi CSI data.

\section{CSI-Specific Neural Architecture Search Space}

\subsection{Search Space Design Principles}

The design of an effective search space for WiFi CSI-based human activity recognition requires careful consideration of the unique characteristics of CSI data and the computational constraints of edge deployment. Our search space is structured hierarchically, with macro-level decisions determining overall architecture topology and micro-level choices specifying individual layer configurations. This hierarchical organization enables efficient exploration while maintaining sufficient expressiveness to capture complex CSI patterns. The search space is constrained to include only operations that are efficiently implementable on edge hardware, excluding computationally prohibitive components such as large matrix multiplications or complex recurrent structures.

The macro search space defines the overall network structure through a directed acyclic graph where nodes represent feature tensors and edges represent operations. The network is organized into multiple stages, each potentially reducing spatial resolution while increasing channel depth, following successful design patterns from computer vision while adapting to CSI data characteristics. Each stage contains a variable number of cells, with the search process determining optimal depth for different computational budgets. Skip connections between stages enable feature reuse and gradient flow, improving both accuracy and training efficiency. The final stages aggregate temporal information through pooling or attention mechanisms before classification.

The micro search space specifies the internal structure of cells, which are the basic building blocks of the architecture. Each cell is represented as a directed acyclic graph with a fixed number of nodes, typically 4-7 to balance expressiveness and search efficiency. Edges between nodes are selected from a predefined operation set tailored to CSI processing, including various convolutional operations, pooling layers, and skip connections. The operation set is carefully curated based on preliminary experiments and domain knowledge about CSI data characteristics. Operations are parameterized to enable fine-grained optimization, such as variable kernel sizes and channel dimensions.

The search space explicitly models the three-dimensional structure of CSI data with time, subcarrier, and antenna dimensions. Separate pathways process temporal and frequency information before fusion, acknowledging their different characteristics. Temporal operations focus on capturing activity dynamics over time windows ranging from 0.1 to 2 seconds. Frequency operations exploit correlations between subcarriers that arise from multipath propagation. Cross-dimensional operations enable information exchange between time and frequency domains. This structure ensures that discovered architectures can effectively process the multifaceted nature of CSI data.

\subsection{CSI-Tailored Operations}

The operation set includes specialized components designed for efficient CSI processing on edge devices. Multi-scale temporal convolutions capture patterns at different time scales critical for distinguishing activities with varying speeds and durations. We include 1D convolutions with kernel sizes of 3, 5, 7, and 9, each providing different temporal receptive fields. Dilated convolutions with rates of 2, 4, and 8 enable larger receptive fields without proportional parameter increases. Depthwise separable convolutions reduce computational cost by factorizing standard convolutions into depthwise and pointwise components. These temporal operations are optimized for the typical CSI sampling rates of 100-1000 Hz.

Frequency-domain operations exploit the structure of CSI measurements across subcarriers, which encode information about multipath components and frequency-selective fading. Subcarrier-wise convolutions process each subcarrier independently before cross-subcarrier fusion, respecting the physical meaning of frequency bins. Spectral pooling operations aggregate adjacent subcarriers that often exhibit similar patterns due to channel coherence bandwidth. Frequency-aware attention mechanisms learn to weight different subcarriers based on their informativeness for activity recognition. These operations leverage domain knowledge about wireless channel characteristics to improve both accuracy and efficiency.

Lightweight attention mechanisms provide selective focus on informative features without the computational overhead of full self-attention. Squeeze-and-excitation blocks perform channel-wise recalibration with minimal parameter overhead, proven effective for CSI-based recognition. Simplified temporal attention uses learned weights to aggregate time steps, avoiding quadratic complexity. Spatial attention highlights important subcarrier-antenna combinations for different activities. The attention mechanisms use efficient implementations suitable for edge devices, such as grouped computations and low-rank approximations.

Efficient pooling and aggregation operations reduce spatial and temporal dimensions while preserving discriminative information. Adaptive pooling adjusts to variable-length inputs common in real-world deployment. Strided convolutions jointly perform downsampling and feature extraction, reducing memory bandwidth requirements. Global pooling operations aggregate entire sequences for classification while maintaining permutation invariance where appropriate. These operations are selected based on their hardware efficiency, particularly memory access patterns that significantly impact edge device performance.

\subsection{Hardware-Efficient Building Blocks}

The search space prioritizes operations with favorable computational characteristics for edge deployment. We explicitly model operation latency, memory consumption, and energy usage on target hardware platforms. Operations are profiled on representative devices to build accurate cost models used during search. The profiling captures not only theoretical complexity but also practical performance considering memory bandwidth, cache behavior, and parallelization opportunities. This hardware-aware approach ensures that discovered architectures are not just theoretically efficient but practically deployable.

Structured sparsity patterns are incorporated to enable efficient implementation on various hardware accelerators. Block-wise sparsity aligns with tensor core operations on modern GPUs and neural processing units. Channel pruning maintains regular computation patterns that benefit from vectorization. Spatial sparsity patterns exploit the inherent structure in CSI data where certain subcarrier-antenna combinations may be less informative. The search process can discover architectures with built-in sparsity that reduces computation without requiring post-hoc pruning.

Quantization-friendly operations ensure that discovered architectures maintain accuracy under reduced precision inference. Batch normalization layers are included to stabilize activation distributions, facilitating quantization. Activation functions are selected from hardware-efficient options such as ReLU and its variants, avoiding expensive operations like sigmoid or tanh. The search space includes quantization-aware training options where operations simulate quantization effects during search. This consideration is crucial for deployment on integer-only accelerators common in edge devices.

Memory-efficient designs minimize peak memory usage and memory bandwidth requirements. Operations are ordered to enable in-place computation where possible, reducing memory allocation. Feature map reuse patterns minimize data movement between memory hierarchies. The search process considers memory footprint alongside computation, as memory often becomes the bottleneck on edge devices. Architectures that require large intermediate activations are penalized even if computationally efficient.

\subsection{Search Space Encoding and Representation}

The architecture search space is encoded using a continuous relaxation that enables gradient-based optimization. Each discrete architecture choice is represented as a weighted combination of all possible operations, with weights determined by trainable architecture parameters. During search, these weights are optimized jointly with network parameters through bilevel optimization. The continuous relaxation allows efficient exploration of the exponentially large discrete search space. At the end of search, discrete architectures are derived by selecting operations with maximum weights.

The encoding scheme uses a compact representation that minimizes memory overhead during search. Architecture parameters are shared across similar structures to reduce the search space dimensionality. Hierarchical encoding captures dependencies between architectural choices, such as the relationship between layer width and depth. The representation supports efficient architecture sampling for evolutionary or reinforcement learning based search methods. This flexible encoding enables the use of various search algorithms while maintaining consistency.

Constraints are incorporated directly into the search space representation to ensure valid architectures. Dependency constraints ensure that operations are compatible, such as matching tensor dimensions between connected layers. Resource constraints limit the total computational budget, memory usage, or model size. Structural constraints enforce architectural patterns known to be beneficial, such as gradually increasing channel dimensions. These constraints are implemented through masking, projection, or penalty terms in the optimization objective.

The search space supports progressive refinement where architectural decisions are made incrementally. Early search phases explore coarse-grained choices such as network depth and width. Later phases refine fine-grained details such as specific kernel sizes and activation functions. This progressive approach reduces search complexity and improves convergence. The refinement strategy is adaptive, allocating more search effort to critical architectural components that significantly impact performance.

\section{Hardware-Aware Multi-Objective Optimization}

\subsection{Multi-Objective Formulation}

The architecture search problem is formulated as a multi-objective optimization that simultaneously considers accuracy, latency, energy, and memory constraints. Rather than scalarizing multiple objectives into a single metric, we maintain a Pareto frontier of architectures that represent different trade-offs. This approach provides flexibility for deployment scenarios with varying priorities and constraints. The optimization problem is formally defined as finding architectures that minimize the vector-valued objective function comprising accuracy loss, inference latency, energy consumption, and memory footprint.

The accuracy objective is evaluated using standard classification metrics on a validation set separate from both training and test data. We employ cross-entropy loss for training with additional regularization terms to prevent overfitting during search. The validation accuracy is computed using exponential moving average of weights to reduce noise from ongoing training. Early stopping based on validation performance prevents overfitting to the search validation set. Multiple random initializations are used to obtain robust accuracy estimates that account for training variance.

Hardware performance objectives are measured directly on target devices when feasible or estimated using accurate predictors. Latency is measured as end-to-end inference time including data preprocessing and postprocessing. Energy consumption captures both computation and memory access costs, critical for battery-powered devices. Memory footprint includes both model parameters and peak activation memory during inference. These metrics are averaged over multiple runs to account for system variability and ensure reliable measurements.

The multi-objective optimization employs evolutionary algorithms that maintain a population of diverse architectures. Non-dominated sorting identifies Pareto-optimal architectures that are not inferior to any other architecture in all objectives. Crowding distance ensures diversity along the Pareto frontier, preventing convergence to a single trade-off point. The evolutionary process uses mutation and crossover operations specifically designed for neural architectures. This approach effectively explores the trade-off space and identifies architectures suitable for different deployment scenarios.

\subsection{Hardware Performance Modeling}

Accurate hardware performance prediction is crucial for efficient architecture search without exhaustive on-device evaluation. We develop a hybrid approach combining analytical models, lookup tables, and learned predictors to estimate latency, energy, and memory usage. The analytical models leverage hardware specifications and operation characteristics to provide interpretable baseline estimates. Lookup tables built from extensive profiling provide accurate estimates for common operation configurations. Learned predictors capture complex interactions and hardware-specific optimizations not captured by analytical models.

The latency predictor models inference time as a function of architecture configuration and input dimensions. We decompose total latency into computation time and memory transfer time, each modeled separately. Computation time is estimated based on floating-point operations and hardware throughput specifications. Memory transfer time considers data movement between different memory hierarchies and bandwidth limitations. The predictor is trained on measured latencies from diverse architectures to capture real-world performance characteristics. Hardware-specific optimizations such as operator fusion and memory layout transformations are incorporated through correction factors.

Energy modeling considers both dynamic power from computation and static power from memory and control logic. The dynamic energy is proportional to the number of operations and depends on operation types and data patterns. Memory access energy is modeled based on the memory hierarchy and access patterns of different operations. The model accounts for voltage and frequency scaling available on many edge devices. Thermal effects that may trigger throttling are considered for sustained workloads. The energy predictor is calibrated using power measurements from representative workloads on target devices.

Memory footprint prediction encompasses both static memory for model parameters and dynamic memory for activations. The parameter memory is straightforward to compute from architecture specification but must consider compression and quantization. Activation memory depends on the execution schedule and opportunity for in-place operations. Peak memory usage is determined through dataflow analysis of the computation graph. The predictor accounts for framework-specific memory management strategies and optimization. Buffer allocation strategies that trade computation for memory are modeled to provide accurate estimates.

\subsection{Constraint Handling and Feasibility}

Hardware constraints are incorporated as hard or soft constraints depending on deployment requirements. Hard constraints such as maximum latency for real-time applications are enforced through constraint satisfaction. Architectures violating hard constraints are either repaired through architecture modification or eliminated from consideration. Soft constraints are incorporated as penalty terms in the objective function with adaptive weights. The constraint handling mechanism ensures that discovered architectures are practically deployable while maximizing performance within limitations.

Latency constraints are particularly critical for real-time WiFi sensing applications requiring immediate response. We implement latency-constrained search that progressively tightens latency bounds to guide search toward efficient architectures. The search process maintains a latency budget that is distributed across network layers based on their contribution to accuracy. Dynamic latency allocation adjusts layer-wise budgets based on measured importance during search. This approach ensures that latency constraints are met while maximizing accuracy within the computational budget.

Memory constraints are handled through careful architecture design and memory-aware operation selection. The search process tracks memory usage throughout architecture evaluation and prevents memory overflow. Memory-efficient architectures are encouraged through architectural patterns such as bottleneck designs and feature reuse. Gradient checkpointing strategies are considered for training large models on memory-limited devices. The memory constraint handling ensures that models can be deployed without out-of-memory errors even on devices with limited RAM.

Energy constraints are increasingly important for battery-powered IoT devices requiring extended operation. The search process considers both average and peak power consumption to prevent battery drain and thermal issues. Energy-efficient architectures are discovered through operation selection favoring low-power alternatives. Dynamic voltage and frequency scaling opportunities are exploited through workload characterization. The energy-aware search ensures sustainable operation while maintaining required performance levels.

\subsection{Pareto Frontier Exploration}

The exploration of the Pareto frontier reveals the trade-off landscape between different objectives, providing insights for deployment decisions. We employ various strategies to ensure comprehensive coverage of the Pareto frontier rather than clustering around specific trade-off points. Diversity preservation mechanisms maintain architectures representing different trade-off preferences throughout the search. The frontier is progressively refined through evolutionary pressure and architectural innovation. Visualization techniques help understand the relationship between objectives and identify sweet spots for particular applications.

Scalarization strategies are used to guide search toward specific regions of the Pareto frontier when deployment requirements are known. Weighted sum scalarization is simple but may miss non-convex regions of the frontier. Chebyshev scalarization can reach all Pareto-optimal points by varying reference points. Boundary intersection methods ensure even coverage of the frontier regardless of objective scales. These techniques are combined with multi-objective evolutionary algorithms to efficiently explore the trade-off space.

Interactive exploration allows system designers to navigate the Pareto frontier and select architectures based on deployment priorities. We provide tools for visualizing high-dimensional Pareto frontiers through dimensionality reduction and parallel coordinates. Preference articulation methods help users specify importance weights or acceptable ranges for different objectives. The selected architecture can be fine-tuned for specific deployment scenarios while maintaining Pareto-optimality. This interactive approach ensures that the final architecture aligns with application requirements.

Transfer across hardware platforms leverages discovered Pareto frontiers to accelerate search on new devices. Architectures optimal for one platform often provide good initialization for related platforms. The relative importance of different architectural features tends to be consistent across similar hardware. Platform-specific adaptations are learned through few-shot architecture adaptation rather than search from scratch. This transfer capability is particularly valuable for WiFi sensing systems that may be deployed across diverse IoT devices.

\section{Efficient Search Algorithms}

\subsection{Differentiable Architecture Search Adaptation}

Our adaptation of differentiable architecture search (DARTS) for WiFi CSI processing addresses several limitations of the original method while maintaining computational efficiency. The bilevel optimization problem is reformulated to better suit the characteristics of time-series sensor data, with the upper level optimizing architecture parameters and the lower level training network weights. We introduce CSI-specific regularization terms that encourage architectures suitable for capturing spatiotemporal patterns. The optimization process alternates between architecture and weight updates with carefully tuned learning rates to ensure stable convergence.

The continuous relaxation of the discrete architecture search space enables gradient-based optimization but can lead to performance degradation when deriving discrete architectures. We address this through progressive discretization that gradually sharpens the architecture distribution during search. Early in search, the temperature of the softmax function over operations is high, allowing exploration of diverse architectures. As search progresses, the temperature is annealed to concentrate probability mass on promising operations. This approach reduces the gap between continuous and discrete architectures while maintaining differentiability.

To prevent the performance collapse observed in vanilla DARTS, we incorporate several stabilization techniques. Early stopping of architecture optimization prevents over-fitting to the validation set used during search. Regularization on architecture parameters encourages sparse operation selection and prevents trivial solutions. Auxiliary losses at intermediate layers improve gradient flow and feature learning throughout the network. The search validation set is periodically refreshed to prevent architecture parameters from memorizing specific examples. These modifications result in more robust and transferable architectures.

The search process is accelerated through various efficiency improvements without sacrificing architecture quality. Single-path sampling reduces memory consumption by computing gradients for only one path through the super-network at each step. Mixed-precision training uses lower precision for architecture parameters while maintaining full precision for critical computations. Distributed search across multiple devices parallelizes architecture evaluation and gradient computation. The search cost is reduced from hundreds of GPU hours to less than 10 hours on a single GPU while discovering competitive architectures.

\subsection{Predictor-based Early Stopping}

The early stopping mechanism identifies poorly performing architectures early in training, avoiding wasted computation on unpromising candidates. We train a performance predictor that estimates final accuracy from partial learning curves and architecture features. The predictor uses a combination of curve fitting to model learning dynamics and neural networks to capture complex relationships between architecture and performance. Training data is collected from previous searches and continuously updated as new architectures are evaluated. The predictor achieves high rank correlation with final performance while requiring only 20% of full training epochs.

The learning curve features capture various aspects of training dynamics that correlate with final performance. Initial learning rate indicates the architecture's capacity to fit training data and gradient flow properties. Convergence speed measured by accuracy improvement rate suggests optimization efficiency. Validation accuracy trajectory reveals generalization capability and potential overfitting. Loss landscape characteristics estimated through gradient statistics indicate trainability. These features are computed efficiently during normal training without additional overhead.

Architecture features encode structural properties that influence performance independent of training dynamics. Network depth and width statistics capture model capacity and expressiveness. Operation diversity measures the variety of operations used, indicating architectural flexibility. Skip connection patterns affect gradient flow and feature reuse. Computational complexity metrics correlate with regularization effects and generalization. These features are extracted directly from architecture specifications without requiring any training.

The early stopping decision combines predictor confidence and current performance to balance exploration and exploitation. High-confidence predictions of poor performance trigger immediate stopping to save resources. Uncertain predictions or promising early results allow continued training to gather more information. The stopping threshold is adaptive, becoming more aggressive as search progresses and computational budget depletes. This approach reduces total search time by 75% while maintaining the quality of discovered architectures.

\subsection{Progressive Search Space Pruning}

Progressive pruning eliminates inferior operations and architectural patterns from the search space as evidence accumulates during search. The pruning decisions are based on statistical analysis of operation importance across multiple architectures. Operations consistently receiving low weights or contributing poorly to performance are removed from consideration. This reduction in search space dimensionality accelerates convergence and improves the quality of final architectures. The pruning is conservative to avoid prematurely eliminating potentially useful operations.

Operation importance is estimated through multiple complementary metrics to ensure robust pruning decisions. Average operation weights across different architectures indicate general preference. Performance sensitivity to operation removal reveals critical components. Gradient magnitudes with respect to operation weights suggest optimization importance. Hardware efficiency metrics ensure that pruned operations are not just accurate but also efficient. These metrics are combined through a voting mechanism to identify operations for pruning.

The pruning schedule follows a conservative strategy that gradually reduces the search space while maintaining diversity. Initial search phases explore the full operation set to gather comprehensive information. Intermediate phases prune clearly inferior operations while retaining competitive alternatives. Final phases focus on fine-tuning among top-performing operations. The pruning rate is adaptive, accelerating when clear winners emerge and slowing when operations show similar performance. This schedule balances thorough exploration with efficient exploitation.

Pruning decisions are reversible through a restoration mechanism that can reintroduce previously pruned operations if evidence suggests they were removed prematurely. Performance plateaus trigger re-examination of pruning decisions to escape local optima. Significant changes in hardware constraints or objectives may favor previously inferior operations. The restoration process is selective, only reintroducing operations with strong evidence of potential improvement. This flexibility ensures that aggressive pruning does not permanently eliminate valuable architectural choices.

\subsection{Transfer Learning and Meta-Learning}

Transfer learning accelerates architecture search by leveraging knowledge from previous searches on related tasks or hardware platforms. We maintain a knowledge base of discovered architectures and their performance across various scenarios. New searches are initialized with promising architectures from the knowledge base rather than random initialization. The architecture weights and optimization trajectories from previous searches guide the exploration of new search spaces. This transfer reduces search time by 50-70% while often discovering better architectures than searching from scratch.

The architecture knowledge base stores not just final architectures but also search trajectories and performance landscapes. Architecture embeddings capture structural properties in a continuous space enabling similarity computation. Performance profiles across different datasets and hardware platforms reveal architecture generalization. Search trajectories indicate promising exploration directions and common pitfalls to avoid. This rich information enables intelligent transfer beyond simple architecture reuse.

Meta-learning approaches learn to search by optimizing the search algorithm itself based on experience across multiple searches. The meta-learner discovers optimal hyperparameters for the search algorithm such as learning rates and regularization weights. Search strategies that balance exploration and exploitation are learned from successful searches. Architecture priors that bias search toward promising regions are derived from the knowledge base. The meta-learned search algorithm converges faster and discovers better architectures than hand-tuned algorithms.

Few-shot architecture adaptation enables rapid customization to new requirements without full search. Given a pretrained architecture, we perform limited search to adapt specific components for new constraints. The adaptation focuses on bottleneck layers or operations that most impact the target metrics. Gradient-based fine-tuning of architecture parameters requires only hours rather than days. This capability is particularly valuable for WiFi sensing where deployment requirements may vary across applications.

\section{Experimental Setup}

\subsection{Datasets and Preprocessing}

Our experimental evaluation employs three diverse WiFi CSI datasets that represent different activity recognition scenarios and complexity levels. The SenseFi dataset contains CSI data from 10 activities performed by 15 subjects in controlled laboratory environments, providing a benchmark for comparing with manually designed architectures~\cite{sensefi2023}. The SignFi dataset focuses on sign language recognition with 276 gestures from 5 subjects, representing fine-grained activity recognition with a large number of classes~\cite{signfi2019}. The Widar3.0 dataset includes 22 gestures from 16 subjects across different rooms and orientations, testing generalization across environmental variations~\cite{widar2019}. Each dataset is processed to extract CSI amplitude and phase information with appropriate denoising and normalization.

Data preprocessing follows established protocols while ensuring compatibility with our search space design. CSI amplitude is normalized using min-max scaling per antenna pair to account for distance and hardware variations. Phase information is sanitized through linear fitting to remove random phase offsets while preserving motion-induced phase changes. The processed CSI forms a 3D tensor with dimensions for time, subcarrier, and antenna pairs. Sliding windows with 50% overlap extract fixed-length segments suitable for neural network processing. Data augmentation including temporal jittering, noise injection, and mixup is applied during training to improve generalization.

The datasets are split into training, validation, and test sets following standard protocols to ensure fair comparison. For architecture search, we further split the training set to create a search training set and search validation set. The search training set is used for updating network weights during search. The search validation set guides architecture parameter updates and early stopping decisions. After search completion, discovered architectures are retrained on the full training set and evaluated on the held-out test set. This evaluation protocol prevents overfitting to the search validation set and provides unbiased performance estimates.

Cross-dataset evaluation assesses the transferability of discovered architectures across different WiFi sensing scenarios. Architectures discovered on one dataset are evaluated on others without modification to test generalization. Fine-tuning experiments measure how quickly architectures can adapt to new datasets. Transfer learning from discovered architectures is compared with training from random initialization. These experiments validate that our NAS framework discovers generally useful architectures rather than dataset-specific solutions.

\subsection{Hardware Platforms and Profiling}

We evaluate discovered architectures across a diverse range of edge computing platforms representing different deployment scenarios. The NVIDIA Jetson Nano serves as a representative embedded GPU platform with 128 CUDA cores and 4GB memory, common in edge AI applications. Raspberry Pi 4 with 4GB RAM represents ARM-based single-board computers widely used in IoT deployments. Intel Neural Compute Stick 2 demonstrates deployment on specialized neural network accelerators. Google Coral Edge TPU showcases integer-only inference on purpose-built hardware. These platforms span a wide range of computational capabilities and power budgets.

Hardware profiling establishes accurate performance models used during architecture search. Each platform is profiled with a comprehensive benchmark suite covering all operations in our search space. Latency measurements use high-resolution timers with warm-up runs to ensure stable performance. Power consumption is measured using external power monitors for accurate energy profiling. Memory usage is tracked through platform-specific tools to capture both peak and average consumption. The profiling data is collected under various operating conditions including different batch sizes, input dimensions, and thermal states.

Platform-specific optimizations are applied to ensure fair comparison and practical deployment performance. TensorRT optimization is used on Jetson Nano to leverage tensor cores and kernel fusion. TensorFlow Lite with XNNPACK acceleration is employed on Raspberry Pi for efficient ARM execution. OpenVINO optimization is applied for Intel Neural Compute Stick deployment. Edge TPU compiler optimizations are used for Coral deployment including model partitioning for operations not supported by TPU. These optimizations reflect real-world deployment practices and ensure discovered architectures are evaluated under realistic conditions.

Deployment pipelines are developed for each platform to streamline the evaluation process. Automated scripts handle model conversion, optimization, and deployment for each target platform. Performance measurement tools ensure consistent and reliable benchmarking across platforms. The pipelines support both floating-point and quantized inference to explore precision-performance trade-offs. This infrastructure enables rapid evaluation of many architectures across multiple platforms during and after search.

\subsection{Search Configuration and Hyperparameters}

The architecture search is configured with hyperparameters tuned through preliminary experiments to balance search quality and efficiency. The search space contains approximately 10^15 possible architectures, requiring careful search strategy to find optimal solutions. Search is conducted for 50 epochs with early stopping based on validation performance plateau. The architecture parameters are optimized using Adam with learning rate 3e-4 and weight decay 1e-3. Network weights use SGD with cosine learning rate schedule starting at 0.025. The bilevel optimization alternates between one epoch of weight training and one step of architecture optimization.

Multi-objective optimization weights are configured based on deployment requirements for different scenarios. For accuracy-focused search, we use weights of [1.0, 0.1, 0.1, 0.1] for accuracy, latency, energy, and memory respectively. Latency-critical applications use [0.7, 1.0, 0.3, 0.3] to prioritize inference speed. Energy-efficient scenarios employ [0.7, 0.3, 1.0, 0.5] for battery-powered devices. Memory-constrained deployments use [0.7, 0.3, 0.3, 1.0] for microcontroller targets. These weight configurations guide search toward different regions of the Pareto frontier.

The search process employs multiple runs with different random seeds to ensure robustness and explore diverse architectures. Each search configuration is repeated 3 times with different initializations. The Pareto frontier is constructed by combining results from all runs. Statistical significance of performance differences is assessed using paired t-tests. The best architectures are selected based on validation performance and verified through retraining from scratch. This rigorous evaluation ensures that reported results are reliable and reproducible.

Hardware-aware search is conducted with target platform specifications to discover platform-specific architectures. Latency constraints are set based on application requirements, typically 20-100ms for real-time processing. Memory budgets reflect platform limitations, ranging from 10MB for microcontrollers to 100MB for embedded GPUs. Energy targets are derived from battery life requirements and thermal limits. The search process continuously monitors these metrics and adjusts architecture exploration accordingly.

\subsection{Baseline Methods and Evaluation Metrics}

We compare our NAS-discovered architectures against comprehensive baselines including manually designed networks and other NAS methods. Manual baselines include the original SenseFi CNN architecture, EfficientFi lightweight models, and custom architectures combining CNN, LSTM, and attention mechanisms. These baselines represent the state-of-the-art in manually designed architectures for WiFi sensing. Generic NAS baselines include DARTS, GDAS, and ProxylessNAS applied to our CSI datasets. These comparisons evaluate the benefit of our CSI-specific search space and hardware-aware optimization.

Evaluation metrics comprehensively assess both model performance and deployment efficiency. Classification accuracy and F1-score measure recognition performance on test sets. Inference latency is measured as end-to-end time from input to prediction on each hardware platform. Energy consumption per inference is computed from power measurements and inference time. Model size in parameters and compressed format affects storage and transmission costs. Memory footprint during inference determines deployability on memory-constrained devices. These metrics provide a complete picture of architecture performance.

Statistical analysis ensures reliable conclusions about architecture performance differences. Each architecture is evaluated with 5 different random initializations to account for training variance. Confidence intervals are computed using bootstrap sampling with 1000 iterations. Paired t-tests assess statistical significance of performance improvements. Effect sizes are reported using Cohen's d to quantify practical significance. These statistical measures ensure that reported improvements are meaningful and reproducible.

Ablation studies isolate the contribution of different components in our NAS framework. We evaluate variants without hardware-aware optimization to assess its impact on deployment efficiency. Search without CSI-specific operations demonstrates the value of domain-specific design. Removing early stopping and pruning quantifies their contribution to search efficiency. Different search algorithms are compared to validate our technical choices. These ablations provide insights into which components are critical for successful architecture discovery.

\section{Results and Analysis}

\subsection{Overall Performance Comparison}

Our NAS framework successfully discovers architectures that achieve superior accuracy-efficiency trade-offs compared to both manually designed networks and generic NAS methods. On the SenseFi dataset, our best discovered architecture achieves 94.3% accuracy while requiring only 152K parameters, compared to the original SenseFi CNN with 87.2% accuracy and 1.2M parameters. This represents a 7.1% accuracy improvement with an 8× reduction in model size. The discovered architecture also achieves 5× faster inference on Jetson Nano (18ms vs 89ms) and 3× lower energy consumption (12mJ vs 38mJ per inference).

Performance on the SignFi dataset with 276 gesture classes demonstrates scalability to fine-grained recognition tasks. Our discovered architecture achieves 98.1% accuracy with 423K parameters, outperforming the manually designed CNN-LSTM baseline achieving 96.3% accuracy with 3.5M parameters. The 8× parameter reduction is particularly impressive given the large number of classes. Inference latency on Raspberry Pi 4 is reduced from 186ms to 47ms, enabling real-time gesture recognition at over 20 fps. The architecture effectively balances model capacity for high accuracy with efficiency for edge deployment.

Widar3.0 results validate the generalization capability of discovered architectures across environmental variations. The NAS architecture achieves 92.7% accuracy in cross-domain evaluation, compared to 88.4% for manual baselines. This improved generalization stems from the architecture's ability to capture robust features that transfer across environments. The discovered architecture maintains consistent performance across different rooms (91.8-93.2%) and user orientations (91.3-93.5%). Hardware performance remains excellent with 22ms latency on Jetson Nano and 71ms on Raspberry Pi 4.

Comparison with generic NAS methods highlights the value of our CSI-specific design. DARTS applied to WiFi sensing achieves 91.2% accuracy with 892K parameters on SenseFi. ProxylessNAS discovers architectures with good latency (15ms on Jetson) but lower accuracy (89.7%). GDAS finds accurate models (92.8%) but with higher complexity (1.8M parameters). Our framework consistently discovers architectures that dominate generic NAS solutions on the Pareto frontier. The CSI-specific search space and hardware-aware optimization are crucial for this superior performance.

\subsection{Pareto Frontier Analysis}

The discovered Pareto frontier reveals the rich trade-off space between accuracy and various efficiency metrics. Figure~\ref{fig:pareto} illustrates architectures ranging from ultra-efficient models with 85% accuracy and 5ms latency to high-accuracy models achieving 94.3% with 25ms latency on Jetson Nano. The frontier is well-populated with diverse architectures, providing options for different deployment scenarios. No manually designed architecture lies on the Pareto frontier, confirming that automated search discovers superior trade-offs.

Analysis of the Pareto frontier across different hardware platforms reveals platform-specific optimization opportunities. Architectures optimal for Jetson Nano with GPU acceleration differ from those best suited for Raspberry Pi's ARM CPU. GPU-optimized architectures favor regular computations with high parallelism such as standard convolutions. CPU-optimized architectures prefer depthwise separable convolutions and linear operations with better cache locality. The framework successfully discovers these platform-specific patterns without manual guidance.

The trade-off between accuracy and model size follows a distinctive pattern with diminishing returns beyond certain thresholds. Architectures with fewer than 100K parameters achieve up to 88% accuracy, suitable for microcontroller deployment. The 100K-500K parameter range provides the best accuracy-size trade-off, reaching 92-94% accuracy. Beyond 500K parameters, accuracy improvements are marginal (less than 1%) while deployment costs increase substantially. This analysis guides the selection of appropriate architectures for different resource constraints.

Energy-accuracy trade-offs are particularly important for battery-powered IoT devices. The Pareto frontier shows that 90% accuracy is achievable with less than 10mJ per inference on Jetson Nano. Reaching 94% accuracy requires approximately 15mJ, while the last 0.5% accuracy costs an additional 10mJ. For continuous monitoring applications, the 90% accuracy point provides 5 hours of operation on a 2000mAh battery. These energy measurements inform deployment decisions for battery-powered scenarios.

\subsection{Architecture Analysis and Design Insights}

Detailed analysis of discovered architectures reveals consistent design patterns that contribute to efficient CSI processing. Successful architectures employ a multi-scale temporal processing strategy with parallel branches handling different time scales. Short-term branches with 3-5 frame receptive fields capture rapid movements and transitions. Long-term branches with 15-20 frame receptive fields model activity-level patterns. This multi-scale design appears in 80% of Pareto-optimal architectures, confirming its importance for WiFi sensing.

The treatment of frequency (subcarrier) and spatial (antenna) dimensions shows interesting patterns in discovered architectures. Early layers typically process subcarriers independently before cross-frequency fusion in deeper layers. This design respects the physical meaning of subcarriers as frequency bins while enabling learned fusion. Antenna dimensions are often processed jointly from the beginning, suggesting that spatial diversity is best exploited through joint processing. Attention mechanisms frequently appear at fusion points, dynamically weighting different information sources.

Depth and width configurations of discovered architectures follow patterns distinct from image classification networks. Optimal architectures for WiFi sensing are relatively shallow (6-10 layers) but wide within each layer. This contrasts with the very deep architectures common in computer vision. The shallow depth suffices for CSI's lower complexity compared to natural images while reducing latency. Wide layers effectively process the multi-dimensional CSI tensor without excessive depth. Skip connections appear primarily within stages rather than across the entire network.

Operation selection statistics reveal preferences for certain building blocks in efficient architectures. Depthwise separable convolutions appear in 73% of operations in efficient architectures, confirming their favorable accuracy-efficiency trade-off. Squeeze-and-excitation modules appear in 90% of architectures, typically after feature extraction stages. Global average pooling is strongly preferred over fully connected layers for final aggregation. ReLU remains the dominant activation function (85%) with occasional use of Swish (15%) in critical layers. These patterns provide guidance for future manual architecture design.

\subsection{Search Efficiency and Ablation Studies}

The proposed search efficiency improvements successfully reduce search cost while maintaining architecture quality. With all efficiency techniques enabled, the average search time is 8.3 GPU-hours on a single NVIDIA V100. Predictor-based early stopping contributes the largest reduction, decreasing search time by 75% from the baseline 33.2 hours. Progressive pruning provides an additional 15% reduction by focusing on promising operations. Transfer learning from previous searches reduces time by 20% when applicable. Combined, these techniques make NAS practical for WiFi sensing applications.

Ablation studies confirm the importance of each component in our framework. Removing the CSI-specific search space and using a generic CNN search space reduces accuracy by 3.2% and increases latency by 45%. Without hardware-aware optimization, discovered architectures are 2.3× slower on target devices despite similar accuracy. Disabling early stopping increases search time by 4× with no improvement in final architecture quality. Removing progressive pruning leads to 15% longer search and occasionally unstable convergence. These results validate our design decisions.

The quality of hardware performance predictors is crucial for effective hardware-aware search. Our hybrid predictors achieve Spearman rank correlation of 0.91 for latency, 0.88 for energy, and 0.94 for memory with actual measurements. Pure analytical models achieve only 0.72, 0.65, and 0.83 correlation respectively. Learned predictors without analytical components show higher variance and occasionally large errors. The combination of analytical models, lookup tables, and learned corrections provides both accuracy and robustness.

Search reproducibility and stability are important for practical deployment of NAS. Across 5 independent runs with different random seeds, the discovered architectures show consistent performance with standard deviation of 0.8% in accuracy. The Pareto frontiers from different runs have 65% overlap in dominated hypervolume. Critical design patterns such as multi-scale processing and SE modules appear consistently across runs. This stability indicates that the search process reliably discovers good architectures rather than relying on lucky initialization.

\subsection{Deployment Case Studies}

Real-world deployment of discovered architectures validates their practical applicability beyond benchmark evaluations. In a smart home deployment for elderly monitoring, we deployed a latency-optimized architecture on Raspberry Pi 4 devices. The system achieves 91.3% accuracy for detecting falls and abnormal activities with 52ms latency, enabling real-time response. Power consumption of 2.1W allows continuous operation on standard USB power. The deployment has been running for 3 months with no performance degradation, demonstrating long-term stability.

A gesture recognition system for human-computer interaction showcases deployment on NVIDIA Jetson Nano. The accuracy-optimized architecture achieves 97.2% accuracy across 50 custom gestures with 19ms latency. The low latency enables smooth interaction without noticeable delay. The system processes 52 fps, exceeding the 30 fps requirement for responsive interaction. Integration with existing applications through a simple API demonstrates ease of deployment. User studies show 94% satisfaction with recognition accuracy and responsiveness.

Deployment on resource-constrained microcontrollers pushes the limits of architecture efficiency. Using an ultra-efficient architecture with 43K parameters, we achieve 85.3% accuracy on an ESP32 microcontroller. The model fits within 256KB flash memory with 8-bit quantization. Inference takes 312ms using the ESP32's dual-core processor at 240MHz. Power consumption of 180mW enables months of operation on battery power. While accuracy is reduced compared to larger models, it suffices for simple activity detection applications.

Cross-platform deployment demonstrates the portability of discovered architectures. The same architecture achieves consistent accuracy (±0.5%) across Jetson Nano, Raspberry Pi, and Intel NCS after platform-specific optimization. Latency varies significantly (18ms on Jetson, 71ms on Pi, 43ms on NCS) but remains within acceptable bounds for real-time processing. Automated deployment pipelines reduce the effort required for multi-platform deployment. This portability is valuable for heterogeneous IoT deployments with diverse hardware.

\subsection{Generalization and Transfer Learning}

Cross-dataset generalization experiments evaluate whether discovered architectures transfer to new WiFi sensing tasks. Architectures discovered on SenseFi and evaluated on SignFi without modification achieve 92.3% accuracy, only 5.8% below architectures searched specifically for SignFi. Fine-tuning for 10 epochs recovers most of this gap, reaching 96.7% accuracy. This strong transfer suggests that discovered architectures capture general principles of CSI processing rather than dataset-specific patterns.

The architecture knowledge base accumulated across multiple searches provides valuable insights and acceleration for new searches. After conducting searches on 10 different configurations, new searches initialize from the knowledge base converge 3× faster than random initialization. The knowledge base identifies consistently successful patterns such as multi-scale processing and attention mechanisms. Failed architecture patterns are also recorded to avoid repeated exploration of poor designs. This accumulated knowledge makes each subsequent search more efficient.

Few-shot architecture adaptation enables rapid customization for new deployment scenarios. Given a discovered architecture, adapting to new hardware constraints requires only 2-3 GPU-hours of fine-tuning. Adaptation to new activity sets through transfer learning achieves 85% of full-search performance with 10× less computation. This capability is particularly valuable for commercial deployment where requirements may vary across customers. The framework provides both general-purpose architectures and customization capability.

Meta-learning experiments demonstrate the potential for learning to search more effectively. After training on search trajectories from 20 different tasks, the meta-learned search algorithm discovers better architectures 25% faster. The meta-learner identifies promising initial architectures and search hyperparameters for new tasks. Search strategies that balance exploration and exploitation are automatically adjusted based on search progress. While meta-learning requires substantial initial investment, it provides long-term benefits for organizations conducting multiple searches.

\section{Discussion}

\subsection{Key Findings and Contributions}

Our comprehensive evaluation demonstrates that neural architecture search can discover WiFi CSI processing architectures that substantially outperform manual designs across multiple metrics. The 8× reduction in parameters while improving accuracy challenges the conventional wisdom that larger models are necessary for complex sensing tasks. The discovered architectures reveal that efficient CSI processing requires different design patterns than computer vision, with shallow-wide networks and multi-scale temporal processing being particularly effective. These insights advance our understanding of how to design neural networks for time-series sensor data.

The success of hardware-aware optimization in discovering platform-specific architectures validates the importance of considering deployment constraints during architecture design rather than as an afterthought. The significant differences between GPU-optimized and CPU-optimized architectures highlight that optimal designs are highly platform-dependent. The ability to discover architectures meeting strict latency, energy, and memory constraints while maintaining high accuracy enables deployment scenarios previously considered infeasible. This hardware-aware approach is essential for practical IoT applications where computational resources are limited.

The efficiency improvements that reduce search cost by 75% make NAS practical for research groups and companies without massive computational resources. The predictor-based early stopping mechanism is particularly valuable, providing reliable performance estimates from partial training. Progressive pruning and transfer learning further reduce search cost while maintaining quality. These techniques democratize access to NAS technology and enable broader adoption in the WiFi sensing community.

\subsection{Limitations and Future Work}

Despite significant advances, several limitations remain that present opportunities for future research. The current search space, while comprehensive, may not capture all possible architectural innovations for CSI processing. Future work should explore more exotic operations such as complex-valued networks that naturally handle CSI's complex representation, graph neural networks for modeling antenna relationships, and temporal convolutions with learnable dilation patterns. Expanding the search space while maintaining efficiency is a key challenge.

The hardware performance predictors, while accurate for common operations, may have larger errors for novel architectural patterns not well-represented in training data. Improving predictor robustness through techniques such as uncertainty quantification and active learning could enhance search reliability. Online learning of predictors during search could adapt to new architectural patterns. Development of analytical models that better capture hardware behavior would provide more interpretable predictions.

Scalability to larger search spaces and more complex multi-modal sensing scenarios remains challenging. Searching over architectures that process multiple sensor modalities simultaneously increases the search space exponentially. Hierarchical search strategies that decompose the problem into manageable sub-problems could address this challenge. Compositional search that combines pre-discovered modules could enable efficient exploration of very large search spaces.

\subsection{Practical Implications}

The availability of automated architecture discovery tools fundamentally changes how WiFi sensing systems are developed. Rather than months of manual architecture engineering, developers can obtain optimized architectures in days. This acceleration enables rapid prototyping and deployment of sensing applications. The ability to generate architectures for specific hardware platforms eliminates the expertise barrier for embedded deployment. Small research groups and companies can now develop competitive sensing systems without deep learning expertise.

The discovered architectures and design insights provide valuable guidance for the broader sensing community. The identified patterns such as multi-scale temporal processing and early frequency-independent processing can inform manual architecture design. The hardware-specific optimization strategies reveal how to adapt architectures for different platforms. The comprehensive evaluation across multiple datasets and platforms establishes new baselines for future research. Open-sourcing our framework and discovered architectures will accelerate progress in WiFi sensing research.

Integration of NAS into the standard development workflow for IoT applications could significantly improve deployed system performance. Automated architecture discovery could be triggered when new hardware platforms become available or requirements change. Continuous architecture improvement through online NAS could adapt to evolving data distributions. The combination of NAS with other AutoML techniques such as hyperparameter optimization and data augmentation search could fully automate model development.

\subsection{Broader Impact}

The democratization of efficient neural architecture design has important implications for accessibility and fairness in AI deployment. By enabling high-performance sensing on low-cost hardware, our framework makes advanced HAR capabilities accessible to resource-constrained communities. The reduced energy consumption of discovered architectures contributes to sustainable AI by decreasing the carbon footprint of deployed systems. The ability to rapidly customize architectures for specific scenarios enables personalized sensing systems that better serve diverse populations.

The potential misuse of highly efficient sensing systems for surveillance raises privacy concerns that must be carefully addressed. While our focus is on beneficial applications such as healthcare and smart homes, the same technology could enable pervasive monitoring. Technical safeguards such as on-device processing and differential privacy should be incorporated into deployment. Policy frameworks governing the use of WiFi sensing technology need to evolve alongside technical capabilities. The research community has a responsibility to consider and address these societal implications.

\section{Conclusion}

This paper presented a comprehensive neural architecture search framework specifically designed for WiFi CSI-based human activity recognition with hardware-aware optimization for edge deployment. Through careful design of a CSI-specific search space, multi-objective optimization considering hardware constraints, and efficient search algorithms, we demonstrated that automated architecture discovery can substantially outperform manual design across multiple metrics. The discovered architectures achieve 94.3% accuracy while requiring 8× fewer parameters and 5× lower latency than manually designed networks, enabling deployment on resource-constrained edge devices that was previously infeasible.

Our key technical contributions advance the state-of-the-art in both neural architecture search and WiFi sensing. The CSI-specific search space captures the unique characteristics of wireless channel data while maintaining computational efficiency. The hardware-aware multi-objective optimization discovers architectures optimized for specific deployment platforms, from microcontrollers to embedded GPUs. The efficient search algorithms reduce search cost by 75% through predictor-based early stopping and progressive pruning, making NAS practical for widespread adoption. Comprehensive evaluation across three datasets and five hardware platforms validates the effectiveness and generality of our approach.

The analysis of discovered architectures reveals important design principles for efficient CSI processing that differ from computer vision networks. Multi-scale temporal processing, shallow-wide configurations, and early frequency-independent processing consistently appear in optimal architectures. These insights advance our understanding of neural network design for time-series sensor data and provide guidance for future research. The strong generalization across datasets and successful deployment case studies demonstrate practical applicability beyond academic benchmarks.

Future work will explore several promising directions to extend and improve the framework. Expansion of the search space to include more exotic operations could discover novel architectural patterns. Improvement of hardware predictors through uncertainty quantification and online learning would enhance search reliability. Extension to multi-modal sensing and larger-scale deployment scenarios would broaden applicability. Integration with other AutoML techniques could fully automate the development of sensing systems.

The successful demonstration of NAS for WiFi sensing opens new possibilities for rapid development and deployment of intelligent IoT applications. By automating the complex process of architecture design and optimization, our framework enables researchers and developers to focus on application-level innovation rather than low-level model engineering. The discovered architectures and open-source framework will accelerate progress in WiFi sensing research and enable new applications in healthcare, smart homes, and human-computer interaction. As edge computing capabilities continue to improve and IoT deployments expand, automated architecture discovery will become increasingly important for realizing the full potential of pervasive sensing systems.

\section*{Acknowledgments}

We thank the anonymous reviewers for their constructive feedback. This work was supported by [funding information placeholder].

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}