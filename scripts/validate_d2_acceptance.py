#!/usr/bin/env python3
"""
D2å®éªŒéªŒæ”¶æ ‡å‡†è‡ªåŠ¨åŒ–æ£€æŸ¥è„šæœ¬
ç”¨äºéªŒè¯D2å®éªŒç»“æœæ˜¯å¦æ»¡è¶³æ‰€æœ‰éªŒæ”¶æ ‡å‡†
"""

import json
import pandas as pd
import numpy as np
from pathlib import Path
import argparse
import sys
import os
from scipy import stats
from typing import Dict, List, Tuple


class D2AcceptanceValidator:
    """D2å®éªŒéªŒæ”¶æ ‡å‡†éªŒè¯å™¨"""
    
    def __init__(self, results_dir: str = "results_gpu/d2"):
        self.results_dir = Path(results_dir)
        self.report = []
        self.passed_checks = 0
        self.total_checks = 0
        
    def log(self, message: str, level: str = "INFO"):
        """è®°å½•éªŒè¯ä¿¡æ¯"""
        prefix = {
            "INFO": "â„¹ï¸",
            "PASS": "âœ…",
            "FAIL": "âŒ", 
            "WARN": "âš ï¸"
        }.get(level, "â€¢")
        
        formatted = f"{prefix} {message}"
        self.report.append(formatted)
        print(formatted)
        
    def check(self, condition: bool, success_msg: str, failure_msg: str):
        """æ‰§è¡Œå•é¡¹éªŒæ”¶æ£€æŸ¥"""
        self.total_checks += 1
        if condition:
            self.passed_checks += 1
            self.log(success_msg, "PASS")
            return True
        else:
            self.log(failure_msg, "FAIL")
            return False
    
    def load_experiment_results(self) -> pd.DataFrame:
        """åŠ è½½æ‰€æœ‰å®éªŒç»“æœ"""
        self.log("Loading experiment results...")
        
        json_files = list(self.results_dir.glob("*.json"))
        if not json_files:
            self.log(f"No JSON files found in {self.results_dir}", "FAIL")
            return pd.DataFrame()
        
        results = []
        for json_file in json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # æå–å…³é”®ä¿¡æ¯
                args = data.get('args', {})
                metrics = data.get('metrics', {})
                
                result = {
                    'file': json_file.name,
                    'model': args.get('model', 'unknown'),
                    'difficulty': args.get('difficulty', 'unknown'),
                    'seed': args.get('seed', 0),
                    'macro_f1': metrics.get('macro_f1', 0.0),
                    'falling_f1': metrics.get('falling_f1', 0.0),
                    'mutual_misclass': metrics.get('mutual_misclass', 0.0),
                    'ece': metrics.get('ece', 1.0),
                    'ece_raw': metrics.get('ece_raw', 1.0),
                    'ece_cal': metrics.get('ece_cal', 1.0),
                    'brier': metrics.get('brier', 1.0),
                    'temperature': metrics.get('temperature', 1.0),
                }
                results.append(result)
                
            except Exception as e:
                self.log(f"Error loading {json_file}: {e}", "WARN")
        
        df = pd.DataFrame(results)
        self.log(f"Loaded {len(df)} experiment results")
        return df
    
    def validate_basic_completeness(self, df: pd.DataFrame) -> bool:
        """éªŒè¯åŸºæœ¬å®Œæ•´æ€§"""
        self.log("\n=== åŸºæœ¬å®Œæ•´æ€§æ£€æŸ¥ ===")
        
        # æ£€æŸ¥å®éªŒæ•°é‡
        expected_experiments = 4 * 8 * 3  # 4æ¨¡å‹ Ã— 8ç§å­ Ã— 3éš¾åº¦
        actual_experiments = len(df)
        
        completeness_ok = self.check(
            actual_experiments >= expected_experiments * 0.9,  # å…è®¸10%çš„å¤±è´¥ç‡
            f"å®éªŒå®Œæ•´æ€§: {actual_experiments}/{expected_experiments} (â‰¥90%)",
            f"å®éªŒå®Œæ•´æ€§ä¸è¶³: {actual_experiments}/{expected_experiments} (<90%)"
        )
        
        # æ£€æŸ¥æ¨¡å‹è¦†ç›–
        expected_models = {'enhanced', 'lstm', 'tcn', 'txf'}
        actual_models = set(df['model'].unique())
        models_ok = self.check(
            expected_models.issubset(actual_models),
            f"æ¨¡å‹è¦†ç›–å®Œæ•´: {sorted(actual_models)}",
            f"æ¨¡å‹è¦†ç›–ä¸è¶³: æœŸæœ›{sorted(expected_models)}, å®é™…{sorted(actual_models)}"
        )
        
        # æ£€æŸ¥éš¾åº¦çº§åˆ«è¦†ç›–
        expected_difficulties = {'easy', 'mid', 'hard'}
        actual_difficulties = set(df['difficulty'].unique())
        difficulties_ok = self.check(
            expected_difficulties.issubset(actual_difficulties),
            f"éš¾åº¦çº§åˆ«å®Œæ•´: {sorted(actual_difficulties)}",
            f"éš¾åº¦çº§åˆ«ä¸è¶³: æœŸæœ›{sorted(expected_difficulties)}, å®é™…{sorted(actual_difficulties)}"
        )
        
        return completeness_ok and models_ok and difficulties_ok
    
    def validate_performance_metrics(self, df: pd.DataFrame) -> bool:
        """éªŒè¯æ€§èƒ½æŒ‡æ ‡"""
        self.log("\n=== æ€§èƒ½æŒ‡æ ‡éªŒè¯ ===")
        
        # 1. Falling F1 < 0.99 (ä¸¥æ ¼æ ‡å‡†)
        falling_f1_max = df['falling_f1'].max()
        falling_f1_ok = self.check(
            falling_f1_max < 0.99,
            f"Falling F1æœªè¾¾å®Œç¾: æœ€å¤§å€¼={falling_f1_max:.4f} < 0.99",
            f"Falling F1è¿‡äºå®Œç¾: æœ€å¤§å€¼={falling_f1_max:.4f} â‰¥ 0.99"
        )
        
        # 2. Mutual Misclass > 0 (å¿…é¡»æœ‰æ··æ·†)
        mutual_misclass_min = df['mutual_misclass'].min()
        mutual_misclass_ok = self.check(
            mutual_misclass_min > 0,
            f"å­˜åœ¨ç±»é—´æ··æ·†: æœ€å°å€¼={mutual_misclass_min:.6f} > 0",
            f"æ— ç±»é—´æ··æ·†: æœ€å°å€¼={mutual_misclass_min:.6f} â‰¤ 0"
        )
        
        # 3. Macro F1 åœ¨åˆç†èŒƒå›´ (0.80-0.98)
        macro_f1_range = (df['macro_f1'].min(), df['macro_f1'].max())
        macro_f1_ok = self.check(
            0.80 <= macro_f1_range[0] and macro_f1_range[1] <= 0.98,
            f"Macro F1èŒƒå›´åˆç†: {macro_f1_range[0]:.3f}-{macro_f1_range[1]:.3f}",
            f"Macro F1èŒƒå›´å¼‚å¸¸: {macro_f1_range[0]:.3f}-{macro_f1_range[1]:.3f}"
        )
        
        return falling_f1_ok and mutual_misclass_ok and macro_f1_ok
    
    def validate_calibration_metrics(self, df: pd.DataFrame) -> bool:
        """éªŒè¯æ ¡å‡†æŒ‡æ ‡"""
        self.log("\n=== æ ¡å‡†æŒ‡æ ‡éªŒè¯ ===")
        
        # 1. Enhancedæ¨¡å‹ECEæ”¹å–„
        enhanced_df = df[df['model'] == 'enhanced']
        if len(enhanced_df) == 0:
            self.log("æœªæ‰¾åˆ°Enhancedæ¨¡å‹ç»“æœ", "WARN")
            return False
        
        # æ¸©åº¦æ ¡å‡†æ”¹å–„
        ece_improvement = (enhanced_df['ece_raw'] - enhanced_df['ece_cal']).mean()
        calibration_ok = self.check(
            ece_improvement > 0,
            f"æ¸©åº¦æ ¡å‡†å¹³å‡æ”¹å–„: {ece_improvement:.4f} > 0",
            f"æ¸©åº¦æ ¡å‡†æ— æ”¹å–„: {ece_improvement:.4f} â‰¤ 0"
        )
        
        # 2. Enhanced vs åŸºçº¿ECEå¯¹æ¯”
        baseline_models = ['lstm', 'tcn', 'txf']
        baseline_df = df[df['model'].isin(baseline_models)]
        
        if len(baseline_df) > 0:
            enhanced_ece_mean = enhanced_df['ece'].mean()
            baseline_ece_mean = baseline_df['ece'].mean()
            ece_advantage = baseline_ece_mean - enhanced_ece_mean
            
            ece_comparison_ok = self.check(
                ece_advantage > 0.01,  # Enhancedåº”è¯¥æ¯”åŸºçº¿ä½è‡³å°‘1%
                f"Enhanced ECEä¼˜åŠ¿: {ece_advantage:.4f} > 0.01",
                f"Enhanced ECEä¼˜åŠ¿ä¸è¶³: {ece_advantage:.4f} â‰¤ 0.01"
            )
        else:
            ece_comparison_ok = False
            self.log("ç¼ºå°‘åŸºçº¿æ¨¡å‹ç»“æœè¿›è¡ŒECEå¯¹æ¯”", "WARN")
        
        return calibration_ok and ece_comparison_ok
    
    def validate_statistical_stability(self, df: pd.DataFrame) -> bool:
        """éªŒè¯ç»Ÿè®¡ç¨³å®šæ€§"""
        self.log("\n=== ç»Ÿè®¡ç¨³å®šæ€§éªŒè¯ ===")
        
        # æ¯ä¸ª(æ¨¡å‹,éš¾åº¦)ç»„åˆçš„ç§å­æ•°
        seed_counts = df.groupby(['model', 'difficulty']).size()
        min_seeds = seed_counts.min()
        
        seed_coverage_ok = self.check(
            min_seeds >= 3,
            f"ç§å­è¦†ç›–å……åˆ†: æœ€å°‘{min_seeds}ä¸ªç§å­/ç»„åˆ â‰¥ 3",
            f"ç§å­è¦†ç›–ä¸è¶³: æœ€å°‘{min_seeds}ä¸ªç§å­/ç»„åˆ < 3"
        )
        
        # æ£€æŸ¥å˜å¼‚ç³»æ•° (CV = std/mean)
        stability_results = []
        for (model, difficulty), group in df.groupby(['model', 'difficulty']):
            if len(group) >= 3:
                cv_f1 = group['macro_f1'].std() / group['macro_f1'].mean()
                cv_ece = group['ece'].std() / group['ece'].mean()
                stability_results.extend([cv_f1, cv_ece])
        
        if stability_results:
            max_cv = max(stability_results)
            stability_ok = self.check(
                max_cv < 0.15,
                f"ç»“æœç¨³å®šæ€§è‰¯å¥½: æœ€å¤§å˜å¼‚ç³»æ•°={max_cv:.3f} < 0.15",
                f"ç»“æœç¨³å®šæ€§ä¸è¶³: æœ€å¤§å˜å¼‚ç³»æ•°={max_cv:.3f} â‰¥ 0.15"
            )
        else:
            stability_ok = False
            self.log("æ— è¶³å¤Ÿæ•°æ®è¿›è¡Œç¨³å®šæ€§åˆ†æ", "WARN")
        
        return seed_coverage_ok and stability_ok
    
    def validate_output_files(self) -> bool:
        """éªŒè¯è¾“å‡ºæ–‡ä»¶"""
        self.log("\n=== è¾“å‡ºæ–‡ä»¶éªŒè¯ ===")
        
        # æ£€æŸ¥å¿…éœ€çš„æ±‡æ€»æ–‡ä»¶
        summary_csv = Path("results/synth/summary.csv")
        summary_ok = self.check(
            summary_csv.exists(),
            f"æ±‡æ€»CSVæ–‡ä»¶å­˜åœ¨: {summary_csv}",
            f"æ±‡æ€»CSVæ–‡ä»¶ç¼ºå¤±: {summary_csv}"
        )
        
        # æ£€æŸ¥å›¾è¡¨æ–‡ä»¶
        plots_dir = Path("plots")
        expected_plots = [
            "fig_synth_bars.pdf",
            "fig_overlap_scatter.pdf"
        ]
        
        plots_ok = True
        for plot_file in expected_plots:
            plot_path = plots_dir / plot_file
            file_ok = self.check(
                plot_path.exists(),
                f"å›¾è¡¨æ–‡ä»¶å­˜åœ¨: {plot_path}",
                f"å›¾è¡¨æ–‡ä»¶ç¼ºå¤±: {plot_path}"
            )
            plots_ok = plots_ok and file_ok
        
        return summary_ok and plots_ok
    
    def generate_summary_statistics(self, df: pd.DataFrame):
        """ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡"""
        self.log("\n=== å®éªŒç»“æœæ‘˜è¦ ===")
        
        # æŒ‰æ¨¡å‹ç»Ÿè®¡
        model_stats = df.groupby('model').agg({
            'macro_f1': ['mean', 'std'],
            'falling_f1': ['mean', 'std'], 
            'ece': ['mean', 'std']
        }).round(4)
        
        self.log("æŒ‰æ¨¡å‹æ±‡æ€»ç»Ÿè®¡:")
        for model in model_stats.index:
            f1_mean, f1_std = model_stats.loc[model, ('macro_f1', 'mean')], model_stats.loc[model, ('macro_f1', 'std')]
            ece_mean, ece_std = model_stats.loc[model, ('ece', 'mean')], model_stats.loc[model, ('ece', 'std')]
            self.log(f"  {model}: F1={f1_mean:.3f}Â±{f1_std:.3f}, ECE={ece_mean:.3f}Â±{ece_std:.3f}")
        
        # æŒ‰éš¾åº¦ç»Ÿè®¡
        difficulty_stats = df.groupby('difficulty').agg({
            'macro_f1': 'mean',
            'falling_f1': 'mean',
            'mutual_misclass': 'mean'
        }).round(4)
        
        self.log("æŒ‰éš¾åº¦æ±‡æ€»ç»Ÿè®¡:")
        for difficulty in difficulty_stats.index:
            stats_row = difficulty_stats.loc[difficulty]
            self.log(f"  {difficulty}: F1={stats_row['macro_f1']:.3f}, "
                    f"Fall F1={stats_row['falling_f1']:.3f}, "
                    f"Misclass={stats_row['mutual_misclass']:.4f}")
    
    def run_full_validation(self) -> bool:
        """è¿è¡Œå®Œæ•´çš„éªŒæ”¶éªŒè¯"""
        self.log("ğŸ” å¼€å§‹D2å®éªŒéªŒæ”¶æ ‡å‡†æ£€æŸ¥")
        self.log(f"ç»“æœç›®å½•: {self.results_dir}")
        
        # åŠ è½½æ•°æ®
        df = self.load_experiment_results()
        if df.empty:
            self.log("æ— æ³•åŠ è½½å®éªŒç»“æœï¼ŒéªŒè¯å¤±è´¥", "FAIL")
            return False
        
        # æ‰§è¡Œå„é¡¹éªŒè¯
        completeness_ok = self.validate_basic_completeness(df)
        performance_ok = self.validate_performance_metrics(df)
        calibration_ok = self.validate_calibration_metrics(df)
        stability_ok = self.validate_statistical_stability(df)
        output_ok = self.validate_output_files()
        
        # ç”Ÿæˆæ‘˜è¦
        self.generate_summary_statistics(df)
        
        # æœ€ç»ˆç»“æœ
        all_passed = all([completeness_ok, performance_ok, calibration_ok, stability_ok, output_ok])
        
        self.log(f"\n{'='*50}")
        self.log(f"éªŒæ”¶ç»“æœ: {self.passed_checks}/{self.total_checks} é¡¹æ£€æŸ¥é€šè¿‡")
        
        if all_passed:
            self.log("ğŸ‰ D2å®éªŒéªŒæ”¶é€šè¿‡ï¼æ‰€æœ‰æ ‡å‡†å‡å·²è¾¾æˆ", "PASS")
        else:
            self.log("âŒ D2å®éªŒéªŒæ”¶å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°å¤±è´¥é¡¹", "FAIL")
        
        return all_passed


def main():
    parser = argparse.ArgumentParser(description="D2å®éªŒéªŒæ”¶æ ‡å‡†æ£€æŸ¥")
    parser.add_argument("--results-dir", type=str, default="results_gpu/d2",
                       help="å®éªŒç»“æœç›®å½•è·¯å¾„")
    parser.add_argument("--save-report", type=str, 
                       help="ä¿å­˜éªŒæ”¶æŠ¥å‘Šåˆ°æŒ‡å®šæ–‡ä»¶")
    
    args = parser.parse_args()
    
    # åˆ›å»ºéªŒè¯å™¨å¹¶è¿è¡Œ
    validator = D2AcceptanceValidator(args.results_dir)
    success = validator.run_full_validation()
    
    # ä¿å­˜æŠ¥å‘Š
    if args.save_report:
        report_path = Path(args.save_report)
        report_path.parent.mkdir(parents=True, exist_ok=True)
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"# D2å®éªŒéªŒæ”¶æŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {pd.Timestamp.now()}\n")
            f.write(f"**ç»“æœç›®å½•**: {args.results_dir}\n")
            f.write(f"**éªŒæ”¶çŠ¶æ€**: {'âœ… é€šè¿‡' if success else 'âŒ å¤±è´¥'}\n\n")
            f.write("## è¯¦ç»†æ£€æŸ¥ç»“æœ\n\n```\n")
            for line in validator.report:
                f.write(line + "\n")
            f.write("```\n")
        
        print(f"\nğŸ“„ éªŒæ”¶æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
    
    # è®¾ç½®é€€å‡ºç 
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()