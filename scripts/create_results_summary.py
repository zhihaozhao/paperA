#!/usr/bin/env python3
"""
D2å®éªŒç»“æœæ‘˜è¦ç”Ÿæˆå™¨
å¿«é€Ÿç”ŸæˆMarkdownæ ¼å¼çš„ç»“æœæ‘˜è¦
"""

import os
import pandas as pd
import numpy as np
from pathlib import Path
import argparse
from datetime import datetime

def load_experiment_results(results_dir):
    """åŠ è½½æ‰€æœ‰å®éªŒç»“æœ"""
    results_path = Path(results_dir)
    all_results = []
    
    # æŸ¥æ‰¾æ‰€æœ‰CSVç»“æœæ–‡ä»¶
    csv_files = list(results_path.glob("**/*.csv"))
    
    for csv_file in csv_files:
        try:
            df = pd.read_csv(csv_file)
            all_results.append(df)
        except Exception as e:
            print(f"Error loading {csv_file}: {e}")
    
    if all_results:
        combined_df = pd.concat(all_results, ignore_index=True)
        return combined_df
    else:
        return None

def create_markdown_summary(df, output_path):
    """åˆ›å»ºMarkdownæ‘˜è¦"""
    
    md_content = f"""# ğŸ”¬ D2å®éªŒç»“æœæ‘˜è¦

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“Š å®éªŒæ¦‚è§ˆ

- **æ€»å®éªŒæ•°**: {len(df)} æ¬¡
- **æ¨¡å‹ç±»å‹**: 4 ç§ (enhanced, cnn, bilstm, conformer_lite)
- **éšæœºç§å­**: 5 ä¸ª (0-4)
- **å‚æ•°ç½‘æ ¼**: 3Ã—3Ã—3 = 27 ç§é…ç½®
- **æ€»é…ç½®æ•°**: 4 Ã— 5 Ã— 27 = **540 æ¬¡å®éªŒ**

## ğŸ† æ¨¡å‹æ€§èƒ½æ’å

### Macro F1 Score
"""
    
    if 'macro_f1' in df.columns and 'model' in df.columns:
        # è®¡ç®—å„æ¨¡å‹çš„å¹³å‡æ€§èƒ½
        model_performance = df.groupby('model')['macro_f1'].agg(['mean', 'std', 'count']).round(4)
        model_performance = model_performance.sort_values('mean', ascending=False)
        
        md_content += """
| æ’å | æ¨¡å‹ | Macro F1 (å‡å€¼Â±æ ‡å‡†å·®) | å®éªŒæ¬¡æ•° |
|------|------|------------------------|----------|
"""
        
        for i, (model, stats) in enumerate(model_performance.iterrows(), 1):
            medal = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else "  "
            md_content += f"| {i} {medal} | `{model}` | {stats['mean']:.4f} Â± {stats['std']:.4f} | {int(stats['count'])} |\n"
    
    # ECEæ’å
    if 'ece' in df.columns:
        md_content += "\n### Expected Calibration Error (ECE)\n"
        ece_performance = df.groupby('model')['ece'].agg(['mean', 'std']).round(4)
        ece_performance = ece_performance.sort_values('mean', ascending=True)  # ECEè¶Šå°è¶Šå¥½
        
        md_content += """
| æ’å | æ¨¡å‹ | ECE (å‡å€¼Â±æ ‡å‡†å·®) |
|------|------|------------------|
"""
        
        for i, (model, stats) in enumerate(ece_performance.iterrows(), 1):
            medal = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else "  "
            md_content += f"| {i} {medal} | `{model}` | {stats['mean']:.4f} Â± {stats['std']:.4f} |\n"
    
    # è¶…å‚æ•°å½±å“åˆ†æ
    md_content += "\n## âš™ï¸ è¶…å‚æ•°å½±å“åˆ†æ\n"
    
    params = ['class_overlap', 'label_noise_prob', 'env_burst_rate']
    for param in params:
        if param in df.columns:
            param_effect = df.groupby(param)['macro_f1'].agg(['mean', 'std']).round(4)
            
            param_name_map = {
                'class_overlap': 'Class Overlap',
                'label_noise_prob': 'Label Noise Probability', 
                'env_burst_rate': 'Environment Burst Rate'
            }
            
            md_content += f"\n### {param_name_map.get(param, param)}\n"
            md_content += """
| å‚æ•°å€¼ | Macro F1 (å‡å€¼Â±æ ‡å‡†å·®) |
|--------|----------------------|
"""
            
            for value, stats in param_effect.iterrows():
                md_content += f"| {value} | {stats['mean']:.4f} Â± {stats['std']:.4f} |\n"
    
    # éªŒæ”¶æ ‡å‡†æ£€æŸ¥
    md_content += "\n## âœ… D2éªŒæ”¶æ ‡å‡†æ£€æŸ¥\n"
    
    # æ£€æŸ¥å®éªŒå®Œæˆåº¦
    expected_experiments = 540
    actual_experiments = len(df)
    completion_rate = (actual_experiments / expected_experiments) * 100
    
    status_icon = "âœ…" if completion_rate >= 95 else "âš ï¸" if completion_rate >= 80 else "âŒ"
    
    md_content += f"""
### å®éªŒå®Œæˆåº¦
- **é¢„æœŸå®éªŒæ•°**: {expected_experiments}
- **å®é™…å®Œæˆæ•°**: {actual_experiments}
- **å®Œæˆç‡**: {completion_rate:.1f}% {status_icon}

### æ•°æ®è´¨é‡æ£€æŸ¥
"""
    
    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ¨¡å‹éƒ½æœ‰ç»“æœ
    if 'model' in df.columns:
        expected_models = {'enhanced', 'cnn', 'bilstm', 'conformer_lite'}
        actual_models = set(df['model'].unique())
        missing_models = expected_models - actual_models
        
        if not missing_models:
            md_content += "- **æ¨¡å‹è¦†ç›–**: âœ… æ‰€æœ‰4ä¸ªæ¨¡å‹éƒ½æœ‰ç»“æœ\n"
        else:
            md_content += f"- **æ¨¡å‹è¦†ç›–**: âš ï¸ ç¼ºå¤±æ¨¡å‹: {missing_models}\n"
    
    # æ£€æŸ¥ç§å­è¦†ç›–
    if 'seed' in df.columns:
        expected_seeds = {0, 1, 2, 3, 4}
        actual_seeds = set(df['seed'].unique())
        missing_seeds = expected_seeds - actual_seeds
        
        if not missing_seeds:
            md_content += "- **ç§å­è¦†ç›–**: âœ… æ‰€æœ‰5ä¸ªç§å­éƒ½æœ‰ç»“æœ\n"
        else:
            md_content += f"- **ç§å­è¦†ç›–**: âš ï¸ ç¼ºå¤±ç§å­: {missing_seeds}\n"
    
    # æ€§èƒ½ç¨³å®šæ€§æ£€æŸ¥
    if 'macro_f1' in df.columns and 'model' in df.columns:
        md_content += "\n### æ€§èƒ½ç¨³å®šæ€§\n"
        
        for model in df['model'].unique():
            model_data = df[df['model'] == model]['macro_f1']
            cv = (model_data.std() / model_data.mean()) * 100  # å˜å¼‚ç³»æ•°
            
            stability_icon = "âœ…" if cv < 10 else "âš ï¸" if cv < 20 else "âŒ"
            md_content += f"- **{model}**: CV = {cv:.2f}% {stability_icon}\n"
    
    # åç»­è®¡åˆ’
    md_content += """
## ğŸš€ ä¸‹ä¸€æ­¥è®¡åˆ’

### ç«‹å³ä»»åŠ¡
- [ ] åˆ›å»ºD2å®Œæˆé‡Œç¨‹ç¢‘æ ‡ç­¾ (`v1.0-d2-complete`)
- [ ] å‡†å¤‡Sim2Realå®éªŒæ•°æ®
- [ ] è®¾ç½®SenseFi benchmarkç¯å¢ƒ

### Sim2Realå®éªŒè®¡åˆ’
- [ ] **åŸºçº¿å»ºç«‹**: åœ¨SenseFiæ•°æ®é›†ä¸Šè®­ç»ƒä¼ ç»Ÿæ¨¡å‹
- [ ] **åŸŸè½¬ç§»æµ‹è¯•**: åˆæˆæ•°æ®è®­ç»ƒ â†’ çœŸå®æ•°æ®æµ‹è¯•
- [ ] **å°‘æ ·æœ¬å­¦ä¹ **: ç”¨å°‘é‡çœŸå®æ•°æ®å¾®è°ƒ
- [ ] **è·¨åŸŸæ³›åŒ–**: ä¸åŒæ•°æ®é›†é—´çš„æ€§èƒ½è¯„ä¼°

### è®ºæ–‡å†™ä½œ
- [ ] æ›´æ–°å®éªŒç»“æœåˆ°`main.tex`
- [ ] ç”Ÿæˆæ€§èƒ½å¯¹æ¯”å›¾è¡¨
- [ ] å®Œå–„è®¨è®ºéƒ¨åˆ†
- [ ] å‡†å¤‡æŠ•ç¨¿åˆ°TMC/IoTJæœŸåˆŠ

## ğŸ“‹ æ–‡ä»¶ä½ç½®

- **è¯¦ç»†åˆ†ææŠ¥å‘Š**: `reports/d2_analysis.html`
- **åŸå§‹ç»“æœæ•°æ®**: `results/`
- **éªŒæ”¶è„šæœ¬**: `scripts/validate_d2_acceptance.py`
- **Gitç®¡ç†æŒ‡å—**: `docs/Git_Management_Commands.md`
- **é¡¹ç›®æ¸…å•**: `PROJECT_MANIFEST.md`

---

*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(md_content)
    
    print(f"Markdownæ‘˜è¦å·²ç”Ÿæˆ: {output_path}")

def main():
    parser = argparse.ArgumentParser(description='ç”ŸæˆD2å®éªŒç»“æœæ‘˜è¦')
    parser.add_argument('results_dir', help='ç»“æœç›®å½•è·¯å¾„')
    parser.add_argument('--format', choices=['markdown'], default='markdown', help='è¾“å‡ºæ ¼å¼')
    parser.add_argument('--output', default='D2_Results_Summary.md', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # åŠ è½½ç»“æœ
    print("åŠ è½½å®éªŒç»“æœ...")
    df = load_experiment_results(args.results_dir)
    
    if df is None or df.empty:
        print("é”™è¯¯: æœªæ‰¾åˆ°æœ‰æ•ˆçš„å®éªŒç»“æœ")
        return
    
    print(f"æˆåŠŸåŠ è½½ {len(df)} æ¡å®éªŒè®°å½•")
    
    # ç”Ÿæˆæ‘˜è¦
    if args.format == 'markdown':
        create_markdown_summary(df, args.output)
    
    print("âœ… D2ç»“æœæ‘˜è¦ç”Ÿæˆå®Œæˆï¼")

if __name__ == "__main__":
    main()